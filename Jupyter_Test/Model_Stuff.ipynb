{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import Phase_Detector.Utility as Utility\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "loss = CrossEntropyLoss()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1,0,0,1,0,0,1,0,0,]\n",
    "y_pre_tensor = [torch.tensor([0.1,0.8]).reshape(1,2) for i in range(len(y_true))]\n",
    "metric = Utility.Metrics(2,y_pred=y_pre_tensor, y_true_label=y_true)\n",
    "#metric.get_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义一个简单的自定义数据集\n",
    "# 示例数据\n",
    "data = [i for i in range(10)]  # 一个包含0-9的简单列表\n",
    "\n",
    "# 创建Dataset\n",
    "simple_dataset = ImageDataset(data)\n",
    "\n",
    "# 创建DataLoader\n",
    "#data_loader = DataLoader(simple_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 迭代DataLoader\n",
    "#for batch in data_loader:\n",
    " #   print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch\n",
    "import nibabel as nib\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from monai.transforms import Compose\n",
    "\n",
    "from monai.data import ImageDataset,DataLoader\n",
    "\n",
    "\n",
    "class Image_Dataset(ImageDataset):\n",
    "    def __init__(self,image_files,labels,transform_methods=None,data_aug=True,label_name=None,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            image_files: list of image files path\n",
    "            labels: list of labels\n",
    "            transform_methods: list of transform methods\n",
    "            data_aug: True if data augmentation is used\n",
    "            label_name: name of the label\n",
    "        \n",
    "        \"\"\"\n",
    "        if data_aug:\n",
    "            transform = Compose(transform_methods)\n",
    "        else:\n",
    "            transform = None\n",
    "\n",
    "        super().__init__(image_files=image_files,labels=labels,transform=transform,*args, **kwargs)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image = self.image_files[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # get image array\n",
    "        image = nib.load(image).get_fdata()\n",
    "        # here to do windowing\n",
    "\n",
    "        image = tensor(image)\n",
    "        return image,label\n",
    "    \n",
    "class Data_Loader(DataLoader):\n",
    "    def __init__(self,dataset,batch_size,num_workers=0,*args,**kwargs):\n",
    "        super().__init__(dataset=dataset,batch_size=batch_size,num_workers=num_workers,*args,**kwargs)\n",
    "    \n",
    "    \n",
    "    def build_train_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=True,num_workers=self.num_workers,drop_last=True,*self.args,**self.kwargs)\n",
    "\n",
    "    def build_vali_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n",
    "    \n",
    "    def build_test_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000186E6371F20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import monai\n",
    "import torch\n",
    "\n",
    "model = monai.networks.nets.resnet10()\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "def build_optimizer(parameters,**kwargs):\n",
    "    print(kwargs)\n",
    "    return torch.optim.Adam(parameters,**kwargs)\n",
    "\n",
    "\n",
    "optimizer_param = {\"lr\":0.001}\n",
    "optimizer = build_optimizer(model.parameters(),**optimizer_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            args only have one loss function\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "\n",
    "    def build_loss(self):\n",
    "        return self.args[0]\n",
    "    \n",
    "    def calculate_loss(self,*args):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_do_transform': True,\n",
       " 'prob': 0.5,\n",
       " '_lazy': False,\n",
       " 'min_zoom': (1.0,),\n",
       " 'max_zoom': (1.2,),\n",
       " 'mode': area,\n",
       " 'padding_mode': edge,\n",
       " 'align_corners': None,\n",
       " 'dtype': torch.float32,\n",
       " 'keep_size': True,\n",
       " 'kwargs': {},\n",
       " '_zoom': [1.0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.transforms import Compose,EnsureChannelFirst,RandZoom,RandRotate,RandFlip,ToTensor\n",
    "import inspect\n",
    "transform_param = {\"transform_methods\":[\n",
    "                                EnsureChannelFirst(),\n",
    "                                # Data augmentation\n",
    "                                RandZoom(prob = 0.5, min_zoom=1.0, max_zoom=1.2),\n",
    "                                RandRotate(range_z = 0.35, prob = 0.8),\n",
    "                                RandFlip(prob = 0.5),\n",
    "                                # To tensor\n",
    "                                ToTensor()\n",
    "                                ]}\n",
    "transform_param['transform_methods'][1].__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "a = torch.tensor([0,1])\n",
    "b = [0,]\n",
    "accuracy_score(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Data_Loader(dataset=dataset,batch_size=1,num_workers=0).build_train_loader()\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classifaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pvp_label = pd.read_csv('../../Data/CT_Phase/True_Label/Phase_PVP.csv')\n",
    "#将Phase中的Phase的1和0都换成0,2换成1\n",
    "pvp_label['Phase'] = pvp_label['Phase'].apply(lambda x: 0 if x == 1 or x==0 else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvp_label.to_csv('../../Data/CT_Phase/True_Label/Phase_PVP.csv',index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samuel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
