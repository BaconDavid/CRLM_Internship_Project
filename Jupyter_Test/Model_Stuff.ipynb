{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import Phase_Detector.Utility as Utility\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "loss = CrossEntropyLoss()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1,0,0,1,0,0,1,0,0,]\n",
    "y_pre_tensor = [torch.tensor([0.1,0.8]).reshape(1,2) for i in range(len(y_true))]\n",
    "metric = Utility.Metrics(2,y_pred=y_pre_tensor, y_true_label=y_true)\n",
    "#metric.get_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 定义一个简单的自定义数据集\n",
    "# 示例数据\n",
    "data = [i for i in range(10)]  # 一个包含0-9的简单列表\n",
    "\n",
    "# 创建Dataset\n",
    "simple_dataset = ImageDataset(data)\n",
    "\n",
    "# 创建DataLoader\n",
    "#data_loader = DataLoader(simple_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 迭代DataLoader\n",
    "#for batch in data_loader:\n",
    " #   print(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch\n",
    "import nibabel as nib\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from monai.transforms import Compose\n",
    "\n",
    "from monai.data import ImageDataset,DataLoader\n",
    "\n",
    "\n",
    "class Image_Dataset(ImageDataset):\n",
    "    def __init__(self,image_files,labels,transform_methods=None,data_aug=True,label_name=None,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            image_files: list of image files path\n",
    "            labels: list of labels\n",
    "            transform_methods: list of transform methods\n",
    "            data_aug: True if data augmentation is used\n",
    "            label_name: name of the label\n",
    "        \n",
    "        \"\"\"\n",
    "        if data_aug:\n",
    "            transform = Compose(transform_methods)\n",
    "        else:\n",
    "            transform = None\n",
    "\n",
    "        super().__init__(image_files=image_files,labels=labels,transform=transform,*args, **kwargs)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        image = self.image_files[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        # get image array\n",
    "        image = nib.load(image).get_fdata()\n",
    "        # here to do windowing\n",
    "\n",
    "        image = tensor(image)\n",
    "        return image,label\n",
    "    \n",
    "class Data_Loader(DataLoader):\n",
    "    def __init__(self,dataset,batch_size,num_workers=0,*args,**kwargs):\n",
    "        super().__init__(dataset=dataset,batch_size=batch_size,num_workers=num_workers,*args,**kwargs)\n",
    "    \n",
    "    \n",
    "    def build_train_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=True,num_workers=self.num_workers,drop_last=True,*self.args,**self.kwargs)\n",
    "\n",
    "    def build_vali_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n",
    "    \n",
    "    def build_test_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000186E6371F20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import monai\n",
    "import torch\n",
    "\n",
    "model = monai.networks.nets.resnet10()\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001}\n"
     ]
    }
   ],
   "source": [
    "def build_optimizer(parameters,**kwargs):\n",
    "    print(kwargs)\n",
    "    return torch.optim.Adam(parameters,**kwargs)\n",
    "\n",
    "\n",
    "optimizer_param = {\"lr\":0.001}\n",
    "optimizer = build_optimizer(model.parameters(),**optimizer_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            args only have one loss function\n",
    "        \"\"\"\n",
    "        self.args = args\n",
    "\n",
    "    def build_loss(self):\n",
    "        return self.args[0]\n",
    "    \n",
    "    def calculate_loss(self,*args):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_do_transform': True,\n",
       " 'prob': 0.5,\n",
       " '_lazy': False,\n",
       " 'min_zoom': (1.0,),\n",
       " 'max_zoom': (1.2,),\n",
       " 'mode': area,\n",
       " 'padding_mode': edge,\n",
       " 'align_corners': None,\n",
       " 'dtype': torch.float32,\n",
       " 'keep_size': True,\n",
       " 'kwargs': {},\n",
       " '_zoom': [1.0]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from monai.transforms import Compose,EnsureChannelFirst,RandZoom,RandRotate,RandFlip,ToTensor\n",
    "import inspect\n",
    "transform_param = {\"transform_methods\":[\n",
    "                                EnsureChannelFirst(),\n",
    "                                # Data augmentation\n",
    "                                RandZoom(prob = 0.5, min_zoom=1.0, max_zoom=1.2),\n",
    "                                RandRotate(range_z = 0.35, prob = 0.8),\n",
    "                                RandFlip(prob = 0.5),\n",
    "                                # To tensor\n",
    "                                ToTensor()\n",
    "                                ]}\n",
    "transform_param['transform_methods'][1].__dict__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import tensor\n",
    "import torch\n",
    "import nibabel as nib\n",
    "\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from monai.transforms import Compose\n",
    "\n",
    "from monai.data import ImageDataset,DataLoader\n",
    "\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    RandZoom,\n",
    "    Compose,\n",
    "    RandRotate,\n",
    "    RandFlip,\n",
    "    RandGaussianNoise,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    "    Rand3DElastic,\n",
    "    RandSpatialCrop,\n",
    "    ScaleIntensityRange,\n",
    "    CenterSpatialCrop\n",
    "    )\n",
    "\n",
    "\n",
    "class DataFiles:\n",
    "    def __init__(self,data_path,label_path,label_name) -> None:\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.label_name = label_name\n",
    "\n",
    "    def get_images(self):\n",
    "        return [os.path.join(self.data_path, filename) for filename in os.listdir(self.data_path)]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return pd.read_csv(self.label_path)[self.label_name].values.tolist()\n",
    "\n",
    "    def Data_check(self):\n",
    "        assert len(self.get_images()) == len(self.get_labels()) , 'The number of images and labels are not equal'\n",
    "\n",
    "    def generate_data_dic(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "class Image_Dataset(ImageDataset):\n",
    "    def __init__(self,image_files,labels,transform_methods=None,data_aug=True,label_name=None,*args,**kwargs):\n",
    "\n",
    "        if data_aug:\n",
    "            transform = Compose(transform_methods)\n",
    "        else:\n",
    "            transform = None\n",
    "\n",
    "        super().__init__(image_files=image_files,labels=labels,transform=transform,*args, **kwargs)\n",
    "\n",
    "    def __getitem__(self,index,*args,**kwargs):\n",
    "        output = super().__getitem__(index,*args,**kwargs)\n",
    "\n",
    "        print(f\"this is image {self.image_files[index]};This is label {self.labels[index]}\")\n",
    "        print(f\"this is image shape {output[0].shape}\")\n",
    "        return output\n",
    "\n",
    "\n",
    "class Data_Loader(DataLoader):\n",
    "    def __init__(self,dataset,batch_size,num_workers=0,*args,**kwargs):\n",
    "        super().__init__(dataset=dataset,batch_size=batch_size,num_workers=num_workers,*args,**kwargs)\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def build_train_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=True,num_workers=self.num_workers,drop_last=True,*self.args,**self.kwargs)\n",
    "\n",
    "    def build_vali_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n",
    "    \n",
    "    def build_test_loader(self):\n",
    "        return DataLoader(self.dataset,batch_size=self.batch_size,shuffle=False,num_workers=self.num_workers,drop_last=False,*self.args,**self.kwargs)\n",
    "    \n",
    "Data = DataFiles('../../Data/CT_Phase/Resample_222/','../../Data/CT_Phase/True_Label/Phase_label_all.csv','Phase')\n",
    "images_lst = Data.get_images()\n",
    "labels_lst = Data.get_labels()\n",
    "Data.Data_check()\n",
    "\n",
    "\n",
    "transform_param = {\"transform_methods\":[\n",
    "                                    EnsureChannelFirst(),\n",
    "                                    # Data augmentation\n",
    "                                    RandZoom(prob = 0.5, min_zoom=1.0, max_zoom=1.2),\n",
    "                                    RandRotate(range_z = 0.35, prob = 0.8),\n",
    "                                    RandFlip(prob = 0.5),\n",
    "                                    # To tensor\n",
    "                                    ToTensor()\n",
    "                                    ]}\n",
    "\n",
    "dataset = Image_Dataset(image_files=images_lst,labels=labels_lst,transform_methods=transform_param['transform_methods'],data_aug=True,label_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "a = torch.tensor([0,1])\n",
    "b = [0,]\n",
    "accuracy_score(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Data_Loader(dataset=dataset,batch_size=1,num_workers=0).build_train_loader()\n",
    "len(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samuel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
