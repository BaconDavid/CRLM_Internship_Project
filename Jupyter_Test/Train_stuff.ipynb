{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "all_data = pd.read_csv('../../Data/Mixed_HGP/True_Label/scans_used_all_info.csv')\n",
    "all_labels = all_data['HGP_Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_split.split(all_data, all_labels)\n",
    "\n",
    "for train_index, test_index in stratified_split.split(all_data, all_labels):\n",
    "    strat_train_set = all_data.loc[train_index]\n",
    "    strat_test_set = all_data.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.reset_index(drop=True, inplace=True)\n",
    "strat_train_set.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set.to_csv('../../Data/Mixed_HGP/True_Label/test_set.csv', index=False)\n",
    "strat_train_set.to_csv('../../Data/Mixed_HGP/True_Label/train_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_0 = 'CILM_'\n",
    "name_1 = '_0000.nii.gz'\n",
    "train_name = strat_train_set.Experiment.tolist()\n",
    "test_name = strat_test_set.Experiment.tolist()\n",
    "train_name = [name_0  + str(i) + '0' + name_1 for i in train_name]\n",
    "test_name = [name_0  + str(i) + '0' + name_1 for i in test_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Scan</th>\n",
       "      <th>pHGP</th>\n",
       "      <th>dHGP</th>\n",
       "      <th>rHGP</th>\n",
       "      <th>HGP_Type</th>\n",
       "      <th>Series_description</th>\n",
       "      <th>acquisition_time</th>\n",
       "      <th>...</th>\n",
       "      <th>kvp</th>\n",
       "      <th>scan_options</th>\n",
       "      <th>seriesdate_y</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>model_name</th>\n",
       "      <th>patient_position</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>convolution_kernel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7152829</td>\n",
       "      <td>CRLM_248</td>\n",
       "      <td>CT_29172</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>94.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>ABD. 5/5</td>\n",
       "      <td>114902.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>079Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>292285</td>\n",
       "      <td>CRLM_005</td>\n",
       "      <td>CT_18862</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93655.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>066Y</td>\n",
       "      <td>Asteion</td>\n",
       "      <td>HFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1820851</td>\n",
       "      <td>CRLM_081</td>\n",
       "      <td>CT_29966</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.666667</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>Thx-lever 5.0 B30f</td>\n",
       "      <td>113904.38850</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040419.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Volume Zoom</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B30f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4932909</td>\n",
       "      <td>CRLM_169</td>\n",
       "      <td>CT_16877</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.428571</td>\n",
       "      <td>98.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>th.abd. alg.  5.0  B31f</td>\n",
       "      <td>104110.53570</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20050506.0</td>\n",
       "      <td>M</td>\n",
       "      <td>057Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2489453</td>\n",
       "      <td>CRLM_101</td>\n",
       "      <td>CT_87243</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>133834.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>072Y</td>\n",
       "      <td>Mx8000 IDT 16</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.0\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7739333</td>\n",
       "      <td>CRLM_264</td>\n",
       "      <td>CT_33728</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Abdomen  5.0  B31s</td>\n",
       "      <td>114908.97490</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20140409.0</td>\n",
       "      <td>M</td>\n",
       "      <td>084Y</td>\n",
       "      <td>Sensation 40</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8859793</td>\n",
       "      <td>CRLM_300</td>\n",
       "      <td>CT_21114</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3F Lever port.  5.0  B31f</td>\n",
       "      <td>84356.95843</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040604.0</td>\n",
       "      <td>M</td>\n",
       "      <td>067Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6643893</td>\n",
       "      <td>CRLM_223</td>\n",
       "      <td>CT_33488</td>\n",
       "      <td>3</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abdomen 6.0 B40s</td>\n",
       "      <td>92112.62500</td>\n",
       "      <td>...</td>\n",
       "      <td>\"110\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20111129.0</td>\n",
       "      <td>M</td>\n",
       "      <td>081Y</td>\n",
       "      <td>Emotion Duo</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"6\"</td>\n",
       "      <td>B40s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6431015</td>\n",
       "      <td>CRLM_214</td>\n",
       "      <td>CT_26150</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.666667</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>ABD. 5MM</td>\n",
       "      <td>180527.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>066Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>121369</td>\n",
       "      <td>CRLM_001</td>\n",
       "      <td>CT_33891</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>92.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101443.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20100730.0</td>\n",
       "      <td>M</td>\n",
       "      <td>077Y</td>\n",
       "      <td>Aquilion</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6019571</td>\n",
       "      <td>CRLM_203</td>\n",
       "      <td>CT_14359</td>\n",
       "      <td>10</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135724.40000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>074Y</td>\n",
       "      <td>Aquilion</td>\n",
       "      <td>HFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5879354</td>\n",
       "      <td>CRLM_197</td>\n",
       "      <td>CT_76657</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.041667</td>\n",
       "      <td>58.958333</td>\n",
       "      <td>0</td>\n",
       "      <td>C+rectaal Body 5.0 CE +C 60s C+rectaal  Axial</td>\n",
       "      <td>181913.25000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20150501.0</td>\n",
       "      <td>F</td>\n",
       "      <td>061Y</td>\n",
       "      <td>Aquilion ONE</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"5.0\"</td>\n",
       "      <td>FC09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8065953</td>\n",
       "      <td>CRLM_277</td>\n",
       "      <td>CT_94912</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Thx-lever 5.0 B30f</td>\n",
       "      <td>115353.33800</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040108.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Volume Zoom</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B30f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1637698</td>\n",
       "      <td>CRLM_071</td>\n",
       "      <td>CT_17713</td>\n",
       "      <td>4</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111412.15000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"135\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>079Y</td>\n",
       "      <td>Asteion</td>\n",
       "      <td>HFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"5.0\"</td>\n",
       "      <td>FC10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>521867</td>\n",
       "      <td>CRLM_017</td>\n",
       "      <td>CT_27354</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ABD. 5MM</td>\n",
       "      <td>110231.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>052Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8205683</td>\n",
       "      <td>CRLM_283</td>\n",
       "      <td>CT_12575</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>Thx Med/Abd 5/5</td>\n",
       "      <td>102012.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>065Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1094923</td>\n",
       "      <td>CRLM_047</td>\n",
       "      <td>CT_32703</td>\n",
       "      <td>22</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>96.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Dyn 4D Lever  1.5  Br36 16</td>\n",
       "      <td>120926.90800</td>\n",
       "      <td>...</td>\n",
       "      <td>\"80\"</td>\n",
       "      <td>['XOP', 'A4DS', '0001', 'CONT', 'RSER000001', ...</td>\n",
       "      <td>20180215.0</td>\n",
       "      <td>M</td>\n",
       "      <td>068Y</td>\n",
       "      <td>SOMATOM Force</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"1.5\"</td>\n",
       "      <td>Br36f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>788823</td>\n",
       "      <td>CRLM_031</td>\n",
       "      <td>CT_25273</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>C+ Body 5.0 CE C+ 60s C+  Axial</td>\n",
       "      <td>141336.75000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20141203.0</td>\n",
       "      <td>M</td>\n",
       "      <td>084Y</td>\n",
       "      <td>Aquilion ONE</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"5.0\"</td>\n",
       "      <td>FC09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2259297</td>\n",
       "      <td>CRLM_099</td>\n",
       "      <td>CT_29977</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO Th/Abd.  3.0  B31f</td>\n",
       "      <td>140937.45300</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20140610.0</td>\n",
       "      <td>M</td>\n",
       "      <td>069Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6746373</td>\n",
       "      <td>CRLM_230</td>\n",
       "      <td>CT_31053</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Th.abd.alg  5.0  B31f</td>\n",
       "      <td>85631.47327</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20070524.0</td>\n",
       "      <td>M</td>\n",
       "      <td>060Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6370665</td>\n",
       "      <td>CRLM_211</td>\n",
       "      <td>CT_63505</td>\n",
       "      <td>6</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110435.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>057Y</td>\n",
       "      <td>Mx8000 IDT 16</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.0\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9801055</td>\n",
       "      <td>CRLM_334</td>\n",
       "      <td>CT_89320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd  ThorAbd  2.0  I41f  3</td>\n",
       "      <td>113700.46300</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20150202.0</td>\n",
       "      <td>M</td>\n",
       "      <td>060Y</td>\n",
       "      <td>SOMATOM Definition Flash</td>\n",
       "      <td>FFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>['I41f', '3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4335562</td>\n",
       "      <td>CRLM_148</td>\n",
       "      <td>CT_26026</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Abd 5.0 mm</td>\n",
       "      <td>83455.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>067Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7845187</td>\n",
       "      <td>CRLM_265</td>\n",
       "      <td>CT_19439</td>\n",
       "      <td>6</td>\n",
       "      <td>21.944444</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>40.555556</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO ThAbd  1.5  B70f</td>\n",
       "      <td>152048.30750</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20151001.0</td>\n",
       "      <td>M</td>\n",
       "      <td>074Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"1.5\"</td>\n",
       "      <td>B70f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6874007</td>\n",
       "      <td>CRLM_240</td>\n",
       "      <td>CT_32665</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd 5.0 mm</td>\n",
       "      <td>141349.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>054Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2995387</td>\n",
       "      <td>CRLM_106</td>\n",
       "      <td>CT_26568</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>ABD AX 4MM</td>\n",
       "      <td>155645.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>054Y</td>\n",
       "      <td>Brilliance 40</td>\n",
       "      <td>HFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"4.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6676629</td>\n",
       "      <td>CRLM_225</td>\n",
       "      <td>CT_50926</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd 5.0 mm</td>\n",
       "      <td>125838.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>067Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9203144</td>\n",
       "      <td>CRLM_315</td>\n",
       "      <td>CT_63660</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Th. tumor 5.0 B30f</td>\n",
       "      <td>135349.43150</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040129.0</td>\n",
       "      <td>F</td>\n",
       "      <td>067Y</td>\n",
       "      <td>Volume Zoom</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B30f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2469868</td>\n",
       "      <td>CRLM_100</td>\n",
       "      <td>CT_26579</td>\n",
       "      <td>4</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>thorax-lever 5.0 B30f</td>\n",
       "      <td>94926.99051</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20060426.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Volume Zoom</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B30f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4327749</td>\n",
       "      <td>CRLM_147</td>\n",
       "      <td>CT_10043</td>\n",
       "      <td>2</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ABDOMEN 2/2</td>\n",
       "      <td>141238.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx8000 IDT 10</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.00\"</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1079460</td>\n",
       "      <td>CRLM_045</td>\n",
       "      <td>CT_80259</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>96.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>D70 Abd. alg.  3.0  Br40  3</td>\n",
       "      <td>102158.15000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"110\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20180123.0</td>\n",
       "      <td>M</td>\n",
       "      <td>064Y</td>\n",
       "      <td>SOMATOM Edge Plus</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>['Br40f', '3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4033021</td>\n",
       "      <td>CRLM_141</td>\n",
       "      <td>CT_12276</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144225.95000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>083Y</td>\n",
       "      <td>Aquilion</td>\n",
       "      <td>HFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>8408989</td>\n",
       "      <td>CRLM_290</td>\n",
       "      <td>CT_99400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>th abd  3.0  B31f</td>\n",
       "      <td>155048.40080</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20130301.0</td>\n",
       "      <td>M</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3092569</td>\n",
       "      <td>CRLM_110</td>\n",
       "      <td>CT_82801</td>\n",
       "      <td>1497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113831.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>086Y</td>\n",
       "      <td>Mx8000</td>\n",
       "      <td>HFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"6.5\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3227315</td>\n",
       "      <td>CRLM_114</td>\n",
       "      <td>CT_22889</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sft Tissue 3.0 VISIPAQUE/320 90CC CE</td>\n",
       "      <td>122732.45000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20090106.0</td>\n",
       "      <td>M</td>\n",
       "      <td>054Y</td>\n",
       "      <td>Aquilion</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2006537</td>\n",
       "      <td>CRLM_088</td>\n",
       "      <td>CT_24329</td>\n",
       "      <td>2-CT2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Thx/Abd   c.m 4.0 B40f</td>\n",
       "      <td>115624.97100</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20100119.0</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Volume Zoom</td>\n",
       "      <td>FFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"4\"</td>\n",
       "      <td>B40f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2972809</td>\n",
       "      <td>CRLM_105</td>\n",
       "      <td>CT_21529</td>\n",
       "      <td>2</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Thorax 3mm</td>\n",
       "      <td>133916.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>046Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"3.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4358305</td>\n",
       "      <td>CRLM_151</td>\n",
       "      <td>CT_10954</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ONCO Th/Abd.  1.5  B70f</td>\n",
       "      <td>85505.58880</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20121219.0</td>\n",
       "      <td>M</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"1.5\"</td>\n",
       "      <td>B70f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5562239</td>\n",
       "      <td>CRLM_185</td>\n",
       "      <td>CT_27741</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>THO/ABD 70SEC PI</td>\n",
       "      <td>92356.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>073Y</td>\n",
       "      <td>iCT 256</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7326641</td>\n",
       "      <td>CRLM_254</td>\n",
       "      <td>CT_19187</td>\n",
       "      <td>5</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>59.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91231.15000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20120601.0</td>\n",
       "      <td>M</td>\n",
       "      <td>076Y</td>\n",
       "      <td>Aquilion</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"3.0\"</td>\n",
       "      <td>FC02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9247015</td>\n",
       "      <td>CRLM_317</td>\n",
       "      <td>CT_94131</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ONCO Th/Abd.  3.0  B31f</td>\n",
       "      <td>120739.99420</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20140214.0</td>\n",
       "      <td>M</td>\n",
       "      <td>070Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3429629</td>\n",
       "      <td>CRLM_121</td>\n",
       "      <td>CT_12724</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Abd 5.0 mm</td>\n",
       "      <td>91304.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>069Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>6818250</td>\n",
       "      <td>CRLM_234</td>\n",
       "      <td>CT_11656</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3F Panc. ven.  5.0  B20f</td>\n",
       "      <td>102104.97440</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040728.0</td>\n",
       "      <td>F</td>\n",
       "      <td>065Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B20f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>6562966</td>\n",
       "      <td>CRLM_221</td>\n",
       "      <td>CT_80994</td>\n",
       "      <td>2</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd. algemeen  3.0  B31f</td>\n",
       "      <td>211154.92100</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20120416.0</td>\n",
       "      <td>F</td>\n",
       "      <td>054Y</td>\n",
       "      <td>SOMATOM Definition Flash</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4892516</td>\n",
       "      <td>CRLM_166</td>\n",
       "      <td>CT_26366</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO Th/Abd.  1.5  B70f</td>\n",
       "      <td>95224.49210</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20130214.0</td>\n",
       "      <td>F</td>\n",
       "      <td>052Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"1.5\"</td>\n",
       "      <td>B70f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5800783</td>\n",
       "      <td>CRLM_192</td>\n",
       "      <td>CT_23433</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>th.tum-abd alg.  2.0  B31f</td>\n",
       "      <td>102029.80000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20040929.0</td>\n",
       "      <td>M</td>\n",
       "      <td>057Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>9759331</td>\n",
       "      <td>CRLM_333</td>\n",
       "      <td>CT_40034</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd 5.0 mm</td>\n",
       "      <td>92656.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>072Y</td>\n",
       "      <td>Brilliance 64</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"5.00\"</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9278300</td>\n",
       "      <td>CRLM_318</td>\n",
       "      <td>CT_23332</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.750000</td>\n",
       "      <td>86.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>D70 Onco ThAb  3.0  I70f  3</td>\n",
       "      <td>161956.41600</td>\n",
       "      <td>...</td>\n",
       "      <td>\"100\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20160921.0</td>\n",
       "      <td>F</td>\n",
       "      <td>064Y</td>\n",
       "      <td>SOMATOM Definition Edge</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>['I70f', '3']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>6455598</td>\n",
       "      <td>CRLM_216</td>\n",
       "      <td>CT_18041</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>99.166667</td>\n",
       "      <td>0</td>\n",
       "      <td>ABDOMEN 2/2</td>\n",
       "      <td>143237.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F</td>\n",
       "      <td>058Y</td>\n",
       "      <td>Mx8000 IDT 10</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.00\"</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3563254</td>\n",
       "      <td>CRLM_125</td>\n",
       "      <td>CT_20191</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO Th/Abd.  3.0  B31f</td>\n",
       "      <td>81611.65119</td>\n",
       "      <td>...</td>\n",
       "      <td>\"120\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20131004.0</td>\n",
       "      <td>F</td>\n",
       "      <td>049Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PID   Subject Experiment   Scan       pHGP        dHGP        rHGP  \\\n",
       "0   7152829  CRLM_248   CT_29172      4   0.000000    5.833333   94.166667   \n",
       "1    292285  CRLM_005   CT_18862      4   0.000000    3.333333   96.666667   \n",
       "2   1820851  CRLM_081   CT_29966      2   0.000000   31.666667   68.333333   \n",
       "3   4932909  CRLM_169   CT_16877      2   0.000000    1.428571   98.571429   \n",
       "4   2489453  CRLM_101   CT_87243      7   0.000000   41.666667   58.333333   \n",
       "5   7739333  CRLM_264   CT_33728      7   0.000000  100.000000    0.000000   \n",
       "6   8859793  CRLM_300   CT_21114      4   0.000000   30.000000   70.000000   \n",
       "7   6643893  CRLM_223   CT_33488      3   5.000000   95.000000    0.000000   \n",
       "8   6431015  CRLM_214   CT_26150      2   0.000000   56.666667   43.333333   \n",
       "9    121369  CRLM_001   CT_33891      5   0.000000    7.500000   92.500000   \n",
       "10  6019571  CRLM_203   CT_14359     10  15.000000    0.000000   85.000000   \n",
       "11  5879354  CRLM_197   CT_76657      2   0.000000   41.041667   58.958333   \n",
       "12  8065953  CRLM_277   CT_94912      2   0.000000  100.000000    0.000000   \n",
       "13  1637698  CRLM_071   CT_17713      4  65.000000    0.000000   35.000000   \n",
       "14   521867  CRLM_017   CT_27354      2   0.000000  100.000000    0.000000   \n",
       "15  8205683  CRLM_283   CT_12575      2   0.000000   12.500000   87.500000   \n",
       "16  1094923  CRLM_047   CT_32703     22   3.125000   96.875000    0.000000   \n",
       "17   788823  CRLM_031   CT_25273      5   0.000000    0.000000  100.000000   \n",
       "18  2259297  CRLM_099   CT_29977      3   0.000000   95.000000    5.000000   \n",
       "19  6746373  CRLM_230   CT_31053      3   0.000000    0.000000  100.000000   \n",
       "20  6370665  CRLM_211   CT_63505      6   1.250000   70.000000   28.750000   \n",
       "21  9801055  CRLM_334   CT_89320      3   0.000000   65.000000   35.000000   \n",
       "22  4335562  CRLM_148   CT_26026      3   0.000000  100.000000    0.000000   \n",
       "23  7845187  CRLM_265   CT_19439      6  21.944444   37.500000   40.555556   \n",
       "24  6874007  CRLM_240   CT_32665      3   0.000000   12.500000   87.500000   \n",
       "25  2995387  CRLM_106   CT_26568      3   0.000000   76.666667   23.333333   \n",
       "26  6676629  CRLM_225   CT_50926      3   0.000000    5.000000   95.000000   \n",
       "27  9203144  CRLM_315   CT_63660      2   0.000000   30.000000   70.000000   \n",
       "28  2469868  CRLM_100   CT_26579      4   1.666667   36.666667   61.666667   \n",
       "29  4327749  CRLM_147   CT_10043      2  32.500000   22.500000   45.000000   \n",
       "30  1079460  CRLM_045   CT_80259      6   0.000000   96.666667    3.333333   \n",
       "31  4033021  CRLM_141   CT_12276      9   0.000000   95.000000    5.000000   \n",
       "32  8408989  CRLM_290   CT_99400      9   0.000000  100.000000    0.000000   \n",
       "33  3092569  CRLM_110   CT_82801   1497   0.000000  100.000000    0.000000   \n",
       "34  3227315  CRLM_114   CT_22889      3   0.000000  100.000000    0.000000   \n",
       "35  2006537  CRLM_088   CT_24329  2-CT2   0.000000    0.000000  100.000000   \n",
       "36  2972809  CRLM_105   CT_21529      2   5.000000   90.000000    5.000000   \n",
       "37  4358305  CRLM_151   CT_10954      6   0.000000  100.000000    0.000000   \n",
       "38  5562239  CRLM_185   CT_27741      2   0.000000    2.500000   97.500000   \n",
       "39  7326641  CRLM_254   CT_19187      5   3.333333   37.500000   59.166667   \n",
       "40  9247015  CRLM_317   CT_94131      3   0.000000  100.000000    0.000000   \n",
       "41  3429629  CRLM_121   CT_12724      2   0.000000  100.000000    0.000000   \n",
       "42  6818250  CRLM_234   CT_11656      4   0.000000    5.000000   95.000000   \n",
       "43  6562966  CRLM_221   CT_80994      2  67.500000   26.250000    6.250000   \n",
       "44  4892516  CRLM_166   CT_26366      6   0.000000    0.000000  100.000000   \n",
       "45  5800783  CRLM_192   CT_23433      4   5.000000   80.000000   15.000000   \n",
       "46  9759331  CRLM_333   CT_40034      5   0.000000    6.666667   93.333333   \n",
       "47  9278300  CRLM_318   CT_23332      4   0.000000   13.750000   86.250000   \n",
       "48  6455598  CRLM_216   CT_18041      5   0.000000    0.833333   99.166667   \n",
       "49  3563254  CRLM_125   CT_20191      3   0.000000   32.500000   67.500000   \n",
       "\n",
       "    HGP_Type                             Series_description  acquisition_time  \\\n",
       "0          0                                       ABD. 5/5      114902.00000   \n",
       "1          0                                            NaN       93655.75000   \n",
       "2          0                             Thx-lever 5.0 B30f      113904.38850   \n",
       "3          0                        th.abd. alg.  5.0  B31f      104110.53570   \n",
       "4          0                                            NaN      133834.00000   \n",
       "5          1                             Abdomen  5.0  B31s      114908.97490   \n",
       "6          0                      3F Lever port.  5.0  B31f       84356.95843   \n",
       "7          0                               Abdomen 6.0 B40s       92112.62500   \n",
       "8          0                                       ABD. 5MM      180527.00000   \n",
       "9          0                                            NaN      101443.00000   \n",
       "10         0                                            NaN      135724.40000   \n",
       "11         0  C+rectaal Body 5.0 CE +C 60s C+rectaal  Axial      181913.25000   \n",
       "12         1                             Thx-lever 5.0 B30f      115353.33800   \n",
       "13         0                                            NaN      111412.15000   \n",
       "14         1                                       ABD. 5MM      110231.00000   \n",
       "15         0                                Thx Med/Abd 5/5      102012.00000   \n",
       "16         0                     Dyn 4D Lever  1.5  Br36 16      120926.90800   \n",
       "17         0                C+ Body 5.0 CE C+ 60s C+  Axial      141336.75000   \n",
       "18         0                        ONCO Th/Abd.  3.0  B31f      140937.45300   \n",
       "19         0                          Th.abd.alg  5.0  B31f       85631.47327   \n",
       "20         0                                            NaN      110435.00000   \n",
       "21         0                     Abd  ThorAbd  2.0  I41f  3      113700.46300   \n",
       "22         1                                     Abd 5.0 mm       83455.00000   \n",
       "23         0                          ONCO ThAbd  1.5  B70f      152048.30750   \n",
       "24         0                                     Abd 5.0 mm      141349.00000   \n",
       "25         0                                     ABD AX 4MM      155645.00000   \n",
       "26         0                                     Abd 5.0 mm      125838.00000   \n",
       "27         0                             Th. tumor 5.0 B30f      135349.43150   \n",
       "28         0                          thorax-lever 5.0 B30f       94926.99051   \n",
       "29         0                                    ABDOMEN 2/2      141238.00000   \n",
       "30         0                    D70 Abd. alg.  3.0  Br40  3      102158.15000   \n",
       "31         0                                            NaN      144225.95000   \n",
       "32         1                              th abd  3.0  B31f      155048.40080   \n",
       "33         1                                            NaN      113831.00000   \n",
       "34         1           Sft Tissue 3.0 VISIPAQUE/320 90CC CE      122732.45000   \n",
       "35         0                         Thx/Abd   c.m 4.0 B40f      115624.97100   \n",
       "36         0                                     Thorax 3mm      133916.00000   \n",
       "37         1                        ONCO Th/Abd.  1.5  B70f       85505.58880   \n",
       "38         0                               THO/ABD 70SEC PI       92356.00000   \n",
       "39         0                                            NaN       91231.15000   \n",
       "40         1                        ONCO Th/Abd.  3.0  B31f      120739.99420   \n",
       "41         1                                     Abd 5.0 mm       91304.00000   \n",
       "42         0                       3F Panc. ven.  5.0  B20f      102104.97440   \n",
       "43         0                       Abd. algemeen  3.0  B31f      211154.92100   \n",
       "44         0                        ONCO Th/Abd.  1.5  B70f       95224.49210   \n",
       "45         0                     th.tum-abd alg.  2.0  B31f      102029.80000   \n",
       "46         0                                     Abd 5.0 mm       92656.00000   \n",
       "47         0                    D70 Onco ThAb  3.0  I70f  3      161956.41600   \n",
       "48         0                                    ABDOMEN 2/2      143237.00000   \n",
       "49         0                        ONCO Th/Abd.  3.0  B31f       81611.65119   \n",
       "\n",
       "    ...    kvp                                       scan_options  \\\n",
       "0   ...  \"120\"                                              HELIX   \n",
       "1   ...  \"120\"                                         HELICAL_CT   \n",
       "2   ...  \"120\"                                                NaN   \n",
       "3   ...  \"120\"                                                NaN   \n",
       "4   ...  \"120\"                                              HELIX   \n",
       "5   ...  \"120\"                                                NaN   \n",
       "6   ...  \"120\"                                                NaN   \n",
       "7   ...  \"110\"                                                NaN   \n",
       "8   ...  \"120\"                                              HELIX   \n",
       "9   ...  \"120\"                                         HELICAL_CT   \n",
       "10  ...  \"120\"                                         HELICAL_CT   \n",
       "11  ...  \"120\"                                         HELICAL_CT   \n",
       "12  ...  \"120\"                                                NaN   \n",
       "13  ...  \"135\"                                         HELICAL_CT   \n",
       "14  ...  \"120\"                                              HELIX   \n",
       "15  ...  \"120\"                                              HELIX   \n",
       "16  ...   \"80\"  ['XOP', 'A4DS', '0001', 'CONT', 'RSER000001', ...   \n",
       "17  ...  \"120\"                                         HELICAL_CT   \n",
       "18  ...  \"120\"                                                NaN   \n",
       "19  ...  \"120\"                                                NaN   \n",
       "20  ...  \"120\"                                              HELIX   \n",
       "21  ...  \"120\"                                                NaN   \n",
       "22  ...  \"120\"                                              HELIX   \n",
       "23  ...  \"120\"                                                NaN   \n",
       "24  ...  \"100\"                                              HELIX   \n",
       "25  ...  \"120\"                                              HELIX   \n",
       "26  ...  \"100\"                                              HELIX   \n",
       "27  ...  \"120\"                                                NaN   \n",
       "28  ...  \"120\"                                                NaN   \n",
       "29  ...  \"120\"                                              HELIX   \n",
       "30  ...  \"110\"                                                NaN   \n",
       "31  ...  \"120\"                                         HELICAL_CT   \n",
       "32  ...  \"120\"                                                NaN   \n",
       "33  ...  \"120\"                                              HELIX   \n",
       "34  ...  \"120\"                                         HELICAL_CT   \n",
       "35  ...  \"120\"                                                NaN   \n",
       "36  ...  \"100\"                                              HELIX   \n",
       "37  ...  \"120\"                                                NaN   \n",
       "38  ...  \"100\"                                              HELIX   \n",
       "39  ...  \"120\"                                         HELICAL_CT   \n",
       "40  ...  \"100\"                                                NaN   \n",
       "41  ...  \"100\"                                              HELIX   \n",
       "42  ...  \"120\"                                                NaN   \n",
       "43  ...  \"120\"                                                NaN   \n",
       "44  ...  \"120\"                                                NaN   \n",
       "45  ...  \"120\"                                                NaN   \n",
       "46  ...  \"100\"                                              HELIX   \n",
       "47  ...  \"100\"                                                NaN   \n",
       "48  ...  \"120\"                                              HELIX   \n",
       "49  ...  \"120\"                                                NaN   \n",
       "\n",
       "   seriesdate_y gender   age                model_name  patient_position  \\\n",
       "0           NaN      M  079Y             Brilliance 64               FFS   \n",
       "1           NaN      M  066Y                   Asteion               HFS   \n",
       "2    20040419.0      M   NaN               Volume Zoom               HFS   \n",
       "3    20050506.0      M  057Y              Sensation 16               HFS   \n",
       "4           NaN      M  072Y             Mx8000 IDT 16               FFS   \n",
       "5    20140409.0      M  084Y              Sensation 40               HFS   \n",
       "6    20040604.0      M  067Y              Sensation 16               HFS   \n",
       "7    20111129.0      M  081Y               Emotion Duo               HFS   \n",
       "8           NaN      M  066Y             Brilliance 64               FFS   \n",
       "9    20100730.0      M  077Y                  Aquilion               FFS   \n",
       "10          NaN      M  074Y                  Aquilion               HFS   \n",
       "11   20150501.0      F  061Y              Aquilion ONE               FFS   \n",
       "12   20040108.0      M   NaN               Volume Zoom               HFS   \n",
       "13          NaN      F  079Y                   Asteion               HFS   \n",
       "14          NaN      M  052Y             Brilliance 64               FFS   \n",
       "15          NaN      M  065Y             Brilliance 64               FFS   \n",
       "16   20180215.0      M  068Y             SOMATOM Force               HFS   \n",
       "17   20141203.0      M  084Y              Aquilion ONE               FFS   \n",
       "18   20140610.0      M  069Y              Sensation 64               HFS   \n",
       "19   20070524.0      M  060Y              Sensation 16               HFS   \n",
       "20          NaN      M  057Y             Mx8000 IDT 16               FFS   \n",
       "21   20150202.0      M  060Y  SOMATOM Definition Flash               FFS   \n",
       "22          NaN      F  067Y             Brilliance 64               FFS   \n",
       "23   20151001.0      M  074Y              Sensation 64               HFS   \n",
       "24          NaN      M  054Y             Brilliance 64               FFS   \n",
       "25          NaN      M  054Y             Brilliance 40               HFS   \n",
       "26          NaN      M  067Y             Brilliance 64               FFS   \n",
       "27   20040129.0      F  067Y               Volume Zoom               HFS   \n",
       "28   20060426.0      F   NaN               Volume Zoom               HFS   \n",
       "29          NaN      M   NaN             Mx8000 IDT 10               FFS   \n",
       "30   20180123.0      M  064Y         SOMATOM Edge Plus               HFS   \n",
       "31          NaN      M  083Y                  Aquilion               HFS   \n",
       "32   20130301.0      M  073Y              Sensation 64               HFS   \n",
       "33          NaN      M  086Y                    Mx8000               HFS   \n",
       "34   20090106.0      M  054Y                  Aquilion               FFS   \n",
       "35   20100119.0      M   NaN               Volume Zoom               FFS   \n",
       "36          NaN      M  046Y             Brilliance 64               FFS   \n",
       "37   20121219.0      M  073Y              Sensation 64               HFS   \n",
       "38          NaN      M  073Y                   iCT 256               FFS   \n",
       "39   20120601.0      M  076Y                  Aquilion               FFS   \n",
       "40   20140214.0      M  070Y              Sensation 64               HFS   \n",
       "41          NaN      M  069Y             Brilliance 64               FFS   \n",
       "42   20040728.0      F  065Y              Sensation 16               HFS   \n",
       "43   20120416.0      F  054Y  SOMATOM Definition Flash               HFS   \n",
       "44   20130214.0      F  052Y              Sensation 64               HFS   \n",
       "45   20040929.0      M  057Y              Sensation 16               HFS   \n",
       "46          NaN      M  072Y             Brilliance 64               FFS   \n",
       "47   20160921.0      F  064Y   SOMATOM Definition Edge               HFS   \n",
       "48          NaN      F  058Y             Mx8000 IDT 10               FFS   \n",
       "49   20131004.0      F  049Y              Sensation 64               HFS   \n",
       "\n",
       "    manufacturer  slice_thickness  convolution_kernel  \n",
       "0        Philips           \"5.00\"                   B  \n",
       "1        TOSHIBA            \"3.0\"                FC10  \n",
       "2        SIEMENS              \"5\"                B30f  \n",
       "3        SIEMENS              \"5\"                B31f  \n",
       "4        Philips            \"2.0\"                   B  \n",
       "5        SIEMENS              \"5\"                B31s  \n",
       "6        SIEMENS              \"5\"                B31f  \n",
       "7        SIEMENS              \"6\"                B40s  \n",
       "8        Philips           \"5.00\"                   B  \n",
       "9        TOSHIBA            \"3.0\"                FC02  \n",
       "10       TOSHIBA            \"3.0\"                FC10  \n",
       "11       TOSHIBA            \"5.0\"                FC09  \n",
       "12       SIEMENS              \"5\"                B30f  \n",
       "13       TOSHIBA            \"5.0\"                FC10  \n",
       "14       Philips           \"5.00\"                   B  \n",
       "15       Philips           \"5.00\"                   B  \n",
       "16       SIEMENS            \"1.5\"               Br36f  \n",
       "17       TOSHIBA            \"5.0\"                FC09  \n",
       "18       SIEMENS              \"3\"                B31f  \n",
       "19       SIEMENS              \"5\"                B31f  \n",
       "20       Philips            \"2.0\"                   B  \n",
       "21       SIEMENS              \"2\"       ['I41f', '3']  \n",
       "22       Philips           \"5.00\"                   B  \n",
       "23       SIEMENS            \"1.5\"                B70f  \n",
       "24       Philips           \"5.00\"                   B  \n",
       "25       Philips           \"4.00\"                   B  \n",
       "26       Philips           \"5.00\"                   B  \n",
       "27       SIEMENS              \"5\"                B30f  \n",
       "28       SIEMENS              \"5\"                B30f  \n",
       "29       Philips           \"2.00\"                   C  \n",
       "30       SIEMENS              \"3\"      ['Br40f', '3']  \n",
       "31       TOSHIBA            \"3.0\"                FC10  \n",
       "32       SIEMENS              \"3\"                B31f  \n",
       "33       Philips            \"6.5\"                   B  \n",
       "34       TOSHIBA            \"3.0\"                FC13  \n",
       "35       SIEMENS              \"4\"                B40f  \n",
       "36       Philips           \"3.00\"                   B  \n",
       "37       SIEMENS            \"1.5\"                B70f  \n",
       "38       Philips           \"2.00\"                   B  \n",
       "39       TOSHIBA            \"3.0\"                FC02  \n",
       "40       SIEMENS              \"3\"                B31f  \n",
       "41       Philips           \"5.00\"                   B  \n",
       "42       SIEMENS              \"5\"                B20f  \n",
       "43       SIEMENS              \"3\"                B31f  \n",
       "44       SIEMENS            \"1.5\"                B70f  \n",
       "45       SIEMENS              \"2\"                B31f  \n",
       "46       Philips           \"5.00\"                   B  \n",
       "47       SIEMENS              \"3\"       ['I70f', '3']  \n",
       "48       Philips           \"2.00\"                   C  \n",
       "49       SIEMENS              \"3\"                B31f  \n",
       "\n",
       "[50 rows x 50 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move these files from the original folder to the new folder\n",
    "import shutil\n",
    "import os\n",
    "for i in test_name:\n",
    "    shutil.copy('../../Data/Mixed_HGP/Mixed_HGP_Only_Liver_07073_Windowed/' + i, '../../Data/Mixed_HGP/Mixed_HGP_Only_Liver_07073_Windowed_Test/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "stratify_kfold = StratifiedKFold(n_splits=5,shuffle=True,random_state=42)\n",
    "data_info = pd.read_csv('../../Data/Mixed_HGP/True_Label/scans_used_all_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = data_info['HGP_Type']\n",
    "for i,(tr_idx,val_idx) in enumerate(stratify_kfold.split(data_info,label)):\n",
    "    train_data = data_info.loc[tr_idx]\n",
    "    val_data = data_info.loc[val_idx]\n",
    "    train_data.to_csv('../../Data/Mixed_HGP/True_Label/train_cv_'+str(i)+'.csv',index=False)\n",
    "    val_data.to_csv('../../Data/Mixed_HGP/True_Label/val_cv_'+str(i)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add data path in each cv in csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Scan</th>\n",
       "      <th>pHGP</th>\n",
       "      <th>dHGP</th>\n",
       "      <th>rHGP</th>\n",
       "      <th>HGP_Type</th>\n",
       "      <th>Series_description</th>\n",
       "      <th>acquisition_time</th>\n",
       "      <th>...</th>\n",
       "      <th>scan_options</th>\n",
       "      <th>seriesdate_y</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>model_name</th>\n",
       "      <th>patient_position</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>convolution_kernel</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3932332</td>\n",
       "      <td>CRLM_137</td>\n",
       "      <td>CT_10033</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Lever meta's  2.0  B31f</td>\n",
       "      <td>84259.93629</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20060906.0</td>\n",
       "      <td>F</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4327749</td>\n",
       "      <td>CRLM_147</td>\n",
       "      <td>CT_10043</td>\n",
       "      <td>2</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ABDOMEN 2/2</td>\n",
       "      <td>141238.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx8000 IDT 10</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.00\"</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1030425</td>\n",
       "      <td>CRLM_042</td>\n",
       "      <td>CT_10104</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>A10 abdomen veneus 5.0 weke delen axial</td>\n",
       "      <td>104531.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>HELICAL_CT</td>\n",
       "      <td>20170614.0</td>\n",
       "      <td>M</td>\n",
       "      <td>061Y</td>\n",
       "      <td>Aquilion ONE</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>FC09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3985131</td>\n",
       "      <td>CRLM_139</td>\n",
       "      <td>CT_10203</td>\n",
       "      <td>7</td>\n",
       "      <td>27.555556</td>\n",
       "      <td>72.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Th-Abd. alg.  2.0  B20f</td>\n",
       "      <td>142730.55920</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20060131.0</td>\n",
       "      <td>M</td>\n",
       "      <td>061Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>B20f</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8202397</td>\n",
       "      <td>CRLM_282</td>\n",
       "      <td>CT_10702</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>98.888889</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO ThAbd  3.0  B31f</td>\n",
       "      <td>115626.45590</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20170921.0</td>\n",
       "      <td>M</td>\n",
       "      <td>071Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>8488669</td>\n",
       "      <td>CRLM_292</td>\n",
       "      <td>CT_89092</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115132.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>HELIX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>064Y</td>\n",
       "      <td>Mx8000 IDT 16</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.0\"</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>9801055</td>\n",
       "      <td>CRLM_334</td>\n",
       "      <td>CT_89320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd  ThorAbd  2.0  I41f  3</td>\n",
       "      <td>113700.46300</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20150202.0</td>\n",
       "      <td>M</td>\n",
       "      <td>060Y</td>\n",
       "      <td>SOMATOM Definition Flash</td>\n",
       "      <td>FFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>['I41f', '3']</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>9247015</td>\n",
       "      <td>CRLM_317</td>\n",
       "      <td>CT_94131</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ONCO Th/Abd.  3.0  B31f</td>\n",
       "      <td>120739.99420</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20140214.0</td>\n",
       "      <td>M</td>\n",
       "      <td>070Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>3835842</td>\n",
       "      <td>CRLM_129</td>\n",
       "      <td>CT_97575</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Thorax  5.0  B31f</td>\n",
       "      <td>91040.23781</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20141016.0</td>\n",
       "      <td>F</td>\n",
       "      <td>071Y</td>\n",
       "      <td>Sensation 40</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>8408989</td>\n",
       "      <td>CRLM_290</td>\n",
       "      <td>CT_99400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>th abd  3.0  B31f</td>\n",
       "      <td>155048.40080</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20130301.0</td>\n",
       "      <td>M</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PID   Subject Experiment Scan       pHGP        dHGP        rHGP  \\\n",
       "0    3932332  CRLM_137   CT_10033    4   0.000000   30.000000   70.000000   \n",
       "1    4327749  CRLM_147   CT_10043    2  32.500000   22.500000   45.000000   \n",
       "2    1030425  CRLM_042   CT_10104    2   0.000000    0.000000  100.000000   \n",
       "3    3985131  CRLM_139   CT_10203    7  27.555556   72.444444    0.000000   \n",
       "4    8202397  CRLM_282   CT_10702    2   0.000000    1.111111   98.888889   \n",
       "..       ...       ...        ...  ...        ...         ...         ...   \n",
       "242  8488669  CRLM_292   CT_89092    5   0.000000  100.000000    0.000000   \n",
       "243  9801055  CRLM_334   CT_89320    3   0.000000   65.000000   35.000000   \n",
       "244  9247015  CRLM_317   CT_94131    3   0.000000  100.000000    0.000000   \n",
       "246  3835842  CRLM_129   CT_97575    2   0.000000  100.000000    0.000000   \n",
       "247  8408989  CRLM_290   CT_99400    9   0.000000  100.000000    0.000000   \n",
       "\n",
       "     HGP_Type                       Series_description  acquisition_time  ...  \\\n",
       "0           0                  Lever meta's  2.0  B31f       84259.93629  ...   \n",
       "1           0                              ABDOMEN 2/2      141238.00000  ...   \n",
       "2           0  A10 abdomen veneus 5.0 weke delen axial      104531.00000  ...   \n",
       "3           0                  Th-Abd. alg.  2.0  B20f      142730.55920  ...   \n",
       "4           0                    ONCO ThAbd  3.0  B31f      115626.45590  ...   \n",
       "..        ...                                      ...               ...  ...   \n",
       "242         1                                      NaN      115132.00000  ...   \n",
       "243         0               Abd  ThorAbd  2.0  I41f  3      113700.46300  ...   \n",
       "244         1                  ONCO Th/Abd.  3.0  B31f      120739.99420  ...   \n",
       "246         1                        Thorax  5.0  B31f       91040.23781  ...   \n",
       "247         1                        th abd  3.0  B31f      155048.40080  ...   \n",
       "\n",
       "     scan_options  seriesdate_y gender   age                model_name  \\\n",
       "0             NaN    20060906.0      F  073Y              Sensation 16   \n",
       "1           HELIX           NaN      M   NaN             Mx8000 IDT 10   \n",
       "2      HELICAL_CT    20170614.0      M  061Y              Aquilion ONE   \n",
       "3             NaN    20060131.0      M  061Y              Sensation 16   \n",
       "4             NaN    20170921.0      M  071Y              Sensation 64   \n",
       "..            ...           ...    ...   ...                       ...   \n",
       "242         HELIX           NaN      M  064Y             Mx8000 IDT 16   \n",
       "243           NaN    20150202.0      M  060Y  SOMATOM Definition Flash   \n",
       "244           NaN    20140214.0      M  070Y              Sensation 64   \n",
       "246           NaN    20141016.0      F  071Y              Sensation 40   \n",
       "247           NaN    20130301.0      M  073Y              Sensation 64   \n",
       "\n",
       "     patient_position  manufacturer  slice_thickness  convolution_kernel  \\\n",
       "0                 HFS       SIEMENS              \"2\"                B31f   \n",
       "1                 FFS       Philips           \"2.00\"                   C   \n",
       "2                 FFS       TOSHIBA              \"5\"                FC09   \n",
       "3                 HFS       SIEMENS              \"2\"                B20f   \n",
       "4                 HFS       SIEMENS              \"3\"                B31f   \n",
       "..                ...           ...              ...                 ...   \n",
       "242               FFS       Philips            \"2.0\"                   B   \n",
       "243               FFS       SIEMENS              \"2\"       ['I41f', '3']   \n",
       "244               HFS       SIEMENS              \"3\"                B31f   \n",
       "246               HFS       SIEMENS              \"5\"                B31f   \n",
       "247               HFS       SIEMENS              \"3\"                B31f   \n",
       "\n",
       "     kfold  \n",
       "0      3.0  \n",
       "1      2.0  \n",
       "2      0.0  \n",
       "3      1.0  \n",
       "4      3.0  \n",
       "..     ...  \n",
       "242    1.0  \n",
       "243    2.0  \n",
       "244    3.0  \n",
       "246    0.0  \n",
       "247    0.0  \n",
       "\n",
       "[199 rows x 51 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prefix = 'CILM_'\n",
    "Suffix = '0_0000.nii.gz'\n",
    "for i in range(5):\n",
    "    train_data = pd.read_csv('../../Data/Mixed_HGP/True_Label/train_cv_'+str(i)+'.csv')\n",
    "    val_data = pd.read_csv('../../Data/Mixed_HGP/True_Label/val_cv_'+str(i)+'.csv')\n",
    "    train_name = train_data.Experiment.tolist()\n",
    "    val_name = val_data.Experiment.tolist()\n",
    "    train_name = [Prefix  + str(i) + Suffix for i in train_name]\n",
    "    val_name = [Prefix + str(i) + Suffix for i in val_name]\n",
    "    train_data['data_path'] = train_name\n",
    "    val_data['data_path'] = val_name\n",
    "    train_data.to_csv('../../Data/Mixed_HGP/True_Label/train_cv_'+str(i)+'.csv',index=False)\n",
    "    val_data.to_csv('../../Data/Mixed_HGP/True_Label/val_cv_'+str(i)+'.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Scan</th>\n",
       "      <th>pHGP</th>\n",
       "      <th>dHGP</th>\n",
       "      <th>rHGP</th>\n",
       "      <th>HGP_Type</th>\n",
       "      <th>Series_description</th>\n",
       "      <th>acquisition_time</th>\n",
       "      <th>...</th>\n",
       "      <th>seriesdate_y</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>model_name</th>\n",
       "      <th>patient_position</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>slice_thickness</th>\n",
       "      <th>convolution_kernel</th>\n",
       "      <th>kfold</th>\n",
       "      <th>data_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3932332</td>\n",
       "      <td>CRLM_137</td>\n",
       "      <td>CT_10033</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Lever meta's  2.0  B31f</td>\n",
       "      <td>84259.93629</td>\n",
       "      <td>...</td>\n",
       "      <td>20060906.0</td>\n",
       "      <td>F</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CILM_CT_100330_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4327749</td>\n",
       "      <td>CRLM_147</td>\n",
       "      <td>CT_10043</td>\n",
       "      <td>2</td>\n",
       "      <td>32.500000</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>ABDOMEN 2/2</td>\n",
       "      <td>141238.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mx8000 IDT 10</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.00\"</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CILM_CT_100430_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1030425</td>\n",
       "      <td>CRLM_042</td>\n",
       "      <td>CT_10104</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>A10 abdomen veneus 5.0 weke delen axial</td>\n",
       "      <td>104531.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>20170614.0</td>\n",
       "      <td>M</td>\n",
       "      <td>061Y</td>\n",
       "      <td>Aquilion ONE</td>\n",
       "      <td>FFS</td>\n",
       "      <td>TOSHIBA</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>FC09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CILM_CT_101040_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3985131</td>\n",
       "      <td>CRLM_139</td>\n",
       "      <td>CT_10203</td>\n",
       "      <td>7</td>\n",
       "      <td>27.555556</td>\n",
       "      <td>72.444444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Th-Abd. alg.  2.0  B20f</td>\n",
       "      <td>142730.55920</td>\n",
       "      <td>...</td>\n",
       "      <td>20060131.0</td>\n",
       "      <td>M</td>\n",
       "      <td>061Y</td>\n",
       "      <td>Sensation 16</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>B20f</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CILM_CT_102030_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8202397</td>\n",
       "      <td>CRLM_282</td>\n",
       "      <td>CT_10702</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.111111</td>\n",
       "      <td>98.888889</td>\n",
       "      <td>0</td>\n",
       "      <td>ONCO ThAbd  3.0  B31f</td>\n",
       "      <td>115626.45590</td>\n",
       "      <td>...</td>\n",
       "      <td>20170921.0</td>\n",
       "      <td>M</td>\n",
       "      <td>071Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CILM_CT_107020_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>8488669</td>\n",
       "      <td>CRLM_292</td>\n",
       "      <td>CT_89092</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115132.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>064Y</td>\n",
       "      <td>Mx8000 IDT 16</td>\n",
       "      <td>FFS</td>\n",
       "      <td>Philips</td>\n",
       "      <td>\"2.0\"</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CILM_CT_890920_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>9801055</td>\n",
       "      <td>CRLM_334</td>\n",
       "      <td>CT_89320</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Abd  ThorAbd  2.0  I41f  3</td>\n",
       "      <td>113700.46300</td>\n",
       "      <td>...</td>\n",
       "      <td>20150202.0</td>\n",
       "      <td>M</td>\n",
       "      <td>060Y</td>\n",
       "      <td>SOMATOM Definition Flash</td>\n",
       "      <td>FFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"2\"</td>\n",
       "      <td>['I41f', '3']</td>\n",
       "      <td>2.0</td>\n",
       "      <td>CILM_CT_893200_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9247015</td>\n",
       "      <td>CRLM_317</td>\n",
       "      <td>CT_94131</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>ONCO Th/Abd.  3.0  B31f</td>\n",
       "      <td>120739.99420</td>\n",
       "      <td>...</td>\n",
       "      <td>20140214.0</td>\n",
       "      <td>M</td>\n",
       "      <td>070Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CILM_CT_941310_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>3835842</td>\n",
       "      <td>CRLM_129</td>\n",
       "      <td>CT_97575</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>Thorax  5.0  B31f</td>\n",
       "      <td>91040.23781</td>\n",
       "      <td>...</td>\n",
       "      <td>20141016.0</td>\n",
       "      <td>F</td>\n",
       "      <td>071Y</td>\n",
       "      <td>Sensation 40</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"5\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CILM_CT_975750_0000.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>8408989</td>\n",
       "      <td>CRLM_290</td>\n",
       "      <td>CT_99400</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>th abd  3.0  B31f</td>\n",
       "      <td>155048.40080</td>\n",
       "      <td>...</td>\n",
       "      <td>20130301.0</td>\n",
       "      <td>M</td>\n",
       "      <td>073Y</td>\n",
       "      <td>Sensation 64</td>\n",
       "      <td>HFS</td>\n",
       "      <td>SIEMENS</td>\n",
       "      <td>\"3\"</td>\n",
       "      <td>B31f</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CILM_CT_994000_0000.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PID   Subject Experiment Scan       pHGP        dHGP        rHGP  \\\n",
       "0    3932332  CRLM_137   CT_10033    4   0.000000   30.000000   70.000000   \n",
       "1    4327749  CRLM_147   CT_10043    2  32.500000   22.500000   45.000000   \n",
       "2    1030425  CRLM_042   CT_10104    2   0.000000    0.000000  100.000000   \n",
       "3    3985131  CRLM_139   CT_10203    7  27.555556   72.444444    0.000000   \n",
       "4    8202397  CRLM_282   CT_10702    2   0.000000    1.111111   98.888889   \n",
       "..       ...       ...        ...  ...        ...         ...         ...   \n",
       "194  8488669  CRLM_292   CT_89092    5   0.000000  100.000000    0.000000   \n",
       "195  9801055  CRLM_334   CT_89320    3   0.000000   65.000000   35.000000   \n",
       "196  9247015  CRLM_317   CT_94131    3   0.000000  100.000000    0.000000   \n",
       "197  3835842  CRLM_129   CT_97575    2   0.000000  100.000000    0.000000   \n",
       "198  8408989  CRLM_290   CT_99400    9   0.000000  100.000000    0.000000   \n",
       "\n",
       "     HGP_Type                       Series_description  acquisition_time  ...  \\\n",
       "0           0                  Lever meta's  2.0  B31f       84259.93629  ...   \n",
       "1           0                              ABDOMEN 2/2      141238.00000  ...   \n",
       "2           0  A10 abdomen veneus 5.0 weke delen axial      104531.00000  ...   \n",
       "3           0                  Th-Abd. alg.  2.0  B20f      142730.55920  ...   \n",
       "4           0                    ONCO ThAbd  3.0  B31f      115626.45590  ...   \n",
       "..        ...                                      ...               ...  ...   \n",
       "194         1                                      NaN      115132.00000  ...   \n",
       "195         0               Abd  ThorAbd  2.0  I41f  3      113700.46300  ...   \n",
       "196         1                  ONCO Th/Abd.  3.0  B31f      120739.99420  ...   \n",
       "197         1                        Thorax  5.0  B31f       91040.23781  ...   \n",
       "198         1                        th abd  3.0  B31f      155048.40080  ...   \n",
       "\n",
       "     seriesdate_y  gender   age                model_name  patient_position  \\\n",
       "0      20060906.0       F  073Y              Sensation 16               HFS   \n",
       "1             NaN       M   NaN             Mx8000 IDT 10               FFS   \n",
       "2      20170614.0       M  061Y              Aquilion ONE               FFS   \n",
       "3      20060131.0       M  061Y              Sensation 16               HFS   \n",
       "4      20170921.0       M  071Y              Sensation 64               HFS   \n",
       "..            ...     ...   ...                       ...               ...   \n",
       "194           NaN       M  064Y             Mx8000 IDT 16               FFS   \n",
       "195    20150202.0       M  060Y  SOMATOM Definition Flash               FFS   \n",
       "196    20140214.0       M  070Y              Sensation 64               HFS   \n",
       "197    20141016.0       F  071Y              Sensation 40               HFS   \n",
       "198    20130301.0       M  073Y              Sensation 64               HFS   \n",
       "\n",
       "     manufacturer  slice_thickness  convolution_kernel  kfold  \\\n",
       "0         SIEMENS              \"2\"                B31f    3.0   \n",
       "1         Philips           \"2.00\"                   C    2.0   \n",
       "2         TOSHIBA              \"5\"                FC09    0.0   \n",
       "3         SIEMENS              \"2\"                B20f    1.0   \n",
       "4         SIEMENS              \"3\"                B31f    3.0   \n",
       "..            ...              ...                 ...    ...   \n",
       "194       Philips            \"2.0\"                   B    1.0   \n",
       "195       SIEMENS              \"2\"       ['I41f', '3']    2.0   \n",
       "196       SIEMENS              \"3\"                B31f    3.0   \n",
       "197       SIEMENS              \"5\"                B31f    0.0   \n",
       "198       SIEMENS              \"3\"                B31f    0.0   \n",
       "\n",
       "                      data_path  \n",
       "0    CILM_CT_100330_0000.nii.gz  \n",
       "1    CILM_CT_100430_0000.nii.gz  \n",
       "2    CILM_CT_101040_0000.nii.gz  \n",
       "3    CILM_CT_102030_0000.nii.gz  \n",
       "4    CILM_CT_107020_0000.nii.gz  \n",
       "..                          ...  \n",
       "194  CILM_CT_890920_0000.nii.gz  \n",
       "195  CILM_CT_893200_0000.nii.gz  \n",
       "196  CILM_CT_941310_0000.nii.gz  \n",
       "197  CILM_CT_975750_0000.nii.gz  \n",
       "198  CILM_CT_994000_0000.nii.gz  \n",
       "\n",
       "[199 rows x 52 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#allow duplicates\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('../')\n",
    "from Core.Utils import Swin_Transformer_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fucking dhw 35 133 133\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 849425780 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\098986\\Intership_CILM\\CRLM_Internship_Project\\Jupyter_Test\\..\\Core\\Utils\\Swin_Transformer_Classification.py:334\u001b[0m, in \u001b[0;36mSwinUNETR.forward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_size(x_in\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m--> 334\u001b[0m hidden_states_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswinViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m#print('hidden states out',hidden_states_out.shape)\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03menc0 = self.encoder1(x_in)\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03menc1 = self.encoder2(hidden_states_out[0])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03mreturn logits\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\098986\\Intership_CILM\\CRLM_Internship_Project\\Jupyter_Test\\..\\Core\\Utils\\Swin_Transformer_Classification.py:1091\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x, normalize)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_v2:\n\u001b[0;32m   1090\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers1c[\u001b[38;5;241m0\u001b[39m](x0\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m-> 1091\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28mprint\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is x1 shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1093\u001b[0m x1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x1, normalize)\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\098986\\Intership_CILM\\CRLM_Internship_Project\\Jupyter_Test\\..\\Core\\Utils\\Swin_Transformer_Classification.py:921\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    919\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(h \u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m*\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    920\u001b[0m wp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(w \u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m2\u001b[39m])) \u001b[38;5;241m*\u001b[39m window_size[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 921\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    923\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x, attn_mask)\n",
      "File \u001b[1;32mc:\\Users\\098986\\Intership_CILM\\CRLM_Internship_Project\\Jupyter_Test\\..\\Core\\Utils\\Swin_Transformer_Classification.py:837\u001b[0m, in \u001b[0;36mcompute_mask\u001b[1;34m(dims, window_size, shift_size, device)\u001b[0m\n\u001b[0;32m    835\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m window_partition(img_mask, window_size)\n\u001b[0;32m    836\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 837\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_windows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mmasked_fill(attn_mask \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100.0\u001b[39m))\u001b[38;5;241m.\u001b[39mmasked_fill(attn_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.0\u001b[39m))\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 849425780 bytes."
     ]
    }
   ],
   "source": [
    "model = Swin_Transformer_Classification.SwinUNETR(img_size=256,in_channels=1, num_classes=2, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],out_channels=1)\n",
    "model_dicts = torch.load('../../Output/Resnet10/Test_local/SwingTransformer/0/best_metric_1.pth')['model']\n",
    "model.load_state_dict(model_dicts)\n",
    "model.train()\n",
    "test_data = torch.randn(1,1,64,256,256)\n",
    "#model.eval()\n",
    "model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_para(para_name,model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if para_name in name:\n",
    "            print(name,param.shape)\n",
    "    return param "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = [0]\n",
    "torch.save(a,'./new.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.load('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.nn import LayerNorm\n",
    "from typing_extensions import Final\n",
    "\n",
    "from monai.networks.blocks import MLPBlock as Mlp\n",
    "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.layers import DropPath, trunc_normal_\n",
    "from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n",
    "from monai.utils.deprecate_utils import deprecated_arg\n",
    "\n",
    "\n",
    "rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n",
    "\n",
    "__all__ = [\n",
    "    \"SwinUNETR\",\n",
    "    \"window_partition\",\n",
    "    \"window_reverse\",\n",
    "    \"WindowAttention\",\n",
    "    \"SwinTransformerBlock\",\n",
    "    \"PatchMerging\",\n",
    "    \"PatchMergingV2\",\n",
    "    \"MERGING_MODE\",\n",
    "    \"BasicLayer\",\n",
    "    \"SwinTransformer\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "class SwinUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin UNETR based on: \"Hatamizadeh et al.,\n",
    "    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n",
    "    <https://arxiv.org/abs/2201.01266>\"\n",
    "    \"\"\"\n",
    "\n",
    "    patch_size: Final[int] = 2\n",
    "\n",
    "    @deprecated_arg(\n",
    "        name=\"img_size\",\n",
    "        since=\"1.3\",\n",
    "        removed=\"1.5\",\n",
    "        msg_suffix=\"The img_size argument is not required anymore and \"\n",
    "        \"checks on the input size are run during forward().\",\n",
    "    )\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Sequence[int] | int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        feature_size: int = 24,\n",
    "        norm_name: tuple | str = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "        num_classes = 2\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size: spatial dimension of input image.\n",
    "                This argument is only used for checking that the input image size is divisible by the patch size.\n",
    "                The tensor passed to forward() can have a dynamic shape as long as its spatial dimensions are divisible by 2**5.\n",
    "                It will be removed in an upcoming version.\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            feature_size: dimension of network feature size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            dropout_path_rate: drop path rate.\n",
    "            normalize: normalize output intermediate features in each stage.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: number of spatial dims.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beggining of each swin stage.\n",
    "            num_class: number of classes for classification.\n",
    "        Examples::\n",
    "\n",
    "            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n",
    "            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n",
    "\n",
    "            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n",
    "            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n",
    "\n",
    "            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n",
    "            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "        #print(patch_sizes,'666')\n",
    "\n",
    "\n",
    "        if spatial_dims not in (2, 3):\n",
    "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
    "\n",
    "        self._check_input_size(img_size)\n",
    "\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "        self.num_class = num_classes\n",
    "        self.final_feature_size = feature_size * 2 * 2 ** (len(depths) - 1)\n",
    "\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_sizes,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n",
    "            use_v2=use_v2,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.head = nn.Linear(self.final_feature_size, self.num_class)\n",
    "        \"\"\"\n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder2 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder3 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder4 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=4 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.encoder10 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=16 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 8,\n",
    "            out_channels=feature_size * 4,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 4,\n",
    "            out_channels=feature_size * 2,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size * 2,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.decoder1 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "        \"\"\"\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n",
    "            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n",
    "            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers1\")\n",
    "            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers2\")\n",
    "            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers3\")\n",
    "            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n",
    "            )\n",
    "            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers4\")\n",
    "            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n",
    "            )\n",
    "            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n",
    "                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n",
    "            )\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _check_input_size(self, spatial_shape):\n",
    "        img_size = np.array(spatial_shape)\n",
    "        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n",
    "        if remainder.any():\n",
    "            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n",
    "            raise ValueError(\n",
    "                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n",
    "                f\" must be divisible by {self.patch_size}**5.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        if not torch.jit.is_scripting():\n",
    "            self._check_input_size(x_in.shape[2:])\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        #print('hidden states out',hidden_states_out.shape)\n",
    "        \n",
    "        \"\"\"\n",
    "        enc0 = self.encoder1(x_in)\n",
    "        enc1 = self.encoder2(hidden_states_out[0])\n",
    "        enc2 = self.encoder3(hidden_states_out[1])\n",
    "        enc3 = self.encoder4(hidden_states_out[2])\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        dec3 = self.decoder5(dec4, hidden_states_out[3])\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        logits = self.out(out)\n",
    "        return logits\n",
    "        \"\"\"\n",
    "        x = self.avgpool(hidden_states_out)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x: input tensor.\n",
    "        window_size: local window size.\n",
    "    \"\"\"\n",
    "    x_shape = x.size()\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            window_size[0],\n",
    "            h // window_size[1],\n",
    "            window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[2],\n",
    "            c,\n",
    "        )\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
    "        )\n",
    "    elif len(x_shape) == 4:\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"window reverse operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        windows: windows tensor.\n",
    "        window_size: local window size.\n",
    "        dims: dimension values.\n",
    "    \"\"\"\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            h // window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[0],\n",
    "            window_size[1],\n",
    "            window_size[2],\n",
    "            -1,\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "\n",
    "    elif len(dims) == 3:\n",
    "        b, h, w = dims\n",
    "        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"Computing window size based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x_size: input size.\n",
    "        window_size: local window size.\n",
    "        shift_size: window shifting size.\n",
    "    \"\"\"\n",
    "\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        qkv_bias: bool = False,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            attn_drop: attention dropout rate.\n",
    "            proj_drop: dropout rate of output.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "\n",
    "        if len(self.window_size) == 3:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "        elif len(self.window_size) == 2:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "            )\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn).to(v.dtype)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        shift_size: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: str = \"GELU\",\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            shift_size: window shift size.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: stochastic depth rate.\n",
    "            act_layer: activation layer.\n",
    "            norm_layer: normalization layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        x_shape = x.size()\n",
    "        x = self.norm1(x)\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "            _, dp, hp, wp, _ = x.shape\n",
    "            dims = [b, dp, hp, wp]\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = 0\n",
    "            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n",
    "            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, hp, wp, _ = x.shape\n",
    "            dims = [b, hp, wp]\n",
    "\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if len(x_shape) == 5:\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "        elif len(x_shape) == 4:\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :h, :w, :].contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        print(\"all < 0\",torch.all(self.norm2(x)<0))\n",
    "        return self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "    def load_from(self, weights, n_block, layer):\n",
    "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
    "        block_names = [\n",
    "            \"norm1.weight\",\n",
    "            \"norm1.bias\",\n",
    "            \"attn.relative_position_bias_table\",\n",
    "            \"attn.relative_position_index\",\n",
    "            \"attn.qkv.weight\",\n",
    "            \"attn.qkv.bias\",\n",
    "            \"attn.proj.weight\",\n",
    "            \"attn.proj.bias\",\n",
    "            \"norm2.weight\",\n",
    "            \"norm2.bias\",\n",
    "            \"mlp.fc1.weight\",\n",
    "            \"mlp.fc1.bias\",\n",
    "            \"mlp.fc2.weight\",\n",
    "            \"mlp.fc2.bias\",\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
    "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
    "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
    "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n",
    "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
    "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
    "            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n",
    "            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n",
    "            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n",
    "            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n",
    "            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n",
    "            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n",
    "            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n",
    "            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        shortcut = x\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = x + self.forward_part2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            norm_layer: normalization layer.\n",
    "            spatial_dims: number of spatial dims.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        elif spatial_dims == 2:\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "            x = torch.cat(\n",
    "                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n",
    "            )\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n",
    "            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(PatchMergingV2):\n",
    "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 4:\n",
    "            return super().forward(x)\n",
    "        if len(x_shape) != 5:\n",
    "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "        b, d, h, w, c = x_shape\n",
    "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x5 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x6 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
    "\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        d, h, w = dims\n",
    "        #print('fucking dhw',d,h,w)\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "    elif len(dims) == 2:\n",
    "        h, w = dims\n",
    "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
    "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        drop_path: list,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        downsample: nn.Module | None = None,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            depth: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            drop_path: stochastic depth rate.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            norm_layer: normalization layer.\n",
    "            downsample: an optional downsampling layer at the end of the layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        #å°±æ˜¯é™¤ä»¥windowsize,æ¯”å¦‚56X56 shiftsizeå°±æ˜¯3X3,å› ä¸ºwindowssize æ˜¯7X7\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)\n",
    "        self.no_shift = tuple(0 for i in window_size)\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=self.window_size,\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_checkpoint=use_checkpoint,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        #å°±æ˜¯patch merging\n",
    "        self.downsample = downsample\n",
    "        if callable(self.downsample):\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, d, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, c, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c h w -> b h w c\")\n",
    "            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n",
    "            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n",
    "            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        embed_dim: int,\n",
    "        window_size: Sequence[int],\n",
    "        patch_size: Sequence[int],\n",
    "        depths: Sequence[int],\n",
    "        num_heads: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: dimension of input channels.\n",
    "            embed_dim: number of linear projection output channels.\n",
    "            window_size: local window size.\n",
    "            patch_size: patch size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            drop_path_rate: stochastic depth rate.\n",
    "            norm_layer: normalization layer.\n",
    "            patch_norm: add normalization after patch embedding.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: spatial dimension.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=self.patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        self.use_v2 = use_v2\n",
    "        self.layers1 = nn.ModuleList()\n",
    "        self.layers2 = nn.ModuleList()\n",
    "        self.layers3 = nn.ModuleList()\n",
    "        self.layers4 = nn.ModuleList()\n",
    "        if self.use_v2:\n",
    "            self.layers1c = nn.ModuleList()\n",
    "            self.layers2c = nn.ModuleList()\n",
    "            self.layers3c = nn.ModuleList()\n",
    "            self.layers4c = nn.ModuleList()\n",
    "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=self.window_size,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=down_sample_mod,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "            if self.use_v2:\n",
    "                layerc = UnetrBasicBlock(\n",
    "                    spatial_dims=3,\n",
    "                    in_channels=embed_dim * 2**i_layer,\n",
    "                    out_channels=embed_dim * 2**i_layer,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    norm_name=\"instance\",\n",
    "                    res_block=True,\n",
    "                )\n",
    "                if i_layer == 0:\n",
    "                    self.layers1c.append(layerc)\n",
    "                elif i_layer == 1:\n",
    "                    self.layers2c.append(layerc)\n",
    "                elif i_layer == 2:\n",
    "                    self.layers3c.append(layerc)\n",
    "                elif i_layer == 3:\n",
    "                    self.layers4c.append(layerc)\n",
    "        #è¿™é‡Œçš„number featuresè·Ÿå±‚æ•°æœ‰å…³ç³»ï¼æ¯”å¦‚è¯´4å±‚çš„è¯ï¼Œæœ€åŽçš„number featureså°±æ˜¯embed_dim * 2 ** 3\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "    def proj_out(self, x, normalize=False):\n",
    "        if normalize:\n",
    "            x_shape = x.size()\n",
    "            if len(x_shape) == 5:\n",
    "\n",
    "                n, ch, d, h, w = x_shape\n",
    "                x = rearrange(x, \"n c d h w -> n d h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n d h w c -> n c d h w\")\n",
    "            elif len(x_shape) == 4:\n",
    "                n, ch, h, w = x_shape\n",
    "                x = rearrange(x, \"n c h w -> n h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n h w c -> n c h w\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, normalize=True):\n",
    "        #print(x.shape,'this is x shape')\n",
    "        x0 = self.patch_embed(x)\n",
    "        #print(x0.shape,'after embed')\n",
    "        x0 = self.pos_drop(x0)\n",
    "        #print(x0.shape,'this is x0 shape')\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "        if self.use_v2:\n",
    "            x0 = self.layers1c[0](x0.contiguous())\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        print(x1.shape,'this is x1 shape')\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        if self.use_v2:\n",
    "            x1 = self.layers2c[0](x1.contiguous())\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        #print(x2.shape,'this is x2 shape')\n",
    "        if self.use_v2:\n",
    "            x2 = self.layers3c[0](x2.contiguous())\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        if self.use_v2:\n",
    "            x3 = self.layers4c[0](x3.contiguous())\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "        #return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
    "        return x4_out\n",
    "\n",
    "\n",
    "def filter_swinunetr(key, value):\n",
    "    \"\"\"\n",
    "    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n",
    "    This function is typically used with `monai.networks.copy_model_state`\n",
    "    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n",
    "    <https://arxiv.org/abs/2307.16896>\"\n",
    "\n",
    "    Args:\n",
    "        key: the key in the source state dict used for the update.\n",
    "        value: the value in the source state dict used for the update.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        import torch\n",
    "        from monai.apps import download_url\n",
    "        from monai.networks.utils import copy_model_state\n",
    "        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n",
    "\n",
    "        model = SwinUNETR(img_size=(96, 96, 96), in_channels=1, out_channels=3, feature_size=48)\n",
    "        resource = (\n",
    "            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n",
    "        )\n",
    "        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n",
    "        download_url(resource, ssl_weights_path)\n",
    "        ssl_weights = torch.load(ssl_weights_path)[\"model\"]\n",
    "\n",
    "        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n",
    "\n",
    "    \"\"\"\n",
    "    if key in [\n",
    "        \"encoder.mask_token\",\n",
    "        \"encoder.norm.weight\",\n",
    "        \"encoder.norm.bias\",\n",
    "        \"out.conv.conv.weight\",\n",
    "        \"out.conv.conv.bias\",\n",
    "    ]:\n",
    "        return None\n",
    "\n",
    "    if key[:8] == \"encoder.\":\n",
    "        if key[8:19] == \"patch_embed\":\n",
    "            new_key = \"swinViT.\" + key[8:]\n",
    "        else:\n",
    "            new_key = \"swinViT.\" + key[8:18] + key[20:]\n",
    "\n",
    "        return new_key, value\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMA(\n",
       "  (online_model): SwinUNETR(\n",
       "    (swinViT): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(1, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers1): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=24, out_features=72, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=24, out_features=96, bias=True)\n",
       "                (linear2): Linear(in_features=96, out_features=24, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=24, out_features=72, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=24, out_features=96, bias=True)\n",
       "                (linear2): Linear(in_features=96, out_features=24, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=192, out_features=48, bias=False)\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers2): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers3): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers4): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (head): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       "  (ema_model): SwinUNETR(\n",
       "    (swinViT): SwinTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv3d(1, 24, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers1): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=24, out_features=72, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=24, out_features=96, bias=True)\n",
       "                (linear2): Linear(in_features=96, out_features=24, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=24, out_features=72, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=24, out_features=24, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=24, out_features=96, bias=True)\n",
       "                (linear2): Linear(in_features=96, out_features=24, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=192, out_features=48, bias=False)\n",
       "            (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers2): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=96, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers3): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (linear2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers4): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): MLPBlock(\n",
       "                (linear1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (linear2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (fn): GELU(approximate='none')\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "    (head): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\monai\\transforms\\io\\array.py:205: UserWarning: required package for reader ITKReader is not installed, or the version doesn't match requirement.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Core.Dataset.Dataloader import *\n",
    "from torch.utils.data import Subset\n",
    "from Core.Utils.Utility import Balanced_sampler\n",
    "from monai.transforms import EnsureChannelFirst,Resize,NormalizeIntensity,ToTensor,RandZoom,RandRotate,RandFlip\n",
    "from Core.Utils import Swin_Transformer_Classification\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "\n",
    "def train_loop(model,dataloader,epoch_num,optimizer,criterion,ema,scheduler=None):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        model: model to be trained\n",
    "        dataloader: dataloader\n",
    "        epoch_num: number of epochs\n",
    "        device: device to train on\n",
    "        optimizer: optimizer\n",
    "        criterion: loss function\n",
    "        learning_rate: learning rate\n",
    "    \"\"\"\n",
    "    #prepare data for training\n",
    "    train_bar = tqdm(dataloader)\n",
    "    average_loss = 0\n",
    "    print(len(train_bar),'length of train_bar')\n",
    "\n",
    "\n",
    "    #set metrics record\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    print(\"##################\")\n",
    "    print(f\"epoch {epoch_num+1}\")\n",
    "    print(\"##################\")\n",
    "    #model = model.to(device)\n",
    "    for i,(im,label) in enumerate(train_bar):\n",
    " \n",
    "\n",
    "        #rotate and flip\n",
    "        im = torch.rot90(im,k=3,dims=(2,3))\n",
    "        im = torch.flip(im,[3])\n",
    "        #permute to [B,C,D,H,W]\n",
    "        im = im.permute(0,1,4,2,3)\n",
    "        #print('this is im',im.shape)\n",
    "\n",
    "\n",
    "        im,label = im.to('cpu'),label.to('cpu')\n",
    "        #print leraing rate\n",
    "        #print(f\"learning rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = (model(im))\n",
    "        #print('mother fucker loss function',criterion)\n",
    "        #print(type(label),'and fucking label',label)\n",
    "        loss = criterion(output,label)\n",
    "        loss.backward()\n",
    "\n",
    "        average_loss += loss.item()\n",
    "        \n",
    "        output = torch.nn.functional.softmax(output,dim=1)\n",
    "\n",
    "        #softmax probability\n",
    "       \n",
    "        y_pred.append(output.cpu())\n",
    "        y_true.extend(label.cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "        #print(y_true,6666)\n",
    "        #set description for tqdm\n",
    "        train_bar.set_description(f\"label:{label},step_loss:{loss},out_put_prob:{output}\")\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        print(nn.functional.softmax(model(im)),666777,label)\n",
    "        model.train()\n",
    "\n",
    "        #print(f\"y_true_label{label};y_predict:{output};step_loss{loss}\")\n",
    "        optimizer.step()\n",
    "        ema.update()\n",
    "        print(ema.step,'step')\n",
    "        ema_model = ema.ema_model.eval()\n",
    "        ema_output = nn.functional.softmax(ema_model(im))\n",
    "        print(ema_output,666,label)\n",
    "        #scheduler.step()\n",
    "\n",
    "        #metrics\n",
    "    \n",
    "\n",
    "    #print('accur',accuracy)\n",
    "    \n",
    "\n",
    "    average_loss /= len(train_bar)\n",
    "    print('average_loss',average_loss)\n",
    "    return average_loss,y_pred,y_true\n",
    "\n",
    "\n",
    "\n",
    "transform_param = {\"transform_methods\":[\n",
    "                        EnsureChannelFirst(),\n",
    "                        # Data augmentation\n",
    "                        RandZoom(prob = 0.5, min_zoom=1.0, max_zoom=1.2),\n",
    "                        RandRotate(range_z = 0.35, prob = 0.5),\n",
    "                        RandFlip(prob = 0.5),\n",
    "                        Resize((64,64,32)),\n",
    "                        #NormalizeIntensity(),\n",
    "                        # To tensor\n",
    "                        ToTensor()\n",
    "                        ]}\n",
    "\n",
    "\n",
    "data_path = '../../Data/Mixed_HGP/Mixed_HGP_lv_with_tumor_07073_Windowed/'\n",
    "label_path = '../../Data/Mixed_HGP/True_Label/train_cv_0.csv'\n",
    "Data = DataFiles(data_path,label_path,'HGP_Type')\n",
    "images_lst = sorted(Data.get_images())\n",
    "labels_lst = Data.get_labels()\n",
    "#model =  Swin_Transformer_Classification.SwinUNETR(in_channels=1, out_channels=2, img_size=(256, 256, 128))\n",
    "\n",
    "tr_dataset =  Image_Dataset(image_files=images_lst,labels=labels_lst,transform_methods=transform_param['transform_methods'],data_aug=True,label_name=None,reader='ITKReader',\n",
    "                            padding_size=None)\n",
    "\n",
    "\n",
    "tr_dataloader = Data_Loader(dataset=tr_dataset,batch_size=2,num_workers=0).build_train_loader() \n",
    "model = Swin_Transformer_Classification.SwinUNETR(in_channels=1, out_channels=2, img_size=(64,64,32))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model = Swin_Transformer_Classification.SwinUNETR(in_channels=1, out_channels=2, img_size=(64,64,32))\n",
    "\n",
    "\n",
    "ema = EMA(\n",
    "    model,\n",
    "    beta = 100000,              # exponential moving average factor\n",
    "    update_after_step = 1,    # only after this number of .update() calls will it start updating\n",
    "    update_every = 1, \n",
    "    power =3/4         # how often to actually update, to save on compute (updates every 10th .update() call)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 length of train_bar\n",
      "##################\n",
      "epoch 1\n",
      "##################\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.5102782845497131,out_put_prob:tensor([[0.6669, 0.3331],\n",
      "        [0.5404, 0.4596]], grad_fn=<AliasBackward0>):   0%|          | 0/99 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\_tensor.py:1278: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  ret = func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metatensor([[0.6669, 0.3331],\n",
      "        [0.5404, 0.4596]], grad_fn=<AliasBackward0>) 666777 tensor([0, 0])\n",
      "tensor(1) step\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.5102782845497131,out_put_prob:tensor([[0.6669, 0.3331],\n",
      "        [0.5404, 0.4596]], grad_fn=<AliasBackward0>):   1%|          | 1/99 [00:02<04:03,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.6669, 0.3331],\n",
      "        [0.5404, 0.4596]]) 666 tensor([0, 0])\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.7427911758422852,out_put_prob:tensor([[0.4998, 0.5002],\n",
      "        [0.4529, 0.5471]], grad_fn=<AliasBackward0>):   1%|          | 1/99 [00:03<04:03,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.4998, 0.5002],\n",
      "        [0.4529, 0.5471]], grad_fn=<AliasBackward0>) 666777 tensor([0, 0])\n",
      "tensor(2) step\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.7427911758422852,out_put_prob:tensor([[0.4998, 0.5002],\n",
      "        [0.4529, 0.5471]], grad_fn=<AliasBackward0>):   2%|â–         | 2/99 [00:04<04:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.4998, 0.5002],\n",
      "        [0.4529, 0.5471]]) 666 tensor([0, 0])\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.7591995000839233,out_put_prob:tensor([[0.4299, 0.5701],\n",
      "        [0.5095, 0.4905]], grad_fn=<AliasBackward0>):   2%|â–         | 2/99 [00:06<04:00,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.4299, 0.5701],\n",
      "        [0.5095, 0.4905]], grad_fn=<AliasBackward0>) 666777 tensor([0, 0])\n",
      "tensor(3) step\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 0]),step_loss:0.7591995000839233,out_put_prob:tensor([[0.4299, 0.5701],\n",
      "        [0.5095, 0.4905]], grad_fn=<AliasBackward0>):   3%|â–Ž         | 3/99 [00:07<03:57,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.4299, 0.5701],\n",
      "        [0.5095, 0.4905]]) 666 tensor([0, 0])\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 1\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 1]),step_loss:0.7281748056411743,out_put_prob:tensor([[0.5082, 0.4918],\n",
      "        [0.5413, 0.4587]], grad_fn=<AliasBackward0>):   3%|â–Ž         | 3/99 [00:08<03:57,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.5082, 0.4918],\n",
      "        [0.5413, 0.4587]], grad_fn=<AliasBackward0>) 666777 tensor([0, 1])\n",
      "tensor(4) step\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 1]),step_loss:0.7281748056411743,out_put_prob:tensor([[0.5082, 0.4918],\n",
      "        [0.5413, 0.4587]], grad_fn=<AliasBackward0>):   4%|â–         | 4/99 [00:09<03:53,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "metatensor([[0.5082, 0.4918],\n",
      "        [0.5413, 0.4587]]) 666 tensor([0, 1])\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "2\n",
      "torch.Size([1, 64, 64, 32]) 0\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "torch.Size([2, 48, 8, 16, 16]) this is x1 shape\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n",
      "all < 0 metatensor(False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "label:tensor([0, 1]),step_loss:0.7281748056411743,out_put_prob:tensor([[0.5082, 0.4918],\n",
      "        [0.5413, 0.4587]], grad_fn=<AliasBackward0>):   4%|â–         | 4/99 [00:11<04:28,  2.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m av_loss,y_pred,y_true \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtr_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mema\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 60\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, dataloader, epoch_num, optimizer, criterion, ema, scheduler)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#print('mother fucker loss function',criterion)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#print(type(label),'and fucking label',label)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output,label)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m average_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     64\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(output,dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1529\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1530\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m   1532\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[1;32m-> 1534\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\monai\\data\\meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 282\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\_tensor.py:1278\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[1;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[1;32m-> 1278\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[0;32m   1280\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "av_loss,y_pred,y_true = train_loop(model,tr_dataloader,0,optimizer,criterion,ema,scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('swinViT.patch_embed.proj.weight',\n",
       "              tensor([[[[[ 0.2711,  0.1817],\n",
       "                         [ 0.1182,  0.2391]],\n",
       "              \n",
       "                        [[ 0.2095,  0.0433],\n",
       "                         [ 0.3156,  0.0804]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2150,  0.0418],\n",
       "                         [ 0.2647, -0.2866]],\n",
       "              \n",
       "                        [[-0.1069, -0.3302],\n",
       "                         [-0.2743,  0.3506]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.0223, -0.1690],\n",
       "                         [-0.1606,  0.2394]],\n",
       "              \n",
       "                        [[-0.3009,  0.0164],\n",
       "                         [ 0.3328, -0.3530]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2040, -0.0941],\n",
       "                         [-0.3221,  0.0888]],\n",
       "              \n",
       "                        [[ 0.1528, -0.2118],\n",
       "                         [-0.2287,  0.2868]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.3532,  0.2700],\n",
       "                         [ 0.3380,  0.3377]],\n",
       "              \n",
       "                        [[ 0.2870, -0.2971],\n",
       "                         [-0.3229, -0.3032]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.3524,  0.3451],\n",
       "                         [ 0.2283, -0.1051]],\n",
       "              \n",
       "                        [[ 0.0759, -0.3518],\n",
       "                         [ 0.1248, -0.1653]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.1711,  0.2351],\n",
       "                         [ 0.2074,  0.0673]],\n",
       "              \n",
       "                        [[ 0.0634, -0.0297],\n",
       "                         [ 0.1414, -0.0623]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2969, -0.3127],\n",
       "                         [ 0.1093, -0.0989]],\n",
       "              \n",
       "                        [[ 0.1872, -0.1537],\n",
       "                         [ 0.1313,  0.0392]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.0940,  0.3299],\n",
       "                         [ 0.1065,  0.2703]],\n",
       "              \n",
       "                        [[-0.2519,  0.0329],\n",
       "                         [ 0.0646, -0.2920]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.0315, -0.1788],\n",
       "                         [ 0.2351, -0.1242]],\n",
       "              \n",
       "                        [[ 0.3200,  0.1833],\n",
       "                         [ 0.1408, -0.2044]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2673,  0.3007],\n",
       "                         [ 0.1584, -0.2174]],\n",
       "              \n",
       "                        [[-0.0582, -0.2579],\n",
       "                         [-0.0931, -0.3489]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.2748,  0.2989],\n",
       "                         [ 0.0481, -0.1588]],\n",
       "              \n",
       "                        [[-0.2810, -0.0580],\n",
       "                         [-0.2516,  0.3051]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.1288,  0.2913],\n",
       "                         [ 0.0392,  0.2245]],\n",
       "              \n",
       "                        [[-0.2844,  0.0633],\n",
       "                         [ 0.1236,  0.0688]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.1803, -0.0798],\n",
       "                         [ 0.1237, -0.1620]],\n",
       "              \n",
       "                        [[ 0.3067, -0.0211],\n",
       "                         [-0.1933, -0.3299]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.0255, -0.1583],\n",
       "                         [ 0.2648,  0.2853]],\n",
       "              \n",
       "                        [[-0.1700,  0.2038],\n",
       "                         [-0.2346, -0.0325]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.2881, -0.2928],\n",
       "                         [ 0.2926,  0.0963]],\n",
       "              \n",
       "                        [[ 0.0465,  0.1879],\n",
       "                         [-0.2204,  0.0556]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.1280, -0.2728],\n",
       "                         [ 0.0386,  0.0344]],\n",
       "              \n",
       "                        [[ 0.3020, -0.0995],\n",
       "                         [ 0.3282, -0.3489]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.0077, -0.0375],\n",
       "                         [ 0.3119,  0.2962]],\n",
       "              \n",
       "                        [[-0.3100,  0.0086],\n",
       "                         [ 0.1630,  0.2573]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.0509,  0.1444],\n",
       "                         [-0.2672, -0.1907]],\n",
       "              \n",
       "                        [[-0.2886, -0.3109],\n",
       "                         [-0.1426, -0.1791]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.0459,  0.0815],\n",
       "                         [ 0.1103,  0.1434]],\n",
       "              \n",
       "                        [[-0.0083, -0.0294],\n",
       "                         [-0.1535, -0.0821]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2436,  0.1285],\n",
       "                         [ 0.1333,  0.2828]],\n",
       "              \n",
       "                        [[ 0.1672, -0.2621],\n",
       "                         [ 0.3015,  0.1044]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[ 0.2112,  0.1263],\n",
       "                         [ 0.2615,  0.2489]],\n",
       "              \n",
       "                        [[-0.0170, -0.1738],\n",
       "                         [ 0.0482, -0.2253]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.1182,  0.0747],\n",
       "                         [-0.0430,  0.0819]],\n",
       "              \n",
       "                        [[ 0.2581, -0.2963],\n",
       "                         [ 0.0925, -0.2753]]]],\n",
       "              \n",
       "              \n",
       "              \n",
       "                      [[[[-0.0397, -0.2695],\n",
       "                         [ 0.2634, -0.0566]],\n",
       "              \n",
       "                        [[ 0.0649,  0.2484],\n",
       "                         [ 0.1544, -0.2582]]]]])),\n",
       "             ('swinViT.patch_embed.proj.bias',\n",
       "              tensor([-0.1630, -0.1574, -0.0533,  0.1240,  0.3188, -0.2787, -0.2862,  0.2662,\n",
       "                       0.0567,  0.0452, -0.3340,  0.0556, -0.3281,  0.1874,  0.3283,  0.3463,\n",
       "                      -0.2277, -0.2059,  0.1013, -0.1381, -0.2346,  0.3329,  0.3083,  0.2062])),\n",
       "             ('swinViT.layers1.0.blocks.0.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers1.0.blocks.0.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.relative_position_bias_table',\n",
       "              tensor([[-0.0013, -0.0138,  0.0100],\n",
       "                      [ 0.0158, -0.0058, -0.0258],\n",
       "                      [ 0.0042,  0.0085, -0.0083],\n",
       "                      ...,\n",
       "                      [-0.0165,  0.0277,  0.0304],\n",
       "                      [-0.0264, -0.0046, -0.0293],\n",
       "                      [ 0.0124, -0.0173, -0.0413]])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.qkv.weight',\n",
       "              tensor([[-0.1094,  0.0121, -0.0933,  ...,  0.1964, -0.0866,  0.0760],\n",
       "                      [ 0.1108,  0.1967,  0.0666,  ..., -0.0934, -0.1239,  0.1325],\n",
       "                      [ 0.1170,  0.2008, -0.0186,  ...,  0.0300,  0.0449,  0.0632],\n",
       "                      ...,\n",
       "                      [-0.0042,  0.1719,  0.0540,  ..., -0.0965, -0.0319,  0.0800],\n",
       "                      [ 0.0647, -0.0077, -0.1075,  ...,  0.0988,  0.1131,  0.1312],\n",
       "                      [ 0.0310,  0.0088, -0.1803,  ..., -0.0635,  0.1658, -0.0571]])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.qkv.bias',\n",
       "              tensor([-0.0609,  0.1706, -0.0248, -0.1928, -0.0342, -0.1365, -0.2014,  0.0906,\n",
       "                      -0.1912,  0.0313, -0.0323,  0.1268, -0.1798, -0.1898,  0.0320, -0.0559,\n",
       "                      -0.0807, -0.1670, -0.0328,  0.0260, -0.1231,  0.1988,  0.0782, -0.0642,\n",
       "                       0.0155,  0.1909, -0.1795, -0.1909, -0.0733, -0.0982, -0.0331, -0.0367,\n",
       "                       0.0237,  0.0899, -0.2041,  0.0660, -0.0250, -0.0680, -0.1355,  0.0884,\n",
       "                      -0.1380,  0.0398, -0.1713, -0.0480, -0.0472,  0.0466, -0.1510, -0.1616,\n",
       "                       0.1615, -0.0732, -0.0985, -0.1358, -0.1942, -0.0118, -0.1020, -0.1133,\n",
       "                       0.0020, -0.0487, -0.1051, -0.0694, -0.0165, -0.1043,  0.0127, -0.1521,\n",
       "                       0.0003, -0.1454,  0.0070,  0.1894,  0.1525, -0.0491, -0.1343, -0.1202])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.proj.weight',\n",
       "              tensor([[-0.0254, -0.0719,  0.0551,  0.0957, -0.1589,  0.0980,  0.1163, -0.1860,\n",
       "                        0.1120,  0.1513, -0.0478,  0.1008, -0.1172,  0.0126, -0.0849, -0.0270,\n",
       "                        0.0266, -0.0778,  0.0785,  0.1624,  0.0030,  0.0350,  0.0081,  0.0470],\n",
       "                      [-0.0455,  0.1833,  0.0681,  0.1896,  0.1594, -0.0213,  0.1710, -0.0883,\n",
       "                       -0.0055, -0.0068,  0.0119, -0.1302, -0.0192, -0.1745, -0.0084, -0.0217,\n",
       "                        0.0265,  0.1119,  0.1039, -0.1376, -0.1741,  0.1782,  0.1207, -0.1109],\n",
       "                      [-0.0466,  0.1411,  0.1543, -0.1239, -0.0659, -0.0749, -0.1423, -0.1160,\n",
       "                        0.0866,  0.0737,  0.0667,  0.0229, -0.0044,  0.1653, -0.1540, -0.0601,\n",
       "                       -0.0822,  0.0268,  0.0877,  0.1302,  0.0304, -0.1172, -0.1624, -0.1314],\n",
       "                      [ 0.0557,  0.0067,  0.0342, -0.1992, -0.1989,  0.0543,  0.1191, -0.1931,\n",
       "                       -0.0136,  0.1122, -0.1939, -0.0018, -0.0467,  0.2018,  0.0294, -0.1383,\n",
       "                        0.0431, -0.1197, -0.1426, -0.1829, -0.1407, -0.1171, -0.0599,  0.2038],\n",
       "                      [-0.0216, -0.1428, -0.1634,  0.0626, -0.0897,  0.1536,  0.0209,  0.0171,\n",
       "                       -0.0494, -0.1478, -0.1749,  0.1420,  0.0951, -0.0578,  0.1125, -0.1043,\n",
       "                        0.0812,  0.0457,  0.1074, -0.0823,  0.1396,  0.1580,  0.1439, -0.1883],\n",
       "                      [ 0.0579, -0.0492, -0.1188, -0.0943, -0.1545,  0.1963,  0.1496,  0.0389,\n",
       "                       -0.0831, -0.1813, -0.1775, -0.1248,  0.0026, -0.1805,  0.1275,  0.1066,\n",
       "                       -0.1914,  0.0662, -0.1056,  0.1386,  0.0808,  0.0420,  0.1749,  0.0286],\n",
       "                      [-0.0160,  0.1077, -0.0435, -0.0567, -0.1680,  0.1741, -0.1565, -0.1873,\n",
       "                        0.0592,  0.1209, -0.0072, -0.1227, -0.0221,  0.1447,  0.1205,  0.0670,\n",
       "                        0.0097,  0.0258, -0.1729, -0.0922,  0.1778, -0.0885,  0.0329,  0.1517],\n",
       "                      [ 0.1423, -0.0653, -0.1215,  0.1351,  0.1592,  0.0511, -0.1674, -0.0805,\n",
       "                        0.0458, -0.1700, -0.0536,  0.0939,  0.0208, -0.0802, -0.1362, -0.0518,\n",
       "                       -0.0160,  0.0503,  0.0517,  0.1830,  0.0798,  0.1361, -0.1963, -0.0182],\n",
       "                      [-0.0826, -0.1170,  0.1851, -0.0361, -0.1597,  0.1922, -0.1050, -0.1642,\n",
       "                       -0.1185, -0.1760,  0.0497, -0.1327,  0.1081, -0.0632,  0.0774,  0.2026,\n",
       "                        0.1867, -0.0723,  0.0036, -0.1998,  0.0334,  0.1350, -0.0742, -0.1446],\n",
       "                      [ 0.1026,  0.1010,  0.0408,  0.1777, -0.0290,  0.0475, -0.1373,  0.0251,\n",
       "                       -0.0944,  0.0121, -0.0713, -0.1207,  0.1516, -0.0289,  0.0419, -0.0546,\n",
       "                       -0.0250,  0.0503,  0.1653, -0.0495, -0.0978, -0.1692,  0.0032, -0.0448],\n",
       "                      [-0.1805,  0.1155, -0.1127, -0.1436, -0.1229, -0.0064, -0.0177, -0.2029,\n",
       "                       -0.1480, -0.0254, -0.0844,  0.0669,  0.0345, -0.1891,  0.1941,  0.1909,\n",
       "                        0.0276, -0.0609,  0.1599,  0.0996, -0.0033,  0.1411,  0.0623, -0.0704],\n",
       "                      [ 0.0949,  0.1031, -0.1505,  0.1853,  0.0710,  0.0514, -0.1148,  0.0110,\n",
       "                       -0.1534, -0.1218,  0.1052, -0.0706,  0.1892,  0.0774, -0.1912,  0.1267,\n",
       "                        0.0786,  0.1834,  0.1650,  0.1947, -0.0785,  0.1814, -0.1935,  0.0175],\n",
       "                      [ 0.1394, -0.1397,  0.1850,  0.0981, -0.1094, -0.0125,  0.1981,  0.0635,\n",
       "                       -0.1361, -0.0394, -0.1530,  0.0557,  0.0560,  0.0795,  0.1673, -0.0471,\n",
       "                       -0.1189, -0.1576,  0.0289, -0.0462, -0.0786, -0.0240,  0.0736,  0.1920],\n",
       "                      [ 0.0265,  0.0044, -0.0993,  0.1836, -0.0777, -0.0135,  0.0599,  0.1827,\n",
       "                       -0.0285, -0.1780,  0.0801, -0.0779, -0.0503, -0.0023, -0.0065,  0.1738,\n",
       "                       -0.1594, -0.0345,  0.0617, -0.0643,  0.0079,  0.1658, -0.2031,  0.1538],\n",
       "                      [-0.1684, -0.1971, -0.1435,  0.0713,  0.2002, -0.1739,  0.2002, -0.1938,\n",
       "                        0.1247, -0.1240,  0.1265, -0.0761, -0.1086, -0.1620,  0.1653,  0.0070,\n",
       "                       -0.0324, -0.1670,  0.1678, -0.0974,  0.0586, -0.0540, -0.0732, -0.1095],\n",
       "                      [-0.0413, -0.0395,  0.0487, -0.1548, -0.0930, -0.1212, -0.0449, -0.0033,\n",
       "                        0.0128,  0.0222,  0.1041,  0.1409,  0.1009, -0.0899, -0.0474,  0.0686,\n",
       "                       -0.0165, -0.1691, -0.0139,  0.1005,  0.1923, -0.1037,  0.0338, -0.0892],\n",
       "                      [ 0.0396, -0.1006,  0.0940, -0.1813,  0.1123,  0.0788, -0.0255,  0.1906,\n",
       "                        0.0438, -0.0721,  0.0555,  0.1081,  0.1274, -0.0737,  0.0254,  0.1059,\n",
       "                       -0.0870, -0.0471,  0.1351, -0.1813,  0.0881,  0.1359,  0.1850, -0.0580],\n",
       "                      [-0.1953,  0.0186, -0.1797, -0.0129,  0.1893, -0.1944,  0.0562,  0.0969,\n",
       "                        0.0593, -0.0439, -0.1513, -0.0297, -0.1505, -0.0193, -0.0763, -0.1954,\n",
       "                       -0.0678, -0.1618,  0.0562,  0.1904, -0.1493, -0.2021,  0.0853, -0.0791],\n",
       "                      [ 0.1489, -0.1990,  0.1375,  0.1444, -0.0367,  0.1386, -0.0940, -0.1918,\n",
       "                        0.0556, -0.0026,  0.1199,  0.0827, -0.1958, -0.1694, -0.1230, -0.1866,\n",
       "                        0.1466,  0.0898,  0.2040,  0.0803, -0.0895,  0.0206, -0.1676, -0.0691],\n",
       "                      [-0.0639,  0.0895,  0.1534,  0.0662, -0.0370,  0.0391,  0.0754,  0.1743,\n",
       "                       -0.1818, -0.0957,  0.1693,  0.2008,  0.0864, -0.0211, -0.0872, -0.2008,\n",
       "                       -0.0852, -0.0723, -0.0417,  0.1180,  0.0424,  0.0549, -0.0195,  0.1563],\n",
       "                      [ 0.0682, -0.0460, -0.1739, -0.0349,  0.1762, -0.0873, -0.1786,  0.0808,\n",
       "                        0.0181, -0.0040, -0.1816, -0.1623,  0.1645, -0.0214,  0.1594, -0.0382,\n",
       "                        0.0162, -0.0438,  0.1402,  0.1749,  0.1512,  0.1624,  0.0995,  0.0618],\n",
       "                      [ 0.0582, -0.0668,  0.0247,  0.1886, -0.0600, -0.1816, -0.0200, -0.1781,\n",
       "                       -0.0373, -0.1018,  0.0043,  0.0685, -0.0810,  0.1664, -0.0050,  0.0240,\n",
       "                       -0.0710, -0.1893, -0.1272,  0.1319, -0.0065,  0.0874,  0.0765,  0.1293],\n",
       "                      [-0.0807,  0.1855,  0.0074,  0.1605,  0.0652,  0.1101,  0.0481, -0.0763,\n",
       "                        0.1216,  0.1844,  0.1389,  0.0004, -0.0713,  0.1801,  0.1898, -0.1645,\n",
       "                        0.0932,  0.0217,  0.1044, -0.0089,  0.1100,  0.1579, -0.0188, -0.1301],\n",
       "                      [ 0.0866, -0.1869, -0.1431,  0.0652,  0.1137, -0.0067,  0.1930,  0.1627,\n",
       "                        0.1735, -0.1506, -0.1894,  0.0614, -0.1436,  0.0496,  0.0438,  0.0459,\n",
       "                        0.0234,  0.0578, -0.1506, -0.1395, -0.0264,  0.1380, -0.0441, -0.1916]])),\n",
       "             ('swinViT.layers1.0.blocks.0.attn.proj.bias',\n",
       "              tensor([-0.0585, -0.0359,  0.0490,  0.1320, -0.0584, -0.0086, -0.1953, -0.1518,\n",
       "                      -0.0713,  0.1989,  0.0914, -0.0505, -0.0175,  0.0936, -0.0212, -0.1958,\n",
       "                      -0.1804,  0.0824,  0.0043,  0.0182,  0.0299, -0.1152, -0.1988,  0.0662])),\n",
       "             ('swinViT.layers1.0.blocks.0.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers1.0.blocks.0.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers1.0.blocks.0.mlp.linear1.weight',\n",
       "              tensor([[ 0.1624,  0.1387,  0.0545,  ..., -0.0008, -0.0502, -0.0859],\n",
       "                      [ 0.0131,  0.1059,  0.1167,  ..., -0.1333,  0.1067,  0.1823],\n",
       "                      [ 0.1220,  0.0553, -0.2001,  ...,  0.0011, -0.1823,  0.1307],\n",
       "                      ...,\n",
       "                      [ 0.1340,  0.1622, -0.0292,  ..., -0.1184,  0.0377,  0.1605],\n",
       "                      [-0.1686, -0.1651,  0.1192,  ..., -0.1859,  0.1019,  0.1966],\n",
       "                      [-0.0172,  0.0853,  0.0735,  ..., -0.0350, -0.1465, -0.1205]])),\n",
       "             ('swinViT.layers1.0.blocks.0.mlp.linear1.bias',\n",
       "              tensor([ 0.2036,  0.0770,  0.1775, -0.1777,  0.1704, -0.0054, -0.0781, -0.1135,\n",
       "                      -0.1586, -0.1122, -0.1247, -0.1784, -0.0873, -0.1334,  0.0273,  0.1083,\n",
       "                      -0.0901,  0.1798, -0.0170, -0.0659, -0.0444,  0.0999, -0.0745,  0.1205,\n",
       "                      -0.0776, -0.1859, -0.0065,  0.1747,  0.0574,  0.0104,  0.1268, -0.1646,\n",
       "                      -0.1260,  0.1384, -0.1777, -0.1535, -0.1295, -0.1485,  0.0924,  0.0252,\n",
       "                       0.1206, -0.1326, -0.0617,  0.1161, -0.0458,  0.0679, -0.0100,  0.0461,\n",
       "                      -0.1576, -0.0067, -0.0143,  0.0509,  0.1925, -0.1778, -0.0787, -0.0023,\n",
       "                      -0.1081, -0.0493, -0.1269,  0.1643, -0.0146,  0.0512,  0.0620,  0.0129,\n",
       "                      -0.1212, -0.1226, -0.1163,  0.1145, -0.0019,  0.1058,  0.0304,  0.0417,\n",
       "                       0.1245, -0.1248, -0.1241, -0.0797,  0.0561, -0.1770,  0.0056,  0.0745,\n",
       "                      -0.1800,  0.0993, -0.0973, -0.1460,  0.1216, -0.0088, -0.1526,  0.0785,\n",
       "                      -0.1489,  0.0226, -0.1932, -0.1503,  0.0461, -0.1298,  0.0444, -0.0895])),\n",
       "             ('swinViT.layers1.0.blocks.0.mlp.linear2.weight',\n",
       "              tensor([[-0.0668, -0.0975,  0.0115,  ...,  0.0763,  0.0991, -0.0228],\n",
       "                      [-0.0546, -0.0610,  0.0784,  ...,  0.0728,  0.0814,  0.0390],\n",
       "                      [ 0.0230,  0.0424,  0.0106,  ..., -0.0970,  0.0149, -0.0812],\n",
       "                      ...,\n",
       "                      [-0.0957,  0.0088,  0.0114,  ..., -0.0422, -0.0106, -0.0341],\n",
       "                      [-0.0015,  0.0856,  0.0317,  ..., -0.0322,  0.0289, -0.0599],\n",
       "                      [-0.0444, -0.0411,  0.0322,  ..., -0.0730,  0.0233, -0.0493]])),\n",
       "             ('swinViT.layers1.0.blocks.0.mlp.linear2.bias',\n",
       "              tensor([ 0.0064,  0.0151,  0.0775, -0.0296, -0.0063, -0.0882, -0.0529, -0.0679,\n",
       "                       0.0166, -0.0245, -0.0845, -0.0698,  0.0957,  0.0876, -0.0736,  0.0344,\n",
       "                       0.0053,  0.0281, -0.0778,  0.0604, -0.0337,  0.0433,  0.0611, -0.0671])),\n",
       "             ('swinViT.layers1.0.blocks.1.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers1.0.blocks.1.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.relative_position_bias_table',\n",
       "              tensor([[ 0.0069, -0.0264,  0.0118],\n",
       "                      [-0.0009,  0.0171, -0.0169],\n",
       "                      [ 0.0073,  0.0574,  0.0070],\n",
       "                      ...,\n",
       "                      [-0.0341,  0.0073, -0.0156],\n",
       "                      [ 0.0023,  0.0372, -0.0003],\n",
       "                      [-0.0211, -0.0039,  0.0091]])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.qkv.weight',\n",
       "              tensor([[-0.1514, -0.0537,  0.1971,  ..., -0.1178,  0.0771,  0.0318],\n",
       "                      [ 0.0958,  0.0322,  0.0326,  ..., -0.1053, -0.0523,  0.1142],\n",
       "                      [-0.1680, -0.1663, -0.0019,  ..., -0.1916,  0.0233, -0.0777],\n",
       "                      ...,\n",
       "                      [-0.1871, -0.1451, -0.0438,  ..., -0.0401, -0.1275, -0.1454],\n",
       "                      [-0.1434,  0.0853,  0.0288,  ..., -0.1629, -0.0232, -0.0034],\n",
       "                      [ 0.0756, -0.0223,  0.0272,  ...,  0.0231, -0.1043, -0.1763]])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.qkv.bias',\n",
       "              tensor([ 0.1672, -0.1752,  0.0299, -0.0274, -0.1767, -0.1782,  0.0495, -0.1724,\n",
       "                       0.0303, -0.1065,  0.0279,  0.1227, -0.1304, -0.0436, -0.0247,  0.0905,\n",
       "                      -0.2036,  0.1207, -0.0709,  0.0386,  0.0574, -0.0556, -0.0521, -0.0612,\n",
       "                       0.1684, -0.1303, -0.1784,  0.1980,  0.0393, -0.1465, -0.0097,  0.1110,\n",
       "                       0.2006,  0.1385, -0.1220, -0.0103, -0.1224, -0.0390, -0.1198, -0.1669,\n",
       "                      -0.0040,  0.1998,  0.1413, -0.0512,  0.1973,  0.0518,  0.0291, -0.0523,\n",
       "                       0.1822,  0.0370, -0.1493,  0.0334,  0.1518, -0.1543,  0.0980,  0.1929,\n",
       "                      -0.1956,  0.0255,  0.0478, -0.1585,  0.1418, -0.0293, -0.1041,  0.0715,\n",
       "                      -0.0624, -0.0699, -0.1989,  0.1646, -0.1025,  0.0027,  0.0512,  0.1947])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.proj.weight',\n",
       "              tensor([[-0.1713,  0.1262, -0.0023, -0.1949, -0.1637,  0.0920,  0.0781,  0.1179,\n",
       "                        0.1573,  0.1698, -0.1774, -0.1347,  0.1326,  0.1554, -0.1311,  0.1533,\n",
       "                       -0.0152, -0.1104,  0.0063, -0.0260, -0.1076,  0.0200, -0.0947,  0.0373],\n",
       "                      [ 0.1247, -0.0820,  0.1048, -0.1712,  0.1513, -0.1736, -0.1619, -0.0870,\n",
       "                        0.1725, -0.1708, -0.1761, -0.1735,  0.0913, -0.0631,  0.1702,  0.0289,\n",
       "                        0.0232, -0.0690, -0.0966, -0.0204, -0.1385, -0.1292,  0.1435, -0.0388],\n",
       "                      [ 0.0330,  0.1883, -0.1906,  0.1156,  0.1123,  0.0692, -0.0039,  0.1632,\n",
       "                        0.0788, -0.1769,  0.1691,  0.0272,  0.0590,  0.1316, -0.0517,  0.0164,\n",
       "                       -0.0920,  0.0405,  0.1482, -0.1172, -0.0256,  0.1544, -0.1608, -0.0060],\n",
       "                      [-0.1048, -0.1401,  0.0546, -0.1926,  0.0029, -0.0758,  0.1343, -0.1891,\n",
       "                        0.0070,  0.0320,  0.1397,  0.1866,  0.1279, -0.1084, -0.1805,  0.0680,\n",
       "                        0.0115, -0.1410,  0.0761,  0.1120, -0.1020,  0.0766,  0.1418, -0.1350],\n",
       "                      [ 0.1362,  0.0689, -0.0975, -0.0614, -0.1111, -0.0015, -0.1298,  0.1279,\n",
       "                        0.0624,  0.0500,  0.1786, -0.0795,  0.1793, -0.0657, -0.0292, -0.0757,\n",
       "                       -0.0036,  0.1074, -0.1394,  0.1409,  0.1071,  0.0961, -0.0850,  0.1191],\n",
       "                      [ 0.1644,  0.0706, -0.1902, -0.1358, -0.1316,  0.1937,  0.0415, -0.1870,\n",
       "                       -0.0041,  0.1649, -0.1825,  0.0186, -0.0425, -0.1010,  0.1308,  0.0817,\n",
       "                       -0.1901,  0.1631, -0.0954,  0.0958, -0.1284,  0.1041,  0.0340,  0.0593],\n",
       "                      [-0.0160,  0.1543,  0.1397, -0.0802,  0.0215,  0.1724, -0.1726,  0.0074,\n",
       "                       -0.1169, -0.0404, -0.0207,  0.0261,  0.0579,  0.1567, -0.1869,  0.1380,\n",
       "                       -0.0931, -0.0959,  0.1647, -0.0546,  0.0682,  0.1869, -0.0262,  0.0989],\n",
       "                      [-0.0038,  0.0917,  0.0490, -0.0330,  0.0156,  0.1360, -0.0589,  0.0240,\n",
       "                       -0.1390,  0.1083, -0.1264, -0.0141, -0.1971, -0.0458,  0.1471,  0.1805,\n",
       "                       -0.0708,  0.0199, -0.1932,  0.0742,  0.1406, -0.1362,  0.0099,  0.1669],\n",
       "                      [-0.0141, -0.1071,  0.0826, -0.1380, -0.0327, -0.1406, -0.0253, -0.0710,\n",
       "                       -0.2029, -0.0673,  0.0251, -0.1512,  0.0964, -0.0801,  0.0986,  0.0959,\n",
       "                       -0.1991,  0.1329,  0.1515, -0.0633, -0.0301, -0.0969, -0.1352, -0.1659],\n",
       "                      [ 0.1700, -0.1360,  0.0566, -0.1197, -0.0465,  0.0420,  0.1333, -0.1004,\n",
       "                       -0.0148,  0.0792, -0.0025, -0.1627, -0.0467,  0.1141,  0.1935, -0.1837,\n",
       "                       -0.0576,  0.0406,  0.0408, -0.0987, -0.1989, -0.1080, -0.1430,  0.1978],\n",
       "                      [-0.1424,  0.0873, -0.1843, -0.0906, -0.1293, -0.1688, -0.0542,  0.1324,\n",
       "                       -0.0871, -0.1578,  0.1659,  0.0268, -0.1630, -0.1083,  0.1254,  0.0067,\n",
       "                       -0.0386, -0.1826, -0.0599, -0.1327, -0.0491, -0.0655,  0.1215,  0.1341],\n",
       "                      [ 0.1919,  0.1328,  0.0613,  0.0243,  0.1938,  0.0997, -0.0525,  0.1757,\n",
       "                       -0.0246, -0.0864, -0.1892, -0.1996, -0.0282, -0.0047, -0.1796, -0.1320,\n",
       "                       -0.1466,  0.1359, -0.0451,  0.1060, -0.1075, -0.0491, -0.1770, -0.0653],\n",
       "                      [ 0.0570,  0.0965, -0.0927,  0.0686, -0.0471,  0.0730, -0.1295,  0.1551,\n",
       "                        0.0297, -0.0508,  0.0808, -0.1355,  0.1996,  0.0783,  0.1749,  0.1800,\n",
       "                        0.1934, -0.1674,  0.0758,  0.0985,  0.1042,  0.1662,  0.1303, -0.0313],\n",
       "                      [ 0.1192, -0.0992,  0.0540, -0.2041, -0.0209,  0.1560, -0.1033, -0.1829,\n",
       "                       -0.1987, -0.0588, -0.0768, -0.0278,  0.1804,  0.1162, -0.0927, -0.0390,\n",
       "                       -0.0407,  0.0853, -0.1129, -0.2027,  0.0639,  0.1660,  0.2007,  0.1515],\n",
       "                      [ 0.0745, -0.0946, -0.1383,  0.1088, -0.1666, -0.0087, -0.0103,  0.1363,\n",
       "                        0.1863,  0.0079,  0.1993, -0.0496,  0.1285, -0.0508, -0.0218,  0.1200,\n",
       "                       -0.1940, -0.2038, -0.0979, -0.1786, -0.1336, -0.1273,  0.1293,  0.1316],\n",
       "                      [ 0.1914, -0.1872,  0.1039,  0.0230, -0.0730,  0.1605,  0.1158, -0.1235,\n",
       "                       -0.0443,  0.0916, -0.0281,  0.0196, -0.1675, -0.1790, -0.0685,  0.0206,\n",
       "                        0.0476,  0.1744,  0.0689,  0.1421,  0.0440, -0.1999, -0.0059,  0.1185],\n",
       "                      [-0.1391,  0.1022, -0.0316,  0.1112,  0.1758, -0.1284,  0.1825,  0.1323,\n",
       "                        0.1267, -0.0961, -0.0836, -0.1070,  0.0959, -0.0423,  0.1657, -0.1600,\n",
       "                        0.1814,  0.1747,  0.1006,  0.1118, -0.1533, -0.1569,  0.1517,  0.1589],\n",
       "                      [-0.0103,  0.1473,  0.0761, -0.1508,  0.1238, -0.1294,  0.2008, -0.1405,\n",
       "                        0.1827, -0.1344,  0.0370,  0.1400, -0.2033, -0.1514, -0.1125, -0.1416,\n",
       "                        0.1168,  0.1141,  0.0954, -0.1381, -0.0194,  0.1164, -0.0413,  0.0351],\n",
       "                      [ 0.0384,  0.0606, -0.0136,  0.1167,  0.1259, -0.0472, -0.0331,  0.0910,\n",
       "                       -0.1317,  0.0306,  0.0826, -0.1244,  0.1693, -0.0782, -0.1787,  0.0911,\n",
       "                        0.0866, -0.1260, -0.1217,  0.0620,  0.0827,  0.0531,  0.1638, -0.0954],\n",
       "                      [-0.1384,  0.0824,  0.1268, -0.1743,  0.2027,  0.0757, -0.1488, -0.0233,\n",
       "                        0.1979, -0.1797, -0.0462, -0.1575, -0.1302, -0.1752,  0.0567,  0.1850,\n",
       "                       -0.1828,  0.0932, -0.1008,  0.0453, -0.1652, -0.1038, -0.0811,  0.1518],\n",
       "                      [-0.0103,  0.1001, -0.1911,  0.0136,  0.0552,  0.0333, -0.1303, -0.1484,\n",
       "                       -0.1605,  0.1538,  0.0343,  0.1099, -0.0681,  0.1401,  0.1128,  0.0034,\n",
       "                        0.2032,  0.0018,  0.0409, -0.1255, -0.1718, -0.0659, -0.1592,  0.1856],\n",
       "                      [ 0.0076,  0.1631,  0.1804, -0.1681,  0.1870, -0.1018,  0.0189, -0.0579,\n",
       "                       -0.0262, -0.1596,  0.1822,  0.0449, -0.0182,  0.0937, -0.0829,  0.0203,\n",
       "                        0.1663, -0.0537,  0.1817, -0.1560, -0.2036, -0.0286, -0.0915,  0.0544],\n",
       "                      [ 0.1731,  0.1968, -0.1173,  0.1692, -0.0677, -0.1822,  0.0729,  0.0170,\n",
       "                       -0.0340,  0.1753,  0.1827,  0.0355, -0.1696,  0.1412, -0.0464,  0.0356,\n",
       "                        0.0087,  0.1408,  0.1311,  0.1787,  0.0859,  0.1395, -0.1846,  0.0990],\n",
       "                      [ 0.0729,  0.1801,  0.0391, -0.0736, -0.0063,  0.1514, -0.0482, -0.1166,\n",
       "                       -0.0528, -0.0997,  0.0893,  0.0352,  0.1025, -0.0435, -0.0993,  0.0722,\n",
       "                        0.1872, -0.1156,  0.1464,  0.0925,  0.0943,  0.1779, -0.1555,  0.1890]])),\n",
       "             ('swinViT.layers1.0.blocks.1.attn.proj.bias',\n",
       "              tensor([-0.0701, -0.1644,  0.0197, -0.0109, -0.0093,  0.0474,  0.1412, -0.0392,\n",
       "                       0.1371,  0.1708,  0.1444,  0.1245, -0.0381,  0.1158,  0.1725,  0.1500,\n",
       "                       0.0659, -0.1955,  0.1283,  0.1316, -0.1166,  0.1977,  0.1183, -0.1034])),\n",
       "             ('swinViT.layers1.0.blocks.1.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers1.0.blocks.1.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers1.0.blocks.1.mlp.linear1.weight',\n",
       "              tensor([[-0.1023, -0.1321,  0.0367,  ..., -0.0025,  0.0209, -0.1867],\n",
       "                      [ 0.1753, -0.1072,  0.1156,  ..., -0.1172, -0.0531,  0.1754],\n",
       "                      [-0.0983, -0.1220,  0.0956,  ...,  0.1338,  0.0404, -0.1486],\n",
       "                      ...,\n",
       "                      [-0.1437,  0.0612, -0.0465,  ...,  0.0513, -0.1041, -0.0044],\n",
       "                      [-0.0667,  0.0923, -0.1526,  ..., -0.0341,  0.1889, -0.0414],\n",
       "                      [ 0.0687,  0.0325, -0.0380,  ...,  0.1379, -0.1780, -0.0244]])),\n",
       "             ('swinViT.layers1.0.blocks.1.mlp.linear1.bias',\n",
       "              tensor([ 0.1047,  0.2013, -0.1154,  0.1895,  0.0414, -0.0873, -0.1843, -0.1387,\n",
       "                      -0.0596,  0.1905,  0.0283,  0.1298, -0.0146, -0.1436,  0.1364,  0.0114,\n",
       "                       0.0036, -0.1412,  0.0432,  0.1774, -0.1157, -0.0527,  0.1067,  0.0278,\n",
       "                      -0.1356, -0.1396, -0.1458,  0.0821, -0.0806, -0.0239,  0.0665, -0.0155,\n",
       "                      -0.0407, -0.0028,  0.1995, -0.0887,  0.0835, -0.0949,  0.1528, -0.1037,\n",
       "                       0.1442, -0.1328,  0.1299,  0.1112,  0.1574,  0.1395,  0.1868,  0.1504,\n",
       "                      -0.0451, -0.1334, -0.1731,  0.1974, -0.1665, -0.0018,  0.1709,  0.1835,\n",
       "                      -0.0401,  0.0275, -0.1474,  0.0743, -0.0607,  0.2028, -0.1002, -0.0997,\n",
       "                      -0.0278,  0.0676,  0.0607, -0.0600, -0.1352,  0.1889, -0.1910, -0.1480,\n",
       "                       0.0863, -0.0704,  0.1903, -0.0898, -0.1776, -0.1961,  0.0133,  0.0512,\n",
       "                       0.1361, -0.1406,  0.0712,  0.0495,  0.1926, -0.0408,  0.0428, -0.0867,\n",
       "                       0.1667, -0.1673,  0.2010, -0.0327, -0.0611,  0.0246,  0.0483, -0.1679])),\n",
       "             ('swinViT.layers1.0.blocks.1.mlp.linear2.weight',\n",
       "              tensor([[-0.0481, -0.0146,  0.0211,  ...,  0.0569,  0.0683, -0.0083],\n",
       "                      [-0.0883, -0.0707,  0.0847,  ..., -0.0213, -0.0437,  0.0080],\n",
       "                      [-0.0272,  0.0816,  0.0712,  ...,  0.0501,  0.0540, -0.0431],\n",
       "                      ...,\n",
       "                      [-0.0175,  0.0234,  0.0326,  ...,  0.0172, -0.0158, -0.0236],\n",
       "                      [ 0.0449,  0.0403,  0.0178,  ...,  0.0666, -0.0407,  0.0801],\n",
       "                      [-0.0924,  0.0941, -0.0594,  ...,  0.0273, -0.0706,  0.0607]])),\n",
       "             ('swinViT.layers1.0.blocks.1.mlp.linear2.bias',\n",
       "              tensor([-0.0018, -0.0439,  0.0259,  0.0524,  0.0392, -0.0273,  0.0968, -0.0118,\n",
       "                       0.0036,  0.0906, -0.0266,  0.0516, -0.0494, -0.0711,  0.0278, -0.0213,\n",
       "                      -0.0778,  0.0047, -0.0066, -0.0544, -0.0414, -0.0775,  0.0689, -0.0304])),\n",
       "             ('swinViT.layers1.0.downsample.reduction.weight',\n",
       "              tensor([[-0.0238,  0.0214,  0.0464,  ...,  0.0350,  0.0296, -0.0702],\n",
       "                      [-0.0521, -0.0431, -0.0087,  ...,  0.0375, -0.0047, -0.0560],\n",
       "                      [ 0.0637,  0.0665, -0.0217,  ..., -0.0670, -0.0358,  0.0316],\n",
       "                      ...,\n",
       "                      [-0.0279,  0.0383, -0.0179,  ..., -0.0239,  0.0643,  0.0020],\n",
       "                      [-0.0285, -0.0080,  0.0124,  ...,  0.0385,  0.0177,  0.0480],\n",
       "                      [ 0.0087,  0.0641,  0.0167,  ...,  0.0325, -0.0155, -0.0273]])),\n",
       "             ('swinViT.layers1.0.downsample.norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers1.0.downsample.norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers2.0.blocks.0.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers2.0.blocks.0.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.relative_position_bias_table',\n",
       "              tensor([[ 0.0125,  0.0281, -0.0280, -0.0068, -0.0240,  0.0021],\n",
       "                      [ 0.0096, -0.0174, -0.0145,  0.0065,  0.0362,  0.0104],\n",
       "                      [-0.0150,  0.0156, -0.0061,  0.0151,  0.0022, -0.0034],\n",
       "                      ...,\n",
       "                      [-0.0173,  0.0133,  0.0064, -0.0177, -0.0034, -0.0166],\n",
       "                      [ 0.0112,  0.0005, -0.0090, -0.0147,  0.0431, -0.0109],\n",
       "                      [ 0.0312, -0.0247, -0.0050, -0.0164,  0.0228, -0.0026]])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.qkv.weight',\n",
       "              tensor([[ 0.0771, -0.0682,  0.0271,  ..., -0.0132,  0.1134, -0.0379],\n",
       "                      [ 0.1307, -0.0902,  0.1133,  ..., -0.1102, -0.0246,  0.1038],\n",
       "                      [-0.1148, -0.0863, -0.0714,  ..., -0.0830,  0.0141, -0.0945],\n",
       "                      ...,\n",
       "                      [ 0.0178,  0.1364,  0.1121,  ..., -0.0313, -0.1206,  0.0356],\n",
       "                      [ 0.0945,  0.0793,  0.0143,  ..., -0.1013, -0.0391,  0.0383],\n",
       "                      [ 0.1027,  0.0718, -0.0516,  ...,  0.0928, -0.1159,  0.0491]])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.qkv.bias',\n",
       "              tensor([-0.0040,  0.0222,  0.0866, -0.1246,  0.1315, -0.1436,  0.1042,  0.0572,\n",
       "                       0.0718,  0.0435, -0.1147, -0.1160, -0.1110, -0.0635,  0.1344, -0.1416,\n",
       "                       0.0330, -0.1374,  0.0933,  0.1102,  0.0707, -0.0836, -0.0642, -0.0346,\n",
       "                       0.0171, -0.0209, -0.0499, -0.0861,  0.1393,  0.0764, -0.0495,  0.0306,\n",
       "                       0.1272, -0.1120,  0.0753, -0.0683, -0.1025, -0.0064, -0.0597,  0.1047,\n",
       "                      -0.0291, -0.1440,  0.1330,  0.0162, -0.0152,  0.1047, -0.0925, -0.0228,\n",
       "                      -0.1066, -0.1181,  0.0364, -0.1079,  0.0229, -0.1382, -0.0131,  0.1114,\n",
       "                       0.0565,  0.0980, -0.0263, -0.0152,  0.0833, -0.0338, -0.1191, -0.0085,\n",
       "                       0.0760, -0.1169,  0.0978, -0.0444, -0.1009, -0.1057,  0.0516, -0.1383,\n",
       "                       0.0422, -0.1023,  0.1119,  0.1305,  0.0455, -0.0360, -0.0255, -0.0185,\n",
       "                      -0.1129, -0.1392,  0.1436,  0.0352,  0.0286,  0.0673,  0.0862, -0.0964,\n",
       "                      -0.1134, -0.0496, -0.1105, -0.0055,  0.0919,  0.0467,  0.0084,  0.0436,\n",
       "                       0.0838, -0.0340,  0.1276, -0.0923,  0.0585, -0.1164, -0.0617,  0.0244,\n",
       "                       0.0722,  0.0548,  0.0407, -0.0440,  0.0176,  0.1204,  0.0009, -0.0560,\n",
       "                      -0.0238,  0.0203, -0.1232, -0.0967,  0.1111,  0.0469,  0.0988,  0.0359,\n",
       "                       0.0433,  0.0956,  0.1091, -0.0374, -0.0182,  0.0294, -0.1146,  0.0869,\n",
       "                       0.1016,  0.0603,  0.0330, -0.1111, -0.0067, -0.0064, -0.0274, -0.0586,\n",
       "                       0.1102, -0.0916,  0.0822, -0.1099, -0.1022, -0.1310,  0.1176,  0.1320])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.proj.weight',\n",
       "              tensor([[ 0.0521, -0.1319, -0.0523,  ..., -0.0785,  0.1172, -0.0763],\n",
       "                      [-0.1441,  0.0429,  0.0623,  ..., -0.0663,  0.1299, -0.1162],\n",
       "                      [ 0.1255, -0.0635,  0.1293,  ..., -0.1021, -0.1084, -0.0804],\n",
       "                      ...,\n",
       "                      [-0.0757,  0.0439, -0.0492,  ...,  0.0790,  0.0169, -0.0174],\n",
       "                      [-0.1047, -0.1266, -0.0138,  ...,  0.0383,  0.0940, -0.0306],\n",
       "                      [-0.1369,  0.0843, -0.1351,  ...,  0.0834,  0.1226,  0.0905]])),\n",
       "             ('swinViT.layers2.0.blocks.0.attn.proj.bias',\n",
       "              tensor([-4.6223e-02, -4.4437e-02,  4.8938e-02,  7.8688e-02, -1.2622e-01,\n",
       "                       2.5076e-02,  1.0942e-02, -7.5054e-02,  2.7716e-05,  6.1485e-02,\n",
       "                      -9.2504e-02,  2.7922e-02, -5.3429e-02, -5.8073e-02,  1.1534e-01,\n",
       "                       8.3747e-02, -1.3526e-01,  3.9713e-02,  2.7844e-02,  4.4099e-02,\n",
       "                      -1.0947e-01,  7.1202e-02,  9.6700e-02,  8.7660e-02,  1.1090e-01,\n",
       "                       1.3109e-01, -1.0784e-01,  2.1721e-04,  1.4028e-01, -1.6367e-03,\n",
       "                       9.3952e-02, -2.8265e-02, -5.4117e-02, -6.1009e-02,  5.2231e-02,\n",
       "                       1.2469e-01,  3.6859e-02,  9.7816e-03,  7.8304e-02,  7.4886e-02,\n",
       "                       1.0237e-01, -4.1921e-02, -1.0152e-01,  1.0892e-01, -1.8127e-02,\n",
       "                      -4.9130e-02,  5.5454e-02, -3.9080e-02])),\n",
       "             ('swinViT.layers2.0.blocks.0.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers2.0.blocks.0.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers2.0.blocks.0.mlp.linear1.weight',\n",
       "              tensor([[ 0.0469, -0.1268,  0.0253,  ..., -0.0075, -0.0715,  0.0379],\n",
       "                      [ 0.0193, -0.0751,  0.0025,  ..., -0.0659, -0.0635, -0.1427],\n",
       "                      [-0.0530, -0.0784, -0.0465,  ..., -0.0579,  0.0740, -0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0184, -0.1422, -0.0242,  ..., -0.1315,  0.0159,  0.1178],\n",
       "                      [ 0.0512, -0.0662, -0.1318,  ...,  0.0400, -0.1006, -0.1257],\n",
       "                      [ 0.0338, -0.0585,  0.0871,  ...,  0.1161, -0.0442, -0.0800]])),\n",
       "             ('swinViT.layers2.0.blocks.0.mlp.linear1.bias',\n",
       "              tensor([ 0.0580, -0.0778, -0.0924, -0.0415,  0.0526,  0.0639,  0.0822,  0.0518,\n",
       "                       0.1024, -0.0138,  0.0460, -0.1339, -0.0373, -0.0667, -0.1325,  0.0820,\n",
       "                      -0.0147, -0.1275, -0.0046, -0.0777,  0.0471,  0.0550, -0.0327,  0.0565,\n",
       "                       0.1254,  0.1363,  0.1043, -0.1354,  0.1101, -0.0808, -0.0543, -0.1344,\n",
       "                       0.0604, -0.0205,  0.1048,  0.1229, -0.0472,  0.1308,  0.1146,  0.0852,\n",
       "                      -0.0226,  0.0689,  0.0411,  0.0589, -0.1259,  0.0642,  0.0770,  0.0324,\n",
       "                      -0.0569,  0.0149, -0.1397,  0.0405, -0.1083, -0.1419, -0.0193, -0.0975,\n",
       "                       0.1286, -0.0254,  0.1328,  0.1032,  0.0414, -0.1262,  0.0277, -0.0802,\n",
       "                       0.0635,  0.0240, -0.1376, -0.0711,  0.0866, -0.0846, -0.0715,  0.0906,\n",
       "                      -0.1254,  0.0907, -0.0400,  0.0961,  0.1179, -0.0721,  0.0018, -0.0269,\n",
       "                      -0.1369, -0.0688,  0.0564,  0.0234, -0.1336, -0.0741,  0.0581, -0.1146,\n",
       "                      -0.1106, -0.0040, -0.0933, -0.1232,  0.1229, -0.0106,  0.0876,  0.0811,\n",
       "                       0.0107,  0.1139,  0.1296,  0.0490,  0.0730, -0.0888,  0.1076, -0.0311,\n",
       "                      -0.0991,  0.0706,  0.0066, -0.1169,  0.1198,  0.0671, -0.0515, -0.0862,\n",
       "                      -0.1186,  0.0994, -0.0726,  0.0432, -0.1206, -0.1101, -0.1089, -0.0993,\n",
       "                      -0.0876, -0.0178,  0.0675,  0.0138,  0.0598,  0.1226, -0.1070,  0.0781,\n",
       "                      -0.0662,  0.0561,  0.0122,  0.0124,  0.0555, -0.1110,  0.0633,  0.0958,\n",
       "                      -0.1257,  0.0609, -0.0451, -0.0899, -0.0223, -0.0293,  0.0518,  0.1441,\n",
       "                       0.0208, -0.0905, -0.1037, -0.0388, -0.0999,  0.1260, -0.0689, -0.0262,\n",
       "                       0.0173,  0.0099, -0.0624,  0.1081, -0.0569, -0.0024,  0.1013,  0.0591,\n",
       "                      -0.1311, -0.0167, -0.1205, -0.0307,  0.1228, -0.1372,  0.0215,  0.1129,\n",
       "                       0.0007,  0.0012,  0.1263,  0.0716,  0.1129,  0.0074, -0.0733, -0.0354,\n",
       "                       0.0351, -0.1222,  0.1144,  0.0161, -0.1379,  0.1225,  0.0917,  0.1390,\n",
       "                      -0.0707, -0.0037, -0.1123, -0.0838, -0.1059,  0.0135,  0.0410, -0.0952])),\n",
       "             ('swinViT.layers2.0.blocks.0.mlp.linear2.weight',\n",
       "              tensor([[-0.0487,  0.0462, -0.0217,  ..., -0.0604,  0.0440,  0.0059],\n",
       "                      [ 0.0391, -0.0536, -0.0071,  ..., -0.0702,  0.0080,  0.0041],\n",
       "                      [-0.0479,  0.0156,  0.0133,  ...,  0.0482, -0.0044,  0.0581],\n",
       "                      ...,\n",
       "                      [-0.0215,  0.0227,  0.0246,  ...,  0.0489, -0.0129,  0.0562],\n",
       "                      [ 0.0224, -0.0245, -0.0009,  ..., -0.0533,  0.0555, -0.0149],\n",
       "                      [ 0.0306,  0.0495,  0.0394,  ..., -0.0446, -0.0064,  0.0520]])),\n",
       "             ('swinViT.layers2.0.blocks.0.mlp.linear2.bias',\n",
       "              tensor([ 0.0453,  0.0520,  0.0680, -0.0123, -0.0098,  0.0217,  0.0339,  0.0111,\n",
       "                       0.0590, -0.0229, -0.0091,  0.0365, -0.0304, -0.0650, -0.0350, -0.0505,\n",
       "                       0.0693,  0.0233, -0.0612, -0.0449,  0.0590,  0.0489,  0.0010, -0.0154,\n",
       "                       0.0390, -0.0051, -0.0325, -0.0241, -0.0718, -0.0437, -0.0448, -0.0541,\n",
       "                       0.0358, -0.0409, -0.0541, -0.0610,  0.0710, -0.0615, -0.0278, -0.0673,\n",
       "                      -0.0537,  0.0435, -0.0693,  0.0286, -0.0708, -0.0439, -0.0180, -0.0668])),\n",
       "             ('swinViT.layers2.0.blocks.1.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers2.0.blocks.1.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.relative_position_bias_table',\n",
       "              tensor([[-0.0033,  0.0015, -0.0041, -0.0186,  0.0301, -0.0096],\n",
       "                      [ 0.0156, -0.0240,  0.0109, -0.0204,  0.0188, -0.0187],\n",
       "                      [ 0.0098, -0.0202, -0.0040,  0.0243,  0.0145,  0.0193],\n",
       "                      ...,\n",
       "                      [-0.0350,  0.0104,  0.0118, -0.0037,  0.0028,  0.0316],\n",
       "                      [ 0.0253, -0.0122, -0.0081, -0.0034,  0.0247, -0.0043],\n",
       "                      [-0.0149,  0.0587, -0.0191, -0.0220, -0.0229,  0.0169]])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.qkv.weight',\n",
       "              tensor([[ 0.1081,  0.1401,  0.0686,  ...,  0.0501, -0.0666,  0.0159],\n",
       "                      [ 0.0965,  0.0140, -0.0259,  ...,  0.0062,  0.0646, -0.0022],\n",
       "                      [-0.1182, -0.0885,  0.0828,  ...,  0.0222, -0.1052,  0.0008],\n",
       "                      ...,\n",
       "                      [-0.0662,  0.1218, -0.1378,  ...,  0.0470, -0.1011,  0.0965],\n",
       "                      [-0.1308, -0.1011,  0.0620,  ..., -0.0881, -0.1141, -0.1411],\n",
       "                      [ 0.0088,  0.0762,  0.0897,  ..., -0.0475, -0.0107, -0.1257]])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.qkv.bias',\n",
       "              tensor([ 0.1285,  0.0655,  0.0312, -0.0684, -0.0016, -0.1210, -0.1135, -0.0932,\n",
       "                      -0.1020, -0.0562, -0.0808,  0.1020, -0.1317,  0.0899, -0.1065, -0.0057,\n",
       "                      -0.1128,  0.0438, -0.0321, -0.1026, -0.1292, -0.0771, -0.1304, -0.0246,\n",
       "                       0.1354,  0.1441, -0.0447, -0.0166, -0.0903,  0.0365,  0.0200,  0.1158,\n",
       "                      -0.1315, -0.1233,  0.1257, -0.0342,  0.1178,  0.1089, -0.1248,  0.0552,\n",
       "                      -0.1292, -0.0747,  0.0405,  0.0886,  0.0351, -0.0826,  0.0985, -0.0852,\n",
       "                      -0.1230,  0.0748, -0.0287, -0.0782, -0.0117,  0.0927, -0.0239,  0.1426,\n",
       "                       0.1360,  0.1191, -0.1158,  0.0426, -0.1113,  0.1156,  0.0672, -0.0773,\n",
       "                      -0.0423,  0.0594, -0.0166,  0.1062,  0.0437, -0.0157,  0.1359,  0.0816,\n",
       "                      -0.1169,  0.1197,  0.0021,  0.0820,  0.1308, -0.1394,  0.1437, -0.1368,\n",
       "                      -0.0207, -0.0522,  0.1379, -0.1099,  0.0550,  0.0351, -0.1440, -0.0367,\n",
       "                      -0.1054, -0.0294,  0.1239,  0.0221, -0.0156, -0.0012,  0.1040,  0.0479,\n",
       "                      -0.0210,  0.0083, -0.0745, -0.0744, -0.1290, -0.1277,  0.0702, -0.1104,\n",
       "                       0.1309, -0.0948,  0.0789, -0.1425, -0.0092, -0.0048, -0.0116, -0.0155,\n",
       "                      -0.0298, -0.0681, -0.0059, -0.0153, -0.1436, -0.0327, -0.1196, -0.0331,\n",
       "                      -0.0909, -0.0148, -0.0412, -0.0713,  0.0644,  0.0041,  0.0719, -0.0223,\n",
       "                       0.0498,  0.0047,  0.0977, -0.0337, -0.0393,  0.0586,  0.0267, -0.0070,\n",
       "                       0.0020,  0.0039, -0.1040,  0.0241, -0.0660, -0.0224,  0.0409,  0.0536])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.proj.weight',\n",
       "              tensor([[ 0.0718, -0.1151,  0.0861,  ..., -0.0896,  0.1307, -0.0530],\n",
       "                      [ 0.1035,  0.0769,  0.0025,  ..., -0.0848,  0.1123,  0.1043],\n",
       "                      [ 0.1185, -0.0070, -0.0091,  ..., -0.0870,  0.1151, -0.0664],\n",
       "                      ...,\n",
       "                      [-0.0710,  0.1219,  0.1227,  ...,  0.0676,  0.0797,  0.1200],\n",
       "                      [ 0.0968,  0.0272,  0.0356,  ...,  0.0550,  0.0473,  0.0227],\n",
       "                      [ 0.0509, -0.0411, -0.0734,  ...,  0.1058, -0.0407,  0.0467]])),\n",
       "             ('swinViT.layers2.0.blocks.1.attn.proj.bias',\n",
       "              tensor([-0.1427,  0.1293, -0.1275,  0.0562, -0.0716,  0.0590,  0.0751, -0.0448,\n",
       "                       0.0688, -0.0240, -0.1379,  0.1131, -0.0177, -0.1430,  0.0475, -0.1280,\n",
       "                       0.0549,  0.1293,  0.0266, -0.0659, -0.0064,  0.0740,  0.0070, -0.1207,\n",
       "                       0.1156,  0.0676,  0.0619, -0.0413,  0.1141,  0.0269,  0.1190, -0.1175,\n",
       "                       0.0838,  0.0050, -0.1252,  0.0614,  0.0171, -0.1015,  0.0631,  0.1326,\n",
       "                      -0.0456,  0.0014,  0.1139,  0.1422,  0.0161,  0.1191,  0.0983, -0.0758])),\n",
       "             ('swinViT.layers2.0.blocks.1.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers2.0.blocks.1.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers2.0.blocks.1.mlp.linear1.weight',\n",
       "              tensor([[-0.1220, -0.0585,  0.1163,  ..., -0.1259, -0.0824,  0.0841],\n",
       "                      [ 0.0774,  0.0706,  0.1423,  ..., -0.0830,  0.0883, -0.0169],\n",
       "                      [ 0.0211, -0.0767, -0.0204,  ..., -0.1122,  0.0015,  0.0035],\n",
       "                      ...,\n",
       "                      [ 0.1283,  0.0530,  0.0743,  ..., -0.1301, -0.0048, -0.1142],\n",
       "                      [ 0.0822, -0.0842, -0.1053,  ...,  0.0539, -0.0574,  0.0184],\n",
       "                      [-0.0286,  0.0930,  0.0125,  ...,  0.0463, -0.1116, -0.1079]])),\n",
       "             ('swinViT.layers2.0.blocks.1.mlp.linear1.bias',\n",
       "              tensor([ 0.0982, -0.0994,  0.0167,  0.1086, -0.0095,  0.1410,  0.1257, -0.0175,\n",
       "                      -0.0540,  0.0497,  0.0721,  0.0849,  0.0302,  0.0562,  0.0102, -0.0388,\n",
       "                      -0.1364, -0.0283, -0.0411,  0.1220, -0.1143, -0.0022, -0.0856, -0.0352,\n",
       "                       0.0011, -0.1427,  0.0689,  0.0785, -0.0883,  0.1025,  0.1438, -0.1121,\n",
       "                       0.1143, -0.0361,  0.1239,  0.1404, -0.0645,  0.0020,  0.0988,  0.1186,\n",
       "                      -0.0620,  0.1051,  0.0118,  0.1244, -0.0689,  0.0657,  0.0942,  0.0243,\n",
       "                       0.0792,  0.0521,  0.1269,  0.1031,  0.1006, -0.1419, -0.0189, -0.0234,\n",
       "                      -0.0858,  0.0936, -0.0828, -0.1434,  0.0253, -0.1357, -0.1213, -0.0635,\n",
       "                      -0.0313, -0.1349,  0.0012,  0.0986, -0.0214, -0.0733,  0.0272,  0.1206,\n",
       "                      -0.0695,  0.0328,  0.1044, -0.0231, -0.0163, -0.0379, -0.0148, -0.1399,\n",
       "                       0.1161,  0.0608, -0.1274,  0.0085,  0.1132,  0.0391, -0.0702,  0.0123,\n",
       "                       0.1291,  0.1023,  0.0094,  0.0519, -0.0523,  0.0101,  0.0120, -0.0797,\n",
       "                      -0.0845,  0.0589, -0.1132,  0.0057,  0.0566,  0.1175,  0.0476, -0.1310,\n",
       "                      -0.0006, -0.0231, -0.0356,  0.0853,  0.0316, -0.0325, -0.0787,  0.0724,\n",
       "                       0.0888, -0.0135,  0.0634,  0.0579,  0.0507, -0.1091,  0.1133, -0.0300,\n",
       "                      -0.0341, -0.0009, -0.1008, -0.1101,  0.0573, -0.0615,  0.1264, -0.0827,\n",
       "                      -0.1027, -0.0474, -0.0910,  0.0074,  0.0272,  0.0431, -0.0679,  0.0578,\n",
       "                       0.0925,  0.0851, -0.0422,  0.0046,  0.0207,  0.0355, -0.0063,  0.0054,\n",
       "                       0.1261,  0.1093, -0.0898,  0.0724,  0.0088,  0.0116,  0.1436, -0.1275,\n",
       "                       0.0754,  0.0761, -0.0619, -0.0130, -0.1220,  0.0964, -0.0201,  0.1405,\n",
       "                       0.1245, -0.0556,  0.1268,  0.1116, -0.0319,  0.0835,  0.0765, -0.1107,\n",
       "                       0.0426,  0.0523, -0.1411,  0.1297, -0.0377,  0.0669,  0.0863,  0.1152,\n",
       "                      -0.0358, -0.1288,  0.0575,  0.0245,  0.0796,  0.1114, -0.0070, -0.0601,\n",
       "                      -0.0023,  0.0664,  0.1218, -0.0327,  0.1128, -0.0998,  0.0755, -0.0440])),\n",
       "             ('swinViT.layers2.0.blocks.1.mlp.linear2.weight',\n",
       "              tensor([[ 0.0079,  0.0650, -0.0304,  ..., -0.0107,  0.0544,  0.0447],\n",
       "                      [-0.0095,  0.0283,  0.0599,  ...,  0.0350, -0.0241,  0.0477],\n",
       "                      [ 0.0235,  0.0114,  0.0356,  ..., -0.0575, -0.0304,  0.0165],\n",
       "                      ...,\n",
       "                      [ 0.0532, -0.0270,  0.0322,  ...,  0.0114,  0.0300,  0.0249],\n",
       "                      [ 0.0599,  0.0255, -0.0712,  ..., -0.0246,  0.0077,  0.0071],\n",
       "                      [-0.0608, -0.0416, -0.0458,  ...,  0.0540, -0.0148,  0.0038]])),\n",
       "             ('swinViT.layers2.0.blocks.1.mlp.linear2.bias',\n",
       "              tensor([-0.0125, -0.0354, -0.0030,  0.0252, -0.0379,  0.0156,  0.0064,  0.0025,\n",
       "                      -0.0031,  0.0588, -0.0527, -0.0214, -0.0512,  0.0245, -0.0677, -0.0256,\n",
       "                      -0.0385, -0.0352,  0.0217, -0.0072, -0.0006,  0.0205,  0.0434, -0.0469,\n",
       "                      -0.0607,  0.0321,  0.0270,  0.0271,  0.0283,  0.0585,  0.0420,  0.0398,\n",
       "                      -0.0219, -0.0370,  0.0224, -0.0522,  0.0349,  0.0458,  0.0550, -0.0314,\n",
       "                      -0.0419, -0.0207,  0.0251, -0.0117, -0.0625,  0.0500,  0.0044, -0.0098])),\n",
       "             ('swinViT.layers2.0.downsample.reduction.weight',\n",
       "              tensor([[-0.0140, -0.0364,  0.0047,  ..., -0.0098, -0.0266, -0.0490],\n",
       "                      [-0.0034, -0.0433, -0.0086,  ..., -0.0214, -0.0347,  0.0211],\n",
       "                      [-0.0243,  0.0017, -0.0384,  ..., -0.0114,  0.0301,  0.0432],\n",
       "                      ...,\n",
       "                      [-0.0377, -0.0012,  0.0194,  ..., -0.0113, -0.0484,  0.0275],\n",
       "                      [-0.0285,  0.0158, -0.0172,  ..., -0.0411, -0.0258,  0.0348],\n",
       "                      [-0.0505,  0.0297, -0.0357,  ...,  0.0024,  0.0290, -0.0103]])),\n",
       "             ('swinViT.layers2.0.downsample.norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers2.0.downsample.norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers3.0.blocks.0.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers3.0.blocks.0.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.relative_position_bias_table',\n",
       "              tensor([[-0.0325,  0.0077,  0.0213,  ..., -0.0068, -0.0032,  0.0274],\n",
       "                      [ 0.0231,  0.0218, -0.0182,  ...,  0.0003, -0.0141,  0.0016],\n",
       "                      [-0.0439, -0.0064, -0.0357,  ...,  0.0058,  0.0167, -0.0270],\n",
       "                      ...,\n",
       "                      [-0.0216,  0.0078,  0.0061,  ...,  0.0033, -0.0112,  0.0311],\n",
       "                      [-0.0393, -0.0045, -0.0250,  ...,  0.0182,  0.0271,  0.0399],\n",
       "                      [-0.0363,  0.0194,  0.0374,  ..., -0.0185,  0.0022,  0.0284]])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.qkv.weight',\n",
       "              tensor([[ 0.0043,  0.0089, -0.0092,  ...,  0.0670, -0.0914, -0.0322],\n",
       "                      [-0.0588, -0.0373, -0.0128,  ...,  0.0127,  0.0776,  0.0055],\n",
       "                      [-0.0827, -0.0618,  0.0014,  ..., -0.0039, -0.0302,  0.0079],\n",
       "                      ...,\n",
       "                      [ 0.0438,  0.0734,  0.0451,  ..., -0.0383, -0.0372,  0.0526],\n",
       "                      [-0.0817,  0.0006, -0.0684,  ...,  0.0589, -0.0566, -0.0507],\n",
       "                      [ 0.0335, -0.0933, -0.0528,  ..., -0.0957, -0.0886, -0.0995]])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.qkv.bias',\n",
       "              tensor([-0.0985, -0.0090, -0.0570, -0.0695, -0.0706,  0.0605,  0.0515, -0.0218,\n",
       "                       0.0740, -0.0495,  0.0903,  0.0361,  0.0917, -0.0727,  0.0838,  0.0449,\n",
       "                       0.0286, -0.0144, -0.0180,  0.0387, -0.0752, -0.0719, -0.0644, -0.0143,\n",
       "                       0.0865,  0.0330,  0.0107,  0.0541, -0.0023,  0.0112, -0.0711, -0.0398,\n",
       "                      -0.0792, -0.0071,  0.0195,  0.0810, -0.0270,  0.0175,  0.0379,  0.0204,\n",
       "                      -0.0082,  0.0079,  0.0769, -0.0754, -0.0158,  0.0857, -0.0970, -0.0545,\n",
       "                       0.0047,  0.0762,  0.0803, -0.0206,  0.0802, -0.1004, -0.0672,  0.0413,\n",
       "                       0.0184, -0.0535, -0.0569, -0.0674,  0.0139,  0.0150,  0.0180,  0.0496,\n",
       "                       0.0440, -0.1005, -0.1014, -0.0253, -0.0044,  0.0380,  0.0631,  0.0700,\n",
       "                      -0.0197,  0.0688, -0.0482, -0.0795, -0.0479,  0.0664,  0.0557,  0.0665,\n",
       "                      -0.0718,  0.0971, -0.0625,  0.0666, -0.0330,  0.0427, -0.0829,  0.0507,\n",
       "                      -0.0454, -0.0375,  0.0998,  0.0088, -0.0217,  0.0273,  0.0749, -0.0507,\n",
       "                      -0.0085,  0.0426, -0.0563, -0.0665,  0.0443,  0.0016, -0.0677,  0.0235,\n",
       "                      -0.0787,  0.0699, -0.0806, -0.0230,  0.0854, -0.0852, -0.0833,  0.0387,\n",
       "                      -0.0497, -0.0099,  0.0866,  0.0436,  0.0931, -0.0581, -0.0262, -0.0721,\n",
       "                      -0.0861, -0.0122,  0.0388,  0.0858,  0.0317, -0.0061,  0.0660,  0.0732,\n",
       "                       0.0859, -0.0257,  0.0027,  0.0920,  0.0542,  0.0430,  0.0721, -0.0497,\n",
       "                       0.0202, -0.0601,  0.0424,  0.0865,  0.0867,  0.0649, -0.0224, -0.0182,\n",
       "                       0.0424, -0.0178, -0.0477,  0.0934, -0.0844,  0.0952, -0.0859, -0.0201,\n",
       "                      -0.0190, -0.0424, -0.0371, -0.0500,  0.0413, -0.0987,  0.0348, -0.0134,\n",
       "                      -0.0124,  0.0818,  0.0373,  0.0736,  0.0194,  0.0177,  0.0587, -0.0972,\n",
       "                      -0.0512, -0.0911,  0.0306,  0.0964, -0.0496, -0.0395,  0.0654,  0.0209,\n",
       "                       0.0816, -0.0058, -0.0982, -0.0682, -0.0855, -0.0350,  0.0033, -0.0649,\n",
       "                      -0.0858,  0.0480, -0.0670, -0.0490,  0.0294,  0.0214, -0.1012,  0.0507,\n",
       "                       0.0894, -0.0482, -0.0820,  0.0098, -0.0724,  0.0481,  0.0271,  0.0710,\n",
       "                      -0.0824, -0.0642,  0.0297, -0.0842,  0.0331,  0.0794, -0.0706, -0.0039,\n",
       "                       0.0562,  0.0208,  0.0004, -0.0189,  0.0984, -0.0810, -0.0229,  0.0923,\n",
       "                      -0.0775, -0.0050, -0.0986, -0.0384,  0.0948, -0.0010,  0.0451,  0.0647,\n",
       "                       0.0496,  0.0963, -0.0112,  0.0432, -0.0469,  0.0371, -0.0589, -0.0135,\n",
       "                       0.0615,  0.0474,  0.0137,  0.0399,  0.0167, -0.0974,  0.0639, -0.0079,\n",
       "                      -0.0029, -0.0828, -0.0530,  0.0249, -0.0175, -0.0855,  0.0130,  0.0435,\n",
       "                      -0.0221,  0.0892, -0.0499,  0.0715,  0.0486,  0.0062, -0.0285,  0.0101,\n",
       "                      -0.0946,  0.0486,  0.0858, -0.0250,  0.0117,  0.0030, -0.0391, -0.0038,\n",
       "                       0.0575, -0.0584,  0.0017, -0.0893, -0.0985, -0.0837,  0.0360, -0.0990,\n",
       "                      -0.0442,  0.0826,  0.0840, -0.0323,  0.0045, -0.0035,  0.0827,  0.0855,\n",
       "                      -0.0722,  0.0533,  0.0671, -0.0895, -0.0678,  0.0560,  0.0275,  0.0148])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.proj.weight',\n",
       "              tensor([[ 1.7224e-02, -8.2875e-03,  6.3578e-02,  ...,  1.2477e-02,\n",
       "                        1.6570e-02, -9.0272e-02],\n",
       "                      [ 3.7528e-02,  4.1194e-05, -7.1622e-02,  ..., -6.3501e-03,\n",
       "                       -6.3971e-04,  4.2964e-02],\n",
       "                      [-3.8917e-02,  8.8944e-02, -8.2056e-02,  ..., -2.4865e-02,\n",
       "                       -4.8484e-02,  1.2447e-02],\n",
       "                      ...,\n",
       "                      [ 2.8151e-02, -2.2151e-02,  4.1593e-02,  ..., -6.1589e-02,\n",
       "                       -9.4972e-02, -4.8402e-02],\n",
       "                      [ 4.0154e-02,  1.7259e-02, -9.9819e-02,  ..., -4.2984e-02,\n",
       "                        1.0206e-01,  6.5547e-02],\n",
       "                      [ 2.9835e-02,  1.4639e-02,  4.7465e-02,  ..., -2.5986e-03,\n",
       "                       -8.9894e-02, -3.8810e-02]])),\n",
       "             ('swinViT.layers3.0.blocks.0.attn.proj.bias',\n",
       "              tensor([ 0.1008,  0.0030,  0.0614,  0.0798, -0.0472, -0.0707,  0.0281, -0.0702,\n",
       "                      -0.0733, -0.0688, -0.0779,  0.0379,  0.0316,  0.0696,  0.0400, -0.0836,\n",
       "                      -0.0904,  0.0966,  0.0550,  0.0399, -0.0422,  0.0505, -0.0566,  0.0996,\n",
       "                       0.0366, -0.0314,  0.0300,  0.1000, -0.0662, -0.0504, -0.0108, -0.0753,\n",
       "                       0.0326,  0.0514, -0.0347, -0.0591,  0.0840,  0.0031,  0.1014, -0.0484,\n",
       "                       0.0148,  0.0887, -0.0310,  0.0649, -0.0196,  0.0432,  0.0500,  0.0416,\n",
       "                      -0.0937, -0.0780, -0.0889,  0.0352,  0.0645, -0.0482, -0.0933,  0.0664,\n",
       "                      -0.0797, -0.0151, -0.0543,  0.0770, -0.0793,  0.0548,  0.0867,  0.0457,\n",
       "                       0.0275,  0.0972,  0.0620,  0.0691, -0.0140, -0.0909, -0.0986, -0.0948,\n",
       "                       0.0720, -0.0928, -0.1016, -0.0745, -0.0832, -0.0256, -0.0223,  0.0550,\n",
       "                      -0.0203, -0.0188, -0.0186, -0.0934, -0.0716,  0.0451, -0.0072, -0.0185,\n",
       "                      -0.0925,  0.0268, -0.0012,  0.0213, -0.0657, -0.0957,  0.0713,  0.0661])),\n",
       "             ('swinViT.layers3.0.blocks.0.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers3.0.blocks.0.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers3.0.blocks.0.mlp.linear1.weight',\n",
       "              tensor([[ 0.0747, -0.0709,  0.0178,  ...,  0.0394,  0.0352,  0.0085],\n",
       "                      [ 0.0220,  0.0718, -0.0483,  ..., -0.0661,  0.0652,  0.0671],\n",
       "                      [ 0.0578, -0.0150,  0.1001,  ..., -0.0611, -0.0163, -0.0529],\n",
       "                      ...,\n",
       "                      [-0.1000,  0.0063, -0.0698,  ...,  0.0382, -0.0773, -0.0028],\n",
       "                      [ 0.0876,  0.1013, -0.0930,  ..., -0.0414, -0.0664, -0.0615],\n",
       "                      [-0.0247, -0.0922, -0.0166,  ..., -0.0860,  0.0764,  0.0317]])),\n",
       "             ('swinViT.layers3.0.blocks.0.mlp.linear1.bias',\n",
       "              tensor([-4.5881e-02,  1.4839e-02,  7.7392e-02, -9.0265e-02,  6.6961e-02,\n",
       "                      -3.3031e-02,  6.3611e-02, -3.4649e-02, -3.6725e-02,  5.2893e-02,\n",
       "                      -2.4904e-02, -8.6489e-02, -1.7061e-02,  7.5909e-02, -6.8238e-02,\n",
       "                      -2.3326e-02, -7.7528e-02, -6.3970e-02, -5.7162e-02,  4.2866e-02,\n",
       "                      -3.6454e-02,  2.7130e-02,  6.8798e-02,  4.2410e-02,  8.3463e-02,\n",
       "                      -2.4634e-02,  3.3846e-02, -6.3799e-02, -7.0750e-02,  9.2337e-02,\n",
       "                      -3.0612e-02, -1.3719e-02,  9.7840e-02, -9.7202e-02, -9.4394e-02,\n",
       "                       3.7092e-03,  7.9445e-02, -2.8089e-02,  1.4292e-02,  7.2579e-02,\n",
       "                      -4.2473e-02,  7.1769e-02,  4.4387e-02, -4.8424e-02,  5.2180e-02,\n",
       "                       6.1270e-02,  1.2197e-02, -6.0994e-02, -4.6319e-02, -3.1777e-02,\n",
       "                      -8.1103e-02,  3.1034e-02,  6.5657e-02,  3.8764e-02, -4.3598e-02,\n",
       "                       5.2416e-02,  8.7852e-02, -6.7344e-02, -4.9634e-02, -3.1233e-02,\n",
       "                       6.6440e-02,  3.4923e-02,  5.0977e-02, -4.4313e-02,  8.4595e-02,\n",
       "                      -9.3714e-02, -7.0360e-02, -3.3939e-02,  7.5660e-02, -2.6890e-02,\n",
       "                       4.5355e-02, -7.8299e-02,  6.4840e-03, -4.6228e-02, -9.1721e-02,\n",
       "                       3.5089e-02, -6.9732e-02, -5.2003e-02,  7.8345e-02, -1.0810e-02,\n",
       "                      -7.2491e-02, -5.3267e-02, -6.1452e-02, -3.4632e-02, -3.8466e-02,\n",
       "                       1.8750e-02, -3.3649e-02, -3.6117e-03, -3.2040e-03, -2.2940e-02,\n",
       "                       2.4995e-02, -6.1956e-02, -9.8254e-02,  5.8206e-02, -1.0844e-02,\n",
       "                      -3.7240e-02,  5.4719e-02, -7.0042e-02,  8.7228e-02, -5.1804e-02,\n",
       "                       8.0576e-02,  5.4017e-02,  6.0262e-02, -5.8242e-02,  6.3554e-03,\n",
       "                      -6.1221e-02, -5.9550e-03, -6.5631e-02,  2.7459e-02, -2.6684e-02,\n",
       "                       5.8478e-02,  7.9749e-02,  1.3499e-03, -4.9017e-02,  4.3428e-02,\n",
       "                       4.3136e-02, -4.1704e-02,  6.0704e-02,  3.5959e-02,  7.5885e-02,\n",
       "                      -3.2310e-02,  3.8931e-02,  1.0011e-01, -5.4643e-02, -9.7085e-02,\n",
       "                       8.7944e-02,  1.4877e-02,  2.2589e-02, -8.0469e-02, -8.0499e-02,\n",
       "                       1.1530e-02,  1.5987e-03, -2.5205e-02,  2.9624e-02, -4.2079e-02,\n",
       "                       9.4888e-02, -1.5535e-02,  2.3163e-02,  2.3331e-03, -8.2220e-02,\n",
       "                      -9.5061e-02,  6.4432e-02, -1.7219e-02,  8.1092e-02, -8.6342e-02,\n",
       "                      -7.9746e-02, -5.5331e-02, -2.6272e-02, -9.0049e-02,  2.0717e-02,\n",
       "                       4.6740e-03,  6.4482e-02,  4.3833e-02,  3.3824e-03,  1.9982e-02,\n",
       "                      -7.7968e-02, -6.1393e-02, -3.4504e-02, -9.3032e-02,  6.0289e-03,\n",
       "                       3.5198e-02,  6.2954e-02, -4.4664e-02,  6.2299e-02,  2.0564e-02,\n",
       "                       4.2121e-02,  6.8597e-02,  7.7540e-02,  3.0601e-02, -9.7502e-02,\n",
       "                      -7.3104e-02, -3.8754e-02,  2.5270e-02,  9.2428e-02, -8.3914e-02,\n",
       "                      -7.9469e-04, -8.0062e-02,  6.9805e-02, -1.0118e-01,  5.8758e-02,\n",
       "                       2.7581e-04, -7.3063e-02,  2.6766e-02, -1.0065e-01,  4.0342e-02,\n",
       "                       3.5429e-02,  6.5777e-03,  1.5646e-02, -8.6996e-02,  3.8295e-02,\n",
       "                       4.6916e-03,  8.8441e-02, -8.7788e-02, -1.3583e-02, -2.8853e-02,\n",
       "                      -9.2360e-02,  3.4455e-02, -1.2990e-02, -4.4764e-03,  7.0697e-02,\n",
       "                      -9.6811e-02,  6.5786e-02, -2.9465e-02,  1.0816e-02, -8.8501e-02,\n",
       "                      -4.7925e-02, -8.0945e-02, -6.5688e-03,  7.6638e-02,  6.6191e-02,\n",
       "                      -4.1527e-02,  7.2297e-02, -1.0643e-02,  5.5357e-03,  6.3418e-02,\n",
       "                      -1.8060e-02, -9.4223e-02,  8.2383e-02,  1.1917e-02,  2.9377e-02,\n",
       "                      -1.9138e-02, -2.2192e-02, -8.2886e-02,  4.6623e-02,  3.0050e-03,\n",
       "                       4.8920e-02, -3.2052e-02, -3.0783e-02,  7.7417e-02, -8.6470e-02,\n",
       "                       2.4360e-02,  3.6437e-02, -5.9659e-02, -5.1721e-02,  3.2043e-03,\n",
       "                      -5.0685e-03,  1.0087e-02, -2.9089e-02, -1.5824e-02, -5.2891e-02,\n",
       "                      -1.4610e-02,  3.4379e-02,  2.1543e-02, -8.1438e-02, -3.4660e-02,\n",
       "                      -4.0067e-02, -9.5526e-02,  5.1120e-02, -5.0813e-03, -2.6176e-02,\n",
       "                      -8.0341e-02, -6.2648e-02, -8.4679e-02,  1.0148e-01,  1.7041e-02,\n",
       "                       9.9506e-02, -5.1511e-02,  2.9789e-02,  9.8598e-02, -7.9283e-02,\n",
       "                       9.8494e-02,  9.8645e-02, -7.6414e-02,  1.0187e-01, -1.6193e-02,\n",
       "                       6.1644e-02, -4.4554e-02, -2.7233e-02, -5.3926e-02,  7.8247e-02,\n",
       "                       6.1637e-02,  6.2296e-02, -4.4171e-02, -2.5074e-02, -2.1960e-02,\n",
       "                      -5.0851e-03,  4.4363e-02, -6.9334e-02, -7.7361e-02, -1.5125e-03,\n",
       "                       3.0289e-02, -1.0157e-01,  7.7839e-02,  5.7165e-03,  5.5611e-02,\n",
       "                      -2.5716e-02,  3.5764e-02, -1.9694e-02,  9.9696e-02, -6.3123e-02,\n",
       "                       7.8718e-02, -5.8269e-02, -1.0117e-01,  5.2514e-02, -2.5379e-02,\n",
       "                      -5.9155e-02,  5.9233e-02, -5.1457e-02, -4.4518e-02, -8.3045e-02,\n",
       "                       1.8965e-02, -7.6385e-02,  8.8959e-02,  6.1762e-02, -8.1067e-02,\n",
       "                       3.8831e-02, -5.2512e-05,  1.2416e-02, -4.6364e-02,  2.6172e-02,\n",
       "                       9.3318e-02,  7.7874e-02,  6.8235e-02, -9.9054e-02,  1.0057e-01,\n",
       "                      -1.0648e-02,  6.1678e-03,  9.8273e-02, -3.9445e-02,  5.0019e-02,\n",
       "                      -5.0442e-02,  9.5518e-02, -4.3467e-02,  4.0121e-02, -7.4148e-02,\n",
       "                      -2.2210e-02, -8.7269e-02,  6.2520e-02,  5.0649e-03,  1.8939e-03,\n",
       "                       2.3581e-02, -2.3767e-02, -9.4801e-02,  5.3252e-02, -2.4153e-02,\n",
       "                      -9.3270e-02, -4.2629e-02, -7.6238e-02,  7.0928e-02, -1.0176e-01,\n",
       "                      -2.0904e-02, -6.8236e-02, -2.7482e-02, -9.3294e-02, -1.2493e-02,\n",
       "                       9.1538e-02,  6.9694e-02, -9.0790e-02,  3.0156e-02,  5.3451e-02,\n",
       "                      -2.7690e-02, -5.5435e-02,  3.8520e-02,  6.8837e-02, -1.0029e-01,\n",
       "                       6.2232e-03,  1.0092e-01,  4.4089e-02, -5.6223e-02,  2.8847e-02,\n",
       "                       4.8625e-02,  7.6968e-03,  2.6355e-02, -6.1672e-02, -9.5046e-02,\n",
       "                      -2.8509e-02,  6.6994e-02,  8.3003e-03,  8.6654e-02, -9.5957e-02,\n",
       "                       8.0214e-02, -3.5608e-02, -6.6061e-02,  6.3928e-03,  3.0182e-02,\n",
       "                       4.4313e-02, -8.5749e-02,  3.0070e-02,  5.8452e-02, -6.3045e-02,\n",
       "                      -6.4006e-02,  8.4265e-02,  6.4824e-02, -9.3066e-02])),\n",
       "             ('swinViT.layers3.0.blocks.0.mlp.linear2.weight',\n",
       "              tensor([[-0.0002,  0.0398,  0.0288,  ...,  0.0086,  0.0239,  0.0198],\n",
       "                      [-0.0420,  0.0133, -0.0285,  ...,  0.0021,  0.0435,  0.0365],\n",
       "                      [-0.0390,  0.0177, -0.0282,  ..., -0.0419,  0.0093, -0.0463],\n",
       "                      ...,\n",
       "                      [-0.0165, -0.0052, -0.0387,  ...,  0.0310,  0.0469,  0.0375],\n",
       "                      [ 0.0158,  0.0032,  0.0043,  ..., -0.0394, -0.0226, -0.0161],\n",
       "                      [-0.0129,  0.0246, -0.0388,  ..., -0.0036,  0.0196, -0.0410]])),\n",
       "             ('swinViT.layers3.0.blocks.0.mlp.linear2.bias',\n",
       "              tensor([ 0.0030,  0.0441, -0.0073,  0.0387,  0.0463,  0.0414,  0.0008, -0.0189,\n",
       "                       0.0221,  0.0202, -0.0088, -0.0089, -0.0027,  0.0501, -0.0227, -0.0148,\n",
       "                       0.0038,  0.0219, -0.0378,  0.0247,  0.0377,  0.0031, -0.0110, -0.0260,\n",
       "                       0.0228, -0.0390,  0.0332,  0.0352,  0.0161, -0.0320, -0.0137,  0.0492,\n",
       "                      -0.0082,  0.0388, -0.0467, -0.0036,  0.0495, -0.0126, -0.0292,  0.0388,\n",
       "                      -0.0353, -0.0421,  0.0418,  0.0089, -0.0473, -0.0366,  0.0349, -0.0479,\n",
       "                      -0.0506, -0.0339, -0.0091,  0.0152,  0.0212, -0.0338, -0.0011, -0.0206,\n",
       "                       0.0437, -0.0347, -0.0365, -0.0174,  0.0448, -0.0026, -0.0469, -0.0380,\n",
       "                      -0.0107,  0.0086, -0.0160, -0.0393,  0.0167, -0.0028, -0.0030, -0.0380,\n",
       "                       0.0202, -0.0073, -0.0207, -0.0362, -0.0342, -0.0394, -0.0036,  0.0351,\n",
       "                      -0.0121, -0.0037,  0.0427, -0.0414, -0.0347,  0.0355,  0.0274, -0.0003,\n",
       "                      -0.0443,  0.0284, -0.0487, -0.0414, -0.0302, -0.0104,  0.0398,  0.0061])),\n",
       "             ('swinViT.layers3.0.blocks.1.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers3.0.blocks.1.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.relative_position_bias_table',\n",
       "              tensor([[ 0.0066,  0.0230, -0.0148,  ..., -0.0147,  0.0079, -0.0160],\n",
       "                      [-0.0179,  0.0120,  0.0101,  ...,  0.0129,  0.0216,  0.0178],\n",
       "                      [-0.0068,  0.0113, -0.0044,  ..., -0.0192, -0.0214,  0.0376],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0269, -0.0299,  ..., -0.0096, -0.0290,  0.0144],\n",
       "                      [-0.0043,  0.0302,  0.0070,  ..., -0.0296,  0.0125,  0.0297],\n",
       "                      [ 0.0003,  0.0103,  0.0184,  ...,  0.0065, -0.0030, -0.0304]])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.qkv.weight',\n",
       "              tensor([[-0.0154, -0.0841, -0.0962,  ...,  0.0956, -0.0893,  0.0114],\n",
       "                      [ 0.0829,  0.0761, -0.0994,  ..., -0.0423,  0.0588, -0.0758],\n",
       "                      [-0.0814,  0.0516, -0.0635,  ...,  0.0700, -0.0297,  0.0141],\n",
       "                      ...,\n",
       "                      [-0.0590,  0.0841,  0.0768,  ...,  0.0694, -0.0059,  0.0685],\n",
       "                      [-0.0669, -0.0808, -0.0484,  ..., -0.0685, -0.0871,  0.1009],\n",
       "                      [-0.0390,  0.0985,  0.0986,  ..., -0.0276,  0.0005, -0.0620]])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.qkv.bias',\n",
       "              tensor([-0.0118, -0.0947,  0.0220, -0.0448, -0.0145, -0.0545,  0.0985,  0.0647,\n",
       "                       0.0525,  0.0712, -0.0548,  0.0751, -0.0815, -0.0329, -0.0882, -0.0098,\n",
       "                      -0.0577, -0.0820,  0.0386, -0.0336,  0.0875, -0.0116, -0.0876,  0.0767,\n",
       "                       0.0040,  0.0903,  0.0759, -0.0731, -0.0211, -0.0038,  0.0813,  0.0348,\n",
       "                      -0.0311, -0.0709, -0.0061,  0.0084,  0.0606,  0.0820, -0.0753,  0.0611,\n",
       "                      -0.0490,  0.0754,  0.0362,  0.0344,  0.0668,  0.0950,  0.0429, -0.0454,\n",
       "                      -0.0751,  0.0674, -0.0330,  0.0741,  0.0874,  0.0999, -0.0356,  0.0521,\n",
       "                       0.0890,  0.0162, -0.0290,  0.0419, -0.0494, -0.0047, -0.0036,  0.0309,\n",
       "                       0.0718, -0.0908,  0.0642,  0.0115,  0.0772, -0.0631, -0.0956,  0.0245,\n",
       "                       0.0081,  0.0292, -0.0828, -0.0556, -0.0942,  0.0334, -0.0274,  0.0665,\n",
       "                       0.0818, -0.0029,  0.0746,  0.0348, -0.0126,  0.0315, -0.0810, -0.0844,\n",
       "                      -0.0118, -0.1010, -0.0884, -0.0975,  0.0320,  0.0489,  0.0299, -0.0284,\n",
       "                       0.0051,  0.0787, -0.0891, -0.0999, -0.0846,  0.0235, -0.0888, -0.0732,\n",
       "                       0.0903,  0.0868,  0.0457, -0.0897, -0.0682, -0.0726,  0.0703, -0.0473,\n",
       "                      -0.0416, -0.0876, -0.0836, -0.0555, -0.0173,  0.0064,  0.0575,  0.0859,\n",
       "                      -0.0586, -0.0630, -0.0103,  0.0503,  0.0106, -0.0621, -0.1017,  0.0437,\n",
       "                       0.0278,  0.0669,  0.0334,  0.0636,  0.0523,  0.0593, -0.0743,  0.0325,\n",
       "                      -0.0088, -0.0796, -0.0478,  0.0052, -0.0937,  0.0761, -0.0690,  0.0634,\n",
       "                       0.0418, -0.0973,  0.0073, -0.0105, -0.0947, -0.0177, -0.0897,  0.0386,\n",
       "                       0.0009,  0.0670, -0.0527,  0.0117, -0.0005, -0.0135,  0.0355, -0.0915,\n",
       "                      -0.0411, -0.0775, -0.0825,  0.0086, -0.0270,  0.0003, -0.0369, -0.0498,\n",
       "                       0.0529, -0.0020,  0.0337, -0.0405, -0.0929,  0.0285, -0.1013, -0.0625,\n",
       "                      -0.0436,  0.0079,  0.0907, -0.0863,  0.0508,  0.0321,  0.0222, -0.0999,\n",
       "                      -0.0310,  0.0230,  0.0220, -0.0010, -0.0900, -0.0638,  0.0908, -0.0836,\n",
       "                       0.0842, -0.0380, -0.0229,  0.0022, -0.0417,  0.0127,  0.0222,  0.0519,\n",
       "                       0.0424,  0.0178, -0.0816, -0.0978, -0.0698, -0.0206, -0.0454, -0.0010,\n",
       "                       0.0562, -0.0697,  0.0298, -0.0639, -0.0649,  0.0814,  0.0334, -0.0677,\n",
       "                      -0.0763, -0.0580,  0.0827, -0.0227,  0.0662, -0.0956,  0.0654, -0.0468,\n",
       "                       0.0041, -0.0519,  0.0477, -0.0582,  0.0402, -0.0879,  0.0040,  0.0815,\n",
       "                      -0.0159,  0.0658, -0.0013,  0.0506,  0.0216,  0.0645, -0.0404,  0.0373,\n",
       "                       0.0743,  0.1014, -0.0234,  0.0413, -0.0460, -0.0360, -0.0988, -0.0182,\n",
       "                       0.0644, -0.0297,  0.0922, -0.0612,  0.0214, -0.1016, -0.0734,  0.0343,\n",
       "                      -0.0214,  0.0865,  0.0595, -0.0508,  0.0080,  0.0627, -0.0939, -0.0754,\n",
       "                       0.0065,  0.0772,  0.0293, -0.0962,  0.0636, -0.0742,  0.0258, -0.0010,\n",
       "                       0.0818, -0.0124,  0.0377,  0.0824,  0.0419, -0.0798,  0.0983,  0.0121,\n",
       "                       0.0026, -0.0422,  0.0297,  0.0071,  0.1018,  0.0380,  0.0966,  0.0742])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.proj.weight',\n",
       "              tensor([[ 0.0300,  0.0105, -0.0183,  ..., -0.0645,  0.0217, -0.0617],\n",
       "                      [-0.0137, -0.1006, -0.0151,  ..., -0.0111,  0.0084,  0.0524],\n",
       "                      [ 0.0127,  0.0192,  0.0178,  ...,  0.0469,  0.0142,  0.0162],\n",
       "                      ...,\n",
       "                      [-0.0957, -0.0257,  0.0059,  ..., -0.0003,  0.0951,  0.0429],\n",
       "                      [-0.0602,  0.0685, -0.0768,  ...,  0.0937, -0.0179, -0.0751],\n",
       "                      [ 0.0361,  0.0107, -0.0019,  ...,  0.0746,  0.0050, -0.0044]])),\n",
       "             ('swinViT.layers3.0.blocks.1.attn.proj.bias',\n",
       "              tensor([-0.0073,  0.0221, -0.0712, -0.0288,  0.0295, -0.0573, -0.1015, -0.0023,\n",
       "                      -0.0680, -0.0167,  0.0525, -0.0492, -0.0494,  0.0450,  0.0792,  0.0549,\n",
       "                       0.0368,  0.0519,  0.0477,  0.0405,  0.0587,  0.0943,  0.0458,  0.0606,\n",
       "                      -0.0211, -0.0042,  0.0254,  0.0773, -0.0892, -0.0962, -0.0004, -0.0681,\n",
       "                       0.1016,  0.0881,  0.0192, -0.0678, -0.0556,  0.0552,  0.0293,  0.0203,\n",
       "                       0.0440,  0.0913,  0.0955, -0.0920,  0.0729,  0.0672, -0.0134, -0.0289,\n",
       "                      -0.0371,  0.0079, -0.0735, -0.0633,  0.0414, -0.0316, -0.0333,  0.0868,\n",
       "                      -0.0232, -0.0006,  0.0834, -0.0419, -0.0048, -0.0028, -0.0844,  0.0229,\n",
       "                      -0.0267, -0.0022, -0.0223, -0.0151,  0.0508, -0.0424,  0.0009,  0.0605,\n",
       "                      -0.0654, -0.0597,  0.0023,  0.0812,  0.0358,  0.0558,  0.0496,  0.0152,\n",
       "                      -0.0773, -0.0171,  0.0935, -0.0550,  0.0182, -0.0256,  0.0298,  0.0703,\n",
       "                       0.0814,  0.0607,  0.0160,  0.0831,  0.0565, -0.0988, -0.0523, -0.0651])),\n",
       "             ('swinViT.layers3.0.blocks.1.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers3.0.blocks.1.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers3.0.blocks.1.mlp.linear1.weight',\n",
       "              tensor([[ 0.0425,  0.0676,  0.0605,  ..., -0.0236,  0.0836, -0.0042],\n",
       "                      [-0.0700,  0.0009, -0.0759,  ...,  0.0380, -0.0362, -0.0179],\n",
       "                      [ 0.0756, -0.0569,  0.0998,  ...,  0.0858,  0.0718, -0.0507],\n",
       "                      ...,\n",
       "                      [-0.0271, -0.0640,  0.0985,  ..., -0.0419,  0.0101, -0.0373],\n",
       "                      [ 0.0278,  0.0028, -0.0244,  ...,  0.0280, -0.0402,  0.0865],\n",
       "                      [ 0.1010, -0.0065, -0.0904,  ...,  0.0179, -0.0033, -0.0061]])),\n",
       "             ('swinViT.layers3.0.blocks.1.mlp.linear1.bias',\n",
       "              tensor([ 0.0474,  0.0302,  0.0706, -0.0344,  0.0478, -0.0063, -0.0240,  0.0153,\n",
       "                       0.0514, -0.0111, -0.0857,  0.0068,  0.1003,  0.0262,  0.0544, -0.0132,\n",
       "                      -0.0711,  0.0489, -0.0557, -0.0643, -0.0679,  0.0073,  0.0482,  0.0292,\n",
       "                       0.0941, -0.0179, -0.0127,  0.0988,  0.0525,  0.0181,  0.0253,  0.0211,\n",
       "                      -0.0856, -0.0106,  0.0768, -0.0295, -0.0259, -0.0312, -0.0853,  0.0281,\n",
       "                       0.0810, -0.0056,  0.0619, -0.0345,  0.0220, -0.0398,  0.0417,  0.0658,\n",
       "                       0.0555,  0.0199,  0.0621,  0.0783,  0.0235, -0.0610, -0.0534, -0.0725,\n",
       "                       0.0621, -0.0350,  0.0266,  0.0670, -0.0332,  0.0274,  0.0271, -0.0002,\n",
       "                       0.0138,  0.0279,  0.0354, -0.0624,  0.0067, -0.1008,  0.0400, -0.0443,\n",
       "                      -0.0908, -0.0901, -0.0901, -0.0209,  0.0096, -0.0188, -0.0199, -0.0576,\n",
       "                       0.0540,  0.0078,  0.0957, -0.0885,  0.0501, -0.0091,  0.0453, -0.0251,\n",
       "                      -0.0603,  0.0693,  0.0553, -0.0469, -0.0809,  0.0497,  0.0918,  0.0417,\n",
       "                       0.0688,  0.0441, -0.0971,  0.0518, -0.0749, -0.0746,  0.0915,  0.0861,\n",
       "                       0.0486,  0.0114,  0.0029,  0.1013, -0.0880, -0.0543,  0.0130,  0.0242,\n",
       "                      -0.0798,  0.0881, -0.1016, -0.0182,  0.0059,  0.0834, -0.0615, -0.0979,\n",
       "                       0.0521, -0.0196, -0.0099, -0.0599,  0.0996, -0.0099,  0.0836,  0.0675,\n",
       "                       0.0250,  0.0653, -0.0987,  0.0346,  0.0467,  0.0807, -0.0863,  0.0921,\n",
       "                      -0.0853,  0.0019,  0.0205, -0.0102,  0.0754, -0.0580,  0.0634, -0.0597,\n",
       "                       0.0069, -0.0146,  0.0986, -0.0525, -0.0550, -0.0985,  0.0274, -0.0325,\n",
       "                       0.0273, -0.0346, -0.0036, -0.0007,  0.0349,  0.0619,  0.0353,  0.0017,\n",
       "                      -0.0980,  0.0278,  0.0257, -0.0938, -0.0678, -0.0936,  0.0411, -0.0838,\n",
       "                      -0.0053, -0.0811, -0.0087,  0.0094,  0.0817,  0.1015,  0.0935, -0.0117,\n",
       "                      -0.0197, -0.0606, -0.0441, -0.0126, -0.0724, -0.0092,  0.0754,  0.0653,\n",
       "                      -0.0641, -0.0265,  0.0930,  0.0914,  0.0372,  0.0710,  0.0714, -0.0445,\n",
       "                      -0.0292, -0.0174, -0.0976,  0.0833,  0.0867, -0.0565, -0.0203,  0.0402,\n",
       "                       0.0003, -0.0476, -0.0361,  0.0691,  0.0533,  0.0514, -0.1015, -0.0418,\n",
       "                       0.0110,  0.0021, -0.0437, -0.1012, -0.0219,  0.0853,  0.0693, -0.0315,\n",
       "                      -0.0723,  0.0588, -0.0479,  0.0934,  0.0346,  0.0530, -0.0468,  0.0042,\n",
       "                       0.0147,  0.0689,  0.0110, -0.0081, -0.0591,  0.0872,  0.0022, -0.0103,\n",
       "                      -0.1013,  0.0944, -0.0351,  0.0249, -0.0265,  0.0175,  0.0256, -0.0097,\n",
       "                       0.0309, -0.1018, -0.0123,  0.0983, -0.0139, -0.0847,  0.0773, -0.0189,\n",
       "                      -0.0394, -0.0124, -0.0915, -0.0971,  0.0858, -0.0417,  0.0815,  0.0930,\n",
       "                      -0.0728, -0.0483,  0.0905,  0.0189,  0.0193,  0.0599, -0.0548,  0.0315,\n",
       "                      -0.0360,  0.0653,  0.0938,  0.0112, -0.0656,  0.0210, -0.0306,  0.0473,\n",
       "                      -0.0979,  0.0971,  0.0432,  0.0983, -0.0476,  0.0515, -0.0772, -0.0148,\n",
       "                      -0.0042,  0.0044,  0.0449,  0.0018,  0.0222, -0.0472, -0.0023,  0.0364,\n",
       "                       0.0278,  0.0129,  0.0688,  0.0432,  0.0847,  0.0713,  0.0351,  0.0362,\n",
       "                       0.0142, -0.0063, -0.0526,  0.0644,  0.0255, -0.0227,  0.0695, -0.0218,\n",
       "                      -0.0818,  0.0527, -0.0480,  0.0903,  0.0450,  0.0850,  0.0602,  0.0261,\n",
       "                      -0.0812,  0.0444,  0.0984, -0.0256, -0.0241,  0.0764, -0.0953,  0.0421,\n",
       "                      -0.0905, -0.0399, -0.0937,  0.0766, -0.1010,  0.0604, -0.0547, -0.0540,\n",
       "                       0.0861,  0.0379,  0.0465,  0.0486,  0.0604,  0.0731,  0.0894, -0.0254,\n",
       "                      -0.0819, -0.0388,  0.0395, -0.0776, -0.0253, -0.0473,  0.0705,  0.0003,\n",
       "                      -0.0284,  0.0583,  0.0271, -0.0304, -0.0206,  0.0224, -0.0925,  0.0935,\n",
       "                      -0.0030, -0.0600,  0.0561,  0.0326,  0.0123, -0.0778,  0.0087,  0.0831,\n",
       "                       0.0588, -0.0675, -0.0608, -0.0456,  0.0122, -0.0978,  0.0464,  0.0890,\n",
       "                      -0.0802,  0.0895, -0.0201,  0.0355, -0.0919, -0.0552,  0.0845,  0.0025,\n",
       "                       0.0241, -0.0313, -0.0827, -0.0384,  0.0405,  0.0691,  0.0446,  0.0297])),\n",
       "             ('swinViT.layers3.0.blocks.1.mlp.linear2.weight',\n",
       "              tensor([[ 0.0264,  0.0157,  0.0305,  ..., -0.0395, -0.0291,  0.0297],\n",
       "                      [ 0.0398, -0.0166, -0.0361,  ..., -0.0241,  0.0141, -0.0018],\n",
       "                      [ 0.0382,  0.0288, -0.0366,  ..., -0.0201,  0.0232, -0.0102],\n",
       "                      ...,\n",
       "                      [-0.0129, -0.0302, -0.0316,  ..., -0.0184,  0.0182, -0.0364],\n",
       "                      [ 0.0261,  0.0506, -0.0156,  ...,  0.0246,  0.0372, -0.0510],\n",
       "                      [ 0.0199,  0.0421,  0.0159,  ...,  0.0106,  0.0492,  0.0144]])),\n",
       "             ('swinViT.layers3.0.blocks.1.mlp.linear2.bias',\n",
       "              tensor([ 2.3733e-02,  4.4651e-02, -3.8155e-02,  1.6256e-02, -7.6906e-03,\n",
       "                      -4.9467e-02,  6.7364e-03, -3.3519e-02,  3.3997e-02, -4.5467e-02,\n",
       "                      -2.1431e-02, -2.3033e-02, -2.8959e-02, -1.6155e-02,  4.1316e-02,\n",
       "                      -1.6917e-02, -4.0071e-02, -2.6253e-02, -1.3496e-02,  2.3060e-02,\n",
       "                       2.3602e-02, -4.9367e-02, -2.5946e-02, -2.2671e-02,  1.6861e-02,\n",
       "                      -3.6239e-02,  9.3178e-03, -4.4389e-02, -1.4636e-02, -3.6075e-02,\n",
       "                       3.9634e-02, -2.9689e-02, -4.3848e-02,  1.6151e-03,  8.9046e-03,\n",
       "                      -3.9036e-02,  3.0862e-02, -8.8115e-03,  1.0001e-02, -1.3378e-02,\n",
       "                       1.4660e-02, -4.3371e-02, -8.3076e-03, -4.1812e-02,  4.1931e-03,\n",
       "                      -4.9509e-02, -2.0064e-02,  1.4393e-02,  3.5221e-02, -2.8860e-03,\n",
       "                       3.6007e-02, -1.5246e-02,  2.4668e-02, -2.4148e-02,  2.2320e-02,\n",
       "                       4.0789e-02,  1.0396e-02,  1.5877e-02, -2.2678e-02,  7.0586e-03,\n",
       "                      -2.6892e-03, -1.0673e-03, -2.4202e-02,  3.9393e-02, -2.3201e-02,\n",
       "                       9.1605e-05, -1.2047e-02, -5.0358e-02,  1.6232e-02,  1.7197e-02,\n",
       "                      -4.7424e-02,  2.0855e-02, -2.6612e-02,  1.3998e-02,  1.2751e-02,\n",
       "                      -1.7241e-02, -4.9873e-02, -2.7951e-02, -4.5444e-02,  3.5384e-02,\n",
       "                      -1.1502e-02,  2.7646e-02, -4.3559e-02, -7.4685e-03,  2.0066e-02,\n",
       "                       2.9423e-02, -4.9145e-02,  4.7239e-03,  3.5102e-03, -4.2617e-02,\n",
       "                       4.6668e-02, -2.9895e-02,  4.1459e-02, -1.3441e-02,  2.0431e-02,\n",
       "                      -3.7710e-02])),\n",
       "             ('swinViT.layers3.0.downsample.reduction.weight',\n",
       "              tensor([[ 0.0301, -0.0175,  0.0108,  ..., -0.0206, -0.0295,  0.0217],\n",
       "                      [-0.0338,  0.0040,  0.0221,  ..., -0.0209,  0.0181, -0.0019],\n",
       "                      [-0.0350, -0.0100, -0.0293,  ...,  0.0067, -0.0185, -0.0325],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0253, -0.0065,  ..., -0.0036, -0.0336,  0.0168],\n",
       "                      [ 0.0124,  0.0213, -0.0229,  ..., -0.0084,  0.0305, -0.0186],\n",
       "                      [ 0.0291,  0.0325,  0.0015,  ..., -0.0066,  0.0092, -0.0334]])),\n",
       "             ('swinViT.layers3.0.downsample.norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers3.0.downsample.norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers4.0.blocks.0.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers4.0.blocks.0.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.relative_position_bias_table',\n",
       "              tensor([[-0.0396, -0.0077, -0.0055,  ..., -0.0367,  0.0198,  0.0256],\n",
       "                      [ 0.0210,  0.0065,  0.0171,  ...,  0.0219,  0.0277, -0.0042],\n",
       "                      [-0.0174,  0.0231,  0.0081,  ...,  0.0038,  0.0194,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0339, -0.0028,  0.0170,  ..., -0.0233,  0.0121,  0.0099],\n",
       "                      [ 0.0093, -0.0264,  0.0168,  ...,  0.0554, -0.0337, -0.0010],\n",
       "                      [-0.0112, -0.0031, -0.0016,  ..., -0.0384,  0.0153, -0.0072]])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.qkv.weight',\n",
       "              tensor([[-0.0362,  0.0620, -0.0684,  ..., -0.0367,  0.0511,  0.0440],\n",
       "                      [-0.0041, -0.0514,  0.0041,  ...,  0.0183, -0.0176,  0.0221],\n",
       "                      [ 0.0028, -0.0075, -0.0063,  ...,  0.0078, -0.0086, -0.0101],\n",
       "                      ...,\n",
       "                      [-0.0331, -0.0702, -0.0051,  ..., -0.0228, -0.0014,  0.0241],\n",
       "                      [ 0.0140, -0.0166, -0.0421,  ..., -0.0227, -0.0218,  0.0474],\n",
       "                      [ 0.0706, -0.0014, -0.0142,  ..., -0.0705,  0.0650, -0.0636]])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.qkv.bias',\n",
       "              tensor([-0.0104,  0.0335,  0.0437,  0.0634, -0.0135, -0.0407, -0.0613, -0.0706,\n",
       "                      -0.0003,  0.0421, -0.0351,  0.0265, -0.0684,  0.0720,  0.0159,  0.0606,\n",
       "                       0.0137,  0.0151, -0.0469, -0.0083,  0.0023,  0.0387,  0.0255,  0.0346,\n",
       "                       0.0277, -0.0256, -0.0007,  0.0541,  0.0579, -0.0216, -0.0608,  0.0151,\n",
       "                      -0.0082,  0.0647,  0.0615, -0.0352, -0.0145,  0.0247,  0.0543, -0.0520,\n",
       "                      -0.0208, -0.0031,  0.0127,  0.0352,  0.0458,  0.0532,  0.0552, -0.0270,\n",
       "                      -0.0437, -0.0621,  0.0591,  0.0260, -0.0044,  0.0420,  0.0308,  0.0109,\n",
       "                       0.0154, -0.0377, -0.0583,  0.0466, -0.0573, -0.0717,  0.0480,  0.0173,\n",
       "                       0.0345,  0.0117, -0.0408, -0.0056,  0.0244, -0.0334, -0.0644,  0.0194,\n",
       "                       0.0439, -0.0510,  0.0403,  0.0689,  0.0127, -0.0070,  0.0126, -0.0706,\n",
       "                       0.0298, -0.0664,  0.0220,  0.0511,  0.0008, -0.0167,  0.0079, -0.0425,\n",
       "                      -0.0133,  0.0563,  0.0624,  0.0448,  0.0501,  0.0177,  0.0363, -0.0245,\n",
       "                       0.0277,  0.0414, -0.0609,  0.0292, -0.0621, -0.0330,  0.0025, -0.0048,\n",
       "                       0.0136, -0.0072, -0.0134, -0.0355, -0.0561,  0.0588,  0.0588, -0.0608,\n",
       "                      -0.0432, -0.0154, -0.0063, -0.0518,  0.0258,  0.0022, -0.0319,  0.0287,\n",
       "                      -0.0662,  0.0396, -0.0184,  0.0456, -0.0255, -0.0130, -0.0091,  0.0349,\n",
       "                      -0.0664,  0.0635, -0.0666,  0.0530,  0.0316, -0.0434,  0.0590, -0.0266,\n",
       "                       0.0264,  0.0698,  0.0356,  0.0093, -0.0631,  0.0490, -0.0601,  0.0549,\n",
       "                       0.0259, -0.0281,  0.0675,  0.0001, -0.0219, -0.0427, -0.0140,  0.0098,\n",
       "                      -0.0212,  0.0698, -0.0054,  0.0314,  0.0711,  0.0098, -0.0390,  0.0341,\n",
       "                      -0.0250,  0.0208, -0.0361,  0.0208,  0.0263,  0.0720,  0.0687,  0.0325,\n",
       "                       0.0323,  0.0574,  0.0539, -0.0178, -0.0274,  0.0479,  0.0551,  0.0375,\n",
       "                      -0.0673,  0.0312,  0.0246,  0.0152,  0.0051, -0.0264,  0.0150,  0.0366,\n",
       "                      -0.0085, -0.0707,  0.0390, -0.0193,  0.0674,  0.0506, -0.0487, -0.0250,\n",
       "                       0.0552,  0.0344,  0.0373,  0.0144,  0.0421,  0.0217,  0.0351, -0.0648,\n",
       "                      -0.0619, -0.0720, -0.0191, -0.0144,  0.0008, -0.0196,  0.0561, -0.0115,\n",
       "                      -0.0209, -0.0241,  0.0222, -0.0302,  0.0058, -0.0558,  0.0529,  0.0043,\n",
       "                      -0.0199, -0.0277,  0.0562, -0.0426, -0.0188,  0.0286, -0.0057,  0.0375,\n",
       "                       0.0152, -0.0128,  0.0331, -0.0514,  0.0285,  0.0583,  0.0258, -0.0466,\n",
       "                      -0.0252,  0.0588,  0.0578,  0.0471,  0.0003, -0.0660, -0.0508,  0.0716,\n",
       "                       0.0526,  0.0199, -0.0483,  0.0335, -0.0080, -0.0073,  0.0386,  0.0663,\n",
       "                       0.0468, -0.0495,  0.0024, -0.0147, -0.0243,  0.0235,  0.0207, -0.0434,\n",
       "                       0.0541, -0.0213, -0.0601, -0.0429,  0.0633, -0.0506, -0.0705,  0.0625,\n",
       "                       0.0054, -0.0717,  0.0620, -0.0135, -0.0476,  0.0533, -0.0123, -0.0238,\n",
       "                      -0.0643,  0.0720, -0.0699,  0.0064, -0.0116, -0.0704, -0.0083, -0.0456,\n",
       "                       0.0523,  0.0023,  0.0077,  0.0719, -0.0703,  0.0547, -0.0680, -0.0249,\n",
       "                      -0.0431, -0.0404,  0.0391,  0.0688, -0.0699,  0.0433, -0.0366,  0.0158,\n",
       "                       0.0216, -0.0042, -0.0206, -0.0613,  0.0716,  0.0152,  0.0217, -0.0026,\n",
       "                      -0.0412, -0.0658, -0.0719, -0.0716,  0.0669,  0.0515,  0.0382, -0.0303,\n",
       "                       0.0177, -0.0399,  0.0389, -0.0443, -0.0286, -0.0451, -0.0362,  0.0003,\n",
       "                      -0.0028,  0.0647, -0.0329, -0.0411, -0.0476,  0.0016,  0.0028, -0.0698,\n",
       "                      -0.0262,  0.0604,  0.0657, -0.0071,  0.0703,  0.0141,  0.0485,  0.0436,\n",
       "                       0.0700, -0.0134,  0.0438,  0.0329, -0.0183,  0.0403,  0.0214, -0.0025,\n",
       "                      -0.0282,  0.0080,  0.0187,  0.0294,  0.0462, -0.0412,  0.0669, -0.0538,\n",
       "                       0.0199, -0.0429,  0.0303, -0.0713,  0.0623, -0.0327, -0.0278,  0.0115,\n",
       "                      -0.0267,  0.0173, -0.0284, -0.0061,  0.0695,  0.0250,  0.0506,  0.0682,\n",
       "                       0.0306,  0.0413,  0.0446,  0.0392, -0.0040, -0.0312, -0.0224,  0.0084,\n",
       "                       0.0322, -0.0044, -0.0657, -0.0024,  0.0703,  0.0570,  0.0463, -0.0566,\n",
       "                      -0.0557,  0.0454, -0.0353, -0.0531,  0.0356,  0.0406,  0.0167, -0.0571,\n",
       "                      -0.0280, -0.0656, -0.0666,  0.0659,  0.0157, -0.0034,  0.0624,  0.0260,\n",
       "                      -0.0222, -0.0032,  0.0250,  0.0104,  0.0114,  0.0210, -0.0296, -0.0054,\n",
       "                      -0.0256, -0.0221, -0.0206,  0.0170, -0.0656,  0.0620, -0.0174, -0.0672,\n",
       "                      -0.0474, -0.0661,  0.0052, -0.0496, -0.0602,  0.0105,  0.0208, -0.0085,\n",
       "                       0.0475,  0.0090, -0.0604,  0.0544,  0.0630,  0.0355, -0.0282, -0.0702,\n",
       "                      -0.0238, -0.0127,  0.0082, -0.0383, -0.0404,  0.0209,  0.0252,  0.0124,\n",
       "                       0.0140,  0.0435,  0.0529,  0.0090,  0.0228, -0.0228, -0.0606, -0.0022,\n",
       "                      -0.0363, -0.0674,  0.0209, -0.0090, -0.0172,  0.0443, -0.0200,  0.0204,\n",
       "                      -0.0044, -0.0361,  0.0317, -0.0056,  0.0495,  0.0667,  0.0165, -0.0356,\n",
       "                      -0.0041,  0.0195, -0.0597, -0.0590, -0.0102,  0.0300, -0.0424,  0.0614,\n",
       "                      -0.0413,  0.0251, -0.0209, -0.0339, -0.0622,  0.0236,  0.0019,  0.0266,\n",
       "                       0.0551,  0.0712, -0.0203, -0.0486, -0.0424,  0.0348,  0.0176, -0.0288,\n",
       "                      -0.0491, -0.0313,  0.0384,  0.0477,  0.0400,  0.0156, -0.0259,  0.0649,\n",
       "                      -0.0124,  0.0498, -0.0095, -0.0618,  0.0157,  0.0308, -0.0245,  0.0250,\n",
       "                      -0.0274, -0.0074, -0.0111,  0.0037,  0.0697,  0.0166, -0.0556,  0.0351,\n",
       "                       0.0576, -0.0707, -0.0628,  0.0343,  0.0690, -0.0451, -0.0026,  0.0322,\n",
       "                       0.0711, -0.0506,  0.0419,  0.0585, -0.0542,  0.0154,  0.0482, -0.0039,\n",
       "                      -0.0132,  0.0045, -0.0417,  0.0167, -0.0209,  0.0313,  0.0016, -0.0163,\n",
       "                      -0.0427, -0.0459,  0.0700, -0.0092, -0.0419, -0.0471,  0.0184, -0.0235,\n",
       "                      -0.0356, -0.0485, -0.0136, -0.0007, -0.0013,  0.0591,  0.0449, -0.0421,\n",
       "                      -0.0158,  0.0421, -0.0324,  0.0304, -0.0023,  0.0380,  0.0474,  0.0293,\n",
       "                      -0.0002, -0.0118,  0.0332, -0.0263, -0.0009, -0.0562,  0.0625, -0.0604,\n",
       "                       0.0078, -0.0454, -0.0326,  0.0608,  0.0521, -0.0442, -0.0384,  0.0464])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.proj.weight',\n",
       "              tensor([[-0.0053, -0.0063, -0.0052,  ...,  0.0089,  0.0384, -0.0339],\n",
       "                      [ 0.0663,  0.0442,  0.0405,  ..., -0.0540, -0.0511,  0.0403],\n",
       "                      [-0.0275, -0.0506, -0.0633,  ..., -0.0554,  0.0660, -0.0566],\n",
       "                      ...,\n",
       "                      [ 0.0180, -0.0199, -0.0274,  ..., -0.0569,  0.0472, -0.0323],\n",
       "                      [-0.0357,  0.0685, -0.0606,  ..., -0.0717, -0.0467,  0.0346],\n",
       "                      [-0.0085, -0.0193,  0.0225,  ..., -0.0344,  0.0234,  0.0408]])),\n",
       "             ('swinViT.layers4.0.blocks.0.attn.proj.bias',\n",
       "              tensor([ 0.0313,  0.0567,  0.0387,  0.0443,  0.0334,  0.0224,  0.0500, -0.0692,\n",
       "                       0.0668,  0.0366, -0.0144,  0.0374, -0.0669, -0.0365, -0.0304,  0.0657,\n",
       "                      -0.0291,  0.0162, -0.0596,  0.0168,  0.0191,  0.0122,  0.0376,  0.0298,\n",
       "                      -0.0285,  0.0268,  0.0513,  0.0541,  0.0305,  0.0652,  0.0046,  0.0468,\n",
       "                      -0.0471, -0.0025, -0.0507,  0.0648, -0.0568,  0.0657, -0.0720, -0.0290,\n",
       "                      -0.0443,  0.0612, -0.0064, -0.0382,  0.0124, -0.0371,  0.0516,  0.0030,\n",
       "                       0.0156, -0.0021,  0.0557,  0.0229,  0.0640, -0.0405,  0.0630,  0.0377,\n",
       "                      -0.0518, -0.0599,  0.0509, -0.0173, -0.0205,  0.0501,  0.0288, -0.0411,\n",
       "                      -0.0306, -0.0259, -0.0714, -0.0228,  0.0407, -0.0274,  0.0047,  0.0562,\n",
       "                       0.0450,  0.0571,  0.0495,  0.0610,  0.0105,  0.0531,  0.0428,  0.0465,\n",
       "                      -0.0020, -0.0178,  0.0043, -0.0392, -0.0438, -0.0327, -0.0305,  0.0352,\n",
       "                       0.0274,  0.0613, -0.0521,  0.0220,  0.0141, -0.0452, -0.0471,  0.0663,\n",
       "                       0.0302, -0.0303,  0.0374,  0.0120,  0.0483,  0.0008,  0.0473,  0.0429,\n",
       "                       0.0637,  0.0715, -0.0318,  0.0356,  0.0457, -0.0511, -0.0692,  0.0238,\n",
       "                      -0.0679, -0.0136, -0.0054,  0.0605, -0.0417, -0.0708, -0.0291,  0.0381,\n",
       "                       0.0294, -0.0275, -0.0214,  0.0200,  0.0680, -0.0113, -0.0024, -0.0159,\n",
       "                      -0.0153, -0.0502,  0.0471, -0.0328, -0.0678,  0.0459,  0.0661,  0.0213,\n",
       "                      -0.0506,  0.0178,  0.0111,  0.0285,  0.0076,  0.0236,  0.0612, -0.0360,\n",
       "                      -0.0370, -0.0368,  0.0376,  0.0544,  0.0480, -0.0018,  0.0163,  0.0404,\n",
       "                      -0.0331,  0.0149,  0.0385, -0.0209, -0.0105, -0.0107,  0.0535, -0.0528,\n",
       "                       0.0069,  0.0114, -0.0070, -0.0236, -0.0208,  0.0630,  0.0422, -0.0628,\n",
       "                       0.0340,  0.0666,  0.0353,  0.0668, -0.0064, -0.0104,  0.0504,  0.0210,\n",
       "                      -0.0162,  0.0559,  0.0239, -0.0129, -0.0352,  0.0403,  0.0616, -0.0121,\n",
       "                       0.0077, -0.0648,  0.0422, -0.0010,  0.0290, -0.0402, -0.0058, -0.0041])),\n",
       "             ('swinViT.layers4.0.blocks.0.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers4.0.blocks.0.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers4.0.blocks.0.mlp.linear1.weight',\n",
       "              tensor([[ 0.0563, -0.0214,  0.0033,  ..., -0.0169, -0.0444,  0.0518],\n",
       "                      [ 0.0601, -0.0068, -0.0500,  ..., -0.0135, -0.0595,  0.0211],\n",
       "                      [ 0.0489, -0.0593,  0.0350,  ..., -0.0680,  0.0149,  0.0421],\n",
       "                      ...,\n",
       "                      [-0.0163,  0.0293,  0.0035,  ..., -0.0004, -0.0219,  0.0013],\n",
       "                      [ 0.0565, -0.0695, -0.0013,  ...,  0.0242,  0.0408,  0.0712],\n",
       "                      [-0.0114, -0.0221, -0.0445,  ...,  0.0141,  0.0161,  0.0257]])),\n",
       "             ('swinViT.layers4.0.blocks.0.mlp.linear1.bias',\n",
       "              tensor([-6.8362e-02,  2.8268e-03, -3.0858e-02,  2.5490e-02,  5.9604e-02,\n",
       "                      -9.4123e-03, -2.7631e-02,  2.1650e-02, -3.5672e-02, -4.5422e-02,\n",
       "                       4.2973e-02, -2.3509e-02,  6.3911e-02, -3.2098e-03,  5.9921e-02,\n",
       "                       6.4981e-02,  6.2756e-02,  4.2802e-02,  3.8033e-02,  6.9055e-02,\n",
       "                      -4.5887e-02, -6.1083e-02, -5.0557e-03, -5.5990e-02, -1.4923e-03,\n",
       "                       1.4860e-02,  5.6053e-02,  3.7581e-02,  7.1054e-02, -1.3782e-03,\n",
       "                      -1.5603e-02, -3.0778e-02,  2.7987e-02,  2.2685e-02, -1.4841e-02,\n",
       "                      -6.4934e-02, -1.3957e-02, -4.0816e-02, -2.9733e-02, -2.7115e-02,\n",
       "                      -1.7760e-02, -1.7612e-02,  3.0680e-02, -1.1734e-02,  3.7644e-02,\n",
       "                      -3.2626e-02,  4.7400e-02,  2.8165e-02,  1.4102e-02, -2.0717e-02,\n",
       "                      -1.7150e-02,  3.9562e-02, -2.0124e-02, -4.8036e-03,  4.9623e-02,\n",
       "                      -7.0041e-02,  9.2819e-03,  3.9246e-02, -2.8868e-02,  6.9655e-02,\n",
       "                       5.2403e-02,  6.2025e-02, -4.6103e-03, -2.2345e-02,  5.8589e-02,\n",
       "                      -2.3766e-02,  4.2964e-02, -5.9228e-03, -5.7278e-02, -7.2031e-02,\n",
       "                       3.5084e-02,  7.0862e-02,  6.6835e-02,  1.9428e-02, -4.5310e-02,\n",
       "                       5.1503e-02,  4.8376e-02,  2.3071e-02, -2.3662e-02,  9.3218e-03,\n",
       "                      -2.0925e-02, -5.5407e-03, -5.9509e-02, -3.0983e-02, -2.4690e-02,\n",
       "                       9.1380e-04,  1.3549e-02,  1.9953e-02,  4.6859e-02, -3.4114e-02,\n",
       "                      -5.1062e-02,  6.6865e-03,  6.7127e-02,  4.1874e-02, -4.3330e-02,\n",
       "                      -1.8553e-02, -6.0745e-03,  2.1080e-02,  3.7205e-02,  7.0559e-02,\n",
       "                       2.4538e-03,  4.8392e-02, -2.6553e-02,  6.0579e-02,  7.8514e-05,\n",
       "                      -6.7736e-02,  6.1423e-02, -3.0700e-02, -4.1146e-02,  6.4625e-02,\n",
       "                      -4.9111e-02,  6.0362e-02,  3.3695e-02, -5.4664e-02, -3.2523e-02,\n",
       "                      -1.4184e-02, -6.9662e-03,  1.5133e-02, -3.2303e-02, -2.4421e-02,\n",
       "                       1.3122e-02,  7.1177e-03, -6.3553e-02,  5.3324e-02,  1.1476e-02,\n",
       "                       3.7516e-02, -2.0218e-02, -3.3870e-02, -4.3446e-02, -2.3048e-02,\n",
       "                       4.6302e-02,  2.4467e-02, -6.7525e-02, -3.4336e-02,  4.8384e-03,\n",
       "                      -3.7482e-02,  6.8532e-02, -5.8042e-02,  5.2514e-02, -6.0420e-02,\n",
       "                       4.9694e-02,  2.0831e-03, -1.9508e-02,  5.5884e-02,  5.7609e-02,\n",
       "                       4.6036e-02, -5.8713e-02,  5.8262e-02, -4.6621e-02,  5.2455e-02,\n",
       "                      -4.1111e-02,  5.3659e-02,  5.9569e-03, -1.2705e-02,  2.1821e-02,\n",
       "                       6.5786e-02,  1.3652e-02,  2.8407e-02,  1.4388e-02, -5.2088e-02,\n",
       "                      -6.7545e-02, -9.3338e-03,  6.7069e-02, -3.8811e-02, -1.4096e-02,\n",
       "                       3.0927e-02, -5.4513e-02, -5.9371e-02,  5.2889e-02, -4.9732e-02,\n",
       "                      -1.9810e-02,  6.5717e-03, -5.0407e-02,  7.1405e-02,  1.0896e-02,\n",
       "                      -2.0092e-02,  5.1868e-02,  7.1373e-02,  2.2178e-02,  1.3039e-02,\n",
       "                      -5.1143e-02, -4.3547e-02, -1.6202e-03, -4.4234e-02, -2.1735e-02,\n",
       "                      -2.8742e-02, -6.3911e-02,  1.5926e-02,  3.0176e-02, -5.1927e-02,\n",
       "                      -4.3206e-02, -1.1938e-02,  6.8040e-02,  2.7588e-02, -5.7328e-02,\n",
       "                       6.0560e-03, -1.6038e-03, -4.9283e-02, -3.7458e-02, -2.0878e-02,\n",
       "                       3.5801e-02, -2.7973e-02, -1.4974e-02, -6.9037e-02, -4.8777e-02,\n",
       "                       6.8793e-02, -2.9650e-02, -3.0442e-02, -4.0094e-02,  6.4465e-02,\n",
       "                      -6.5912e-03,  5.8355e-02,  5.7554e-02,  6.8107e-02,  9.2866e-03,\n",
       "                       4.6916e-02,  1.9794e-02,  6.1670e-02,  2.1817e-02, -2.5497e-02,\n",
       "                       3.5531e-02, -3.5795e-02,  7.8220e-04,  3.9902e-02, -7.1767e-03,\n",
       "                      -1.0656e-02, -4.7992e-02, -6.0357e-02, -9.6843e-03, -6.9097e-02,\n",
       "                      -6.8542e-02,  2.5506e-02, -2.8191e-02,  2.5035e-03, -2.7044e-02,\n",
       "                      -6.5169e-02,  2.1801e-03, -1.3527e-02, -7.4114e-03,  5.4545e-03,\n",
       "                      -3.0532e-02,  7.5922e-03, -1.8160e-02,  1.9378e-03, -4.2101e-02,\n",
       "                       2.9414e-02, -2.2705e-02,  3.1886e-02, -2.4860e-02,  4.7347e-02,\n",
       "                      -2.1222e-02, -4.9268e-02,  2.5492e-02, -2.4347e-02, -2.3755e-02,\n",
       "                      -1.9601e-02, -3.4589e-02, -5.3684e-02,  3.8481e-02, -6.8399e-02,\n",
       "                       5.3722e-02, -5.1559e-02,  6.1835e-02, -2.4863e-02,  4.6804e-03,\n",
       "                      -7.0803e-02,  1.2401e-02,  5.6495e-02, -9.5943e-03, -4.2099e-02,\n",
       "                       1.7129e-02,  9.0074e-03, -4.0476e-02,  6.9252e-02, -4.8529e-02,\n",
       "                      -1.5843e-02, -1.8490e-02, -4.2787e-02, -3.2290e-02,  4.5209e-02,\n",
       "                       4.5269e-03, -4.5886e-02,  1.2902e-02, -3.5702e-02,  2.1344e-02,\n",
       "                      -4.7262e-02,  6.0200e-02,  2.1858e-02, -7.6266e-03,  1.3373e-02,\n",
       "                       1.9412e-02, -8.2582e-03, -3.5848e-02,  4.4220e-02,  2.1087e-02,\n",
       "                       5.1651e-02, -4.8452e-02, -3.6284e-03, -5.9140e-02,  1.2819e-02,\n",
       "                      -6.5651e-02,  3.0070e-02, -2.7787e-02,  2.8010e-02,  3.2535e-02,\n",
       "                       3.3161e-02, -6.6249e-02, -1.8256e-02,  3.6281e-03,  7.1991e-02,\n",
       "                       1.4667e-02, -2.2738e-02, -5.9292e-02, -5.5300e-04, -2.2335e-02,\n",
       "                       3.7096e-02, -6.0906e-02, -1.9075e-02,  1.1656e-02,  2.6663e-03,\n",
       "                      -4.9521e-02,  3.6117e-02, -3.8168e-02,  3.1791e-02, -3.6770e-02,\n",
       "                      -4.1247e-02, -5.6724e-02, -3.4868e-02, -2.4503e-02,  6.5650e-02,\n",
       "                       7.1667e-02,  5.3603e-03, -1.0951e-02, -6.1022e-02,  4.7614e-02,\n",
       "                      -1.4891e-02,  3.0030e-02, -5.2964e-02,  4.5476e-02,  2.8920e-02,\n",
       "                      -4.8405e-02,  3.3937e-03,  2.5967e-02, -2.0847e-02,  5.8172e-02,\n",
       "                      -1.4404e-02, -2.6690e-02,  3.8770e-02, -6.6036e-02, -4.0186e-02,\n",
       "                      -5.2294e-03, -4.3374e-02,  6.8914e-02,  2.7839e-02,  6.3916e-02,\n",
       "                       5.9574e-02,  3.6766e-02, -6.0599e-02,  1.1032e-02,  2.4313e-02,\n",
       "                      -5.7082e-02,  1.9489e-02,  6.3811e-02, -1.0632e-02,  7.1480e-02,\n",
       "                       5.5530e-02,  5.4357e-02,  6.5374e-02,  6.6685e-02, -3.3413e-02,\n",
       "                       4.4795e-02,  1.2674e-02,  5.7154e-02, -4.8030e-02,  5.7476e-02,\n",
       "                      -2.6952e-02,  3.4101e-03,  5.9713e-02, -3.3277e-02,  5.8802e-02,\n",
       "                      -6.9765e-02,  1.8894e-02,  9.5614e-03,  2.7803e-02,  2.4030e-02,\n",
       "                       6.0768e-02, -2.8018e-02, -3.6151e-02,  5.0970e-02,  4.4299e-02,\n",
       "                       4.3475e-02,  4.7982e-02,  2.0305e-02,  8.7188e-03,  1.1523e-02,\n",
       "                      -1.1998e-02, -2.9405e-02,  4.6203e-02,  4.4734e-02,  6.8438e-02,\n",
       "                       6.1507e-02,  1.2942e-02, -4.1615e-02,  3.6333e-02, -7.1944e-02,\n",
       "                      -2.8276e-02,  4.0059e-02,  4.8001e-02, -4.8434e-02, -1.0030e-02,\n",
       "                      -3.0400e-02,  1.0195e-02, -6.4398e-03,  6.8775e-02, -5.8879e-02,\n",
       "                       2.8972e-02,  7.6187e-03, -5.8107e-02, -3.0324e-02,  3.7705e-02,\n",
       "                      -4.7811e-02,  1.3108e-02,  4.5636e-02, -2.7604e-02,  2.2269e-02,\n",
       "                      -2.0320e-02, -6.4572e-02, -6.1569e-02, -4.3524e-02,  5.0180e-02,\n",
       "                       6.7766e-02, -5.4190e-03,  6.1170e-02, -2.0255e-02,  2.6064e-02,\n",
       "                      -5.7584e-02,  3.6137e-02,  7.5771e-03, -4.3783e-03,  6.9616e-03,\n",
       "                      -1.7581e-03, -3.3922e-02,  4.9769e-02, -4.8216e-02,  4.3660e-02,\n",
       "                       3.6004e-02, -6.4716e-03,  2.9487e-02, -4.7633e-02, -1.6268e-02,\n",
       "                       7.2719e-03,  4.7383e-02,  4.3771e-02, -2.0885e-02,  3.6603e-02,\n",
       "                      -9.7802e-03,  6.8324e-02,  5.1375e-02, -4.4241e-02,  6.8116e-02,\n",
       "                       4.2813e-02, -4.3098e-02, -1.2438e-02, -4.1077e-02, -4.2924e-02,\n",
       "                       2.8375e-04, -1.2198e-02,  1.3106e-02,  5.9943e-02, -5.3199e-02,\n",
       "                       5.6202e-02, -7.0547e-02, -4.7205e-02,  5.6087e-02,  6.1263e-02,\n",
       "                      -4.4908e-03, -1.5088e-02,  3.0599e-02,  3.1670e-02, -1.1600e-02,\n",
       "                       7.0722e-02, -6.8450e-02, -3.9915e-02, -4.4220e-03, -3.0153e-03,\n",
       "                      -2.4524e-02,  2.0591e-02, -1.4447e-02,  3.9447e-02,  1.9096e-02,\n",
       "                      -1.8562e-02,  6.5945e-02,  1.2208e-02, -5.7391e-02, -4.3975e-02,\n",
       "                      -6.2413e-02,  1.7823e-02, -2.5965e-02,  3.8361e-02, -4.0105e-02,\n",
       "                      -3.9469e-02,  6.8588e-02,  3.3162e-02, -5.1306e-02, -2.4125e-02,\n",
       "                       3.0288e-02,  2.3141e-02,  4.7108e-02,  3.1882e-02, -1.2909e-02,\n",
       "                      -5.2063e-02, -5.2682e-02,  2.3311e-03, -2.6045e-02, -5.1925e-02,\n",
       "                       2.1487e-02,  6.0791e-02,  6.3228e-02, -3.5759e-02, -3.2459e-02,\n",
       "                       5.9336e-02,  4.4391e-02,  1.8963e-02,  2.3715e-02,  2.0704e-02,\n",
       "                       4.8890e-02, -3.9597e-03,  2.0832e-04,  1.8891e-02,  2.1062e-02,\n",
       "                      -2.1041e-02, -1.9799e-02,  5.7773e-02,  3.2749e-02, -6.5003e-02,\n",
       "                      -1.7928e-02, -7.1285e-02, -3.3870e-03, -5.7979e-03,  2.7767e-02,\n",
       "                      -6.0769e-02,  2.4949e-02,  1.9138e-02,  7.5457e-03, -6.7889e-02,\n",
       "                      -1.6616e-02, -1.4046e-02, -5.8412e-02, -3.3921e-04, -5.9959e-02,\n",
       "                      -1.8925e-02,  3.3044e-02,  1.7608e-02,  2.9243e-02, -4.8905e-02,\n",
       "                      -1.3407e-02, -2.8480e-02, -6.0559e-02,  5.3593e-02,  5.1936e-02,\n",
       "                       4.8460e-02, -2.1484e-02, -3.1446e-02, -5.3970e-02,  5.7937e-02,\n",
       "                       3.7644e-02, -4.6730e-02,  6.5562e-02, -4.6283e-02,  3.7185e-02,\n",
       "                      -6.5149e-03,  2.2911e-02, -2.1544e-02, -1.0623e-02,  3.1886e-02,\n",
       "                       6.4670e-02,  5.8410e-02,  4.2137e-02,  4.4110e-02,  5.6566e-02,\n",
       "                      -2.0141e-02,  5.8664e-02,  3.5341e-02,  2.9471e-02, -1.4811e-02,\n",
       "                       4.6951e-02, -7.0903e-02, -1.8413e-02, -2.3597e-02, -2.3845e-02,\n",
       "                      -2.6416e-02,  3.5289e-02, -3.7933e-02,  5.5440e-02,  1.8924e-03,\n",
       "                       2.9038e-02, -3.0910e-02, -6.6113e-03, -4.4374e-02, -4.5046e-02,\n",
       "                      -2.5723e-02,  5.0926e-02,  5.9687e-02,  3.1375e-02, -2.0549e-02,\n",
       "                       3.6308e-03, -4.4319e-02, -5.2438e-03,  2.3082e-02, -6.4180e-02,\n",
       "                      -5.1365e-02, -2.5602e-02, -6.3566e-02, -1.3767e-02, -2.2710e-02,\n",
       "                      -3.0038e-02, -1.7217e-02,  1.9288e-02,  5.3602e-02, -2.1943e-02,\n",
       "                       5.1579e-02, -3.9643e-02, -4.0332e-02,  3.5797e-02,  4.2190e-02,\n",
       "                      -4.3377e-02, -3.3909e-02,  2.0787e-02,  5.5780e-02, -4.2139e-02,\n",
       "                      -2.1700e-03, -3.2499e-02,  4.8154e-02,  3.2251e-02, -4.9627e-02,\n",
       "                      -6.0100e-02,  2.1825e-02, -3.9467e-02, -5.1243e-02,  6.9898e-02,\n",
       "                      -7.0468e-02, -5.8092e-02,  7.6197e-03, -2.9712e-02, -1.6364e-03,\n",
       "                       1.1408e-02,  3.3191e-03, -5.8964e-02, -6.3552e-02,  7.1802e-02,\n",
       "                      -3.5382e-03, -8.5224e-03,  3.9573e-02, -5.3341e-02, -1.9768e-02,\n",
       "                      -6.5905e-02,  3.7235e-02,  3.3819e-02,  3.5114e-02, -2.7704e-02,\n",
       "                      -7.0572e-02, -2.0604e-02, -2.0623e-02,  6.0252e-02,  1.5915e-02,\n",
       "                      -5.3286e-02,  1.5788e-02, -3.7277e-02,  6.0266e-02,  5.5002e-02,\n",
       "                       8.9906e-03, -1.7737e-02,  1.1683e-02,  5.1692e-02,  4.1082e-02,\n",
       "                       7.7245e-03,  6.7721e-03, -8.1492e-03, -1.2455e-02,  4.7902e-02,\n",
       "                       4.4343e-03,  5.8800e-02, -2.2723e-02,  8.7726e-03,  5.1851e-02,\n",
       "                      -6.4998e-02, -3.5898e-02,  7.2006e-02,  4.8993e-02, -4.0729e-02,\n",
       "                      -2.0886e-02,  1.9670e-02,  1.8878e-03,  5.8857e-03, -2.5758e-02,\n",
       "                      -2.7206e-02, -1.4092e-02,  4.9741e-02,  1.6482e-02, -5.3948e-02,\n",
       "                       7.0620e-03, -6.4991e-02,  1.2322e-03, -1.1854e-02, -1.8077e-02,\n",
       "                       5.7774e-02, -1.0030e-02, -4.6204e-02, -1.4523e-02,  1.6440e-02,\n",
       "                       6.5391e-03,  4.1499e-02,  4.3959e-02, -6.0637e-02, -3.3415e-02,\n",
       "                       4.8195e-02, -4.3324e-03, -6.6914e-02, -1.7045e-02, -4.8979e-02,\n",
       "                       6.7244e-02, -6.1134e-02,  1.1105e-02, -3.6890e-02,  3.6160e-02,\n",
       "                       7.1209e-02, -4.4815e-02,  1.3169e-02,  4.7244e-02,  1.4324e-02,\n",
       "                       5.6400e-02,  2.3260e-03,  2.2314e-02,  3.8840e-03, -9.8096e-03,\n",
       "                      -2.6270e-02,  4.6324e-02,  6.5156e-02,  4.9190e-02,  5.4598e-02,\n",
       "                       2.0741e-02, -1.3553e-02, -2.5457e-02, -3.7632e-02,  6.0643e-02,\n",
       "                      -3.9543e-02, -8.2316e-03, -5.9013e-02, -6.7959e-03, -1.3040e-02,\n",
       "                       5.2756e-02,  5.5999e-02, -1.9423e-02,  7.0106e-02, -4.1413e-02,\n",
       "                      -2.8786e-02, -2.3593e-02, -6.1000e-02,  2.3679e-02, -5.0408e-02,\n",
       "                      -1.8174e-02, -5.1104e-02,  5.1362e-03, -4.3770e-02,  7.0049e-02,\n",
       "                       4.9935e-02,  6.5640e-03, -6.2185e-02])),\n",
       "             ('swinViT.layers4.0.blocks.0.mlp.linear2.weight',\n",
       "              tensor([[ 0.0188,  0.0316, -0.0050,  ...,  0.0078,  0.0073, -0.0126],\n",
       "                      [-0.0195,  0.0156, -0.0064,  ...,  0.0143,  0.0267, -0.0258],\n",
       "                      [ 0.0009,  0.0186, -0.0130,  ...,  0.0189, -0.0264,  0.0350],\n",
       "                      ...,\n",
       "                      [ 0.0245,  0.0305,  0.0233,  ...,  0.0104,  0.0030, -0.0267],\n",
       "                      [ 0.0034,  0.0079,  0.0060,  ..., -0.0111, -0.0326, -0.0087],\n",
       "                      [ 0.0325, -0.0185,  0.0220,  ...,  0.0062,  0.0003, -0.0152]])),\n",
       "             ('swinViT.layers4.0.blocks.0.mlp.linear2.bias',\n",
       "              tensor([-1.3919e-02,  3.2451e-03,  8.7454e-03, -1.2140e-02,  6.7637e-03,\n",
       "                      -1.8453e-02,  3.1972e-02, -1.3508e-02,  4.0927e-03,  1.5724e-02,\n",
       "                      -3.5767e-03, -2.6446e-02,  9.5842e-03,  1.7836e-02,  3.4999e-02,\n",
       "                      -1.6582e-02, -2.5881e-02, -1.2396e-02,  1.7588e-02, -1.2260e-03,\n",
       "                       1.7361e-02, -3.4961e-02,  1.1615e-02, -3.4865e-02,  3.1904e-02,\n",
       "                      -2.8941e-02, -1.3371e-02, -1.1653e-02,  1.3746e-02,  1.1482e-04,\n",
       "                      -2.6291e-02, -4.7859e-04,  4.9269e-03,  3.1192e-02,  2.4213e-02,\n",
       "                      -2.3171e-02, -6.0916e-03,  1.8695e-02, -2.6345e-02,  1.9058e-02,\n",
       "                       1.8605e-02, -1.8940e-02, -1.7978e-03,  3.0354e-02, -1.5910e-02,\n",
       "                       8.5898e-05,  3.2530e-02,  2.0634e-02, -1.0582e-03, -9.5384e-03,\n",
       "                       1.9939e-02,  1.7464e-02,  2.0514e-02,  4.3887e-03,  2.9717e-03,\n",
       "                      -2.5150e-02,  2.4408e-02, -7.9922e-03, -3.4684e-03,  2.8297e-02,\n",
       "                       3.2357e-02,  2.6558e-02, -2.2728e-02,  1.7056e-02,  1.9778e-02,\n",
       "                       2.7172e-02,  2.8202e-02,  5.9509e-03,  1.5553e-02, -1.3183e-02,\n",
       "                      -3.2640e-02, -4.1837e-03, -1.3579e-02,  2.9301e-02,  2.1757e-03,\n",
       "                      -2.0066e-02,  1.8304e-02, -1.6514e-02,  2.6313e-02, -1.4002e-02,\n",
       "                      -4.0809e-03,  1.3866e-02,  8.1681e-03,  2.1032e-02,  1.0099e-02,\n",
       "                       2.4673e-02,  2.1895e-02,  2.7387e-02, -6.3205e-03, -3.1717e-02,\n",
       "                      -2.5109e-02,  2.4045e-02,  3.1566e-02,  5.5880e-03, -2.6135e-02,\n",
       "                       3.0972e-03,  1.5287e-02,  4.0587e-03,  1.8523e-02, -1.7781e-02,\n",
       "                      -1.3901e-02,  4.1221e-03,  1.9511e-02,  2.9579e-02,  1.3846e-02,\n",
       "                      -2.9171e-02,  7.7793e-03, -1.7153e-02,  2.9354e-03, -1.3509e-02,\n",
       "                      -1.2446e-02, -2.5041e-02, -3.5983e-03,  2.0671e-02,  2.7739e-02,\n",
       "                       9.5256e-03,  1.8726e-02,  1.9071e-02,  3.3375e-02, -3.2409e-02,\n",
       "                       1.4298e-02, -1.9053e-03,  4.3015e-03,  2.3124e-02,  3.3318e-02,\n",
       "                      -1.5647e-02,  1.8842e-02,  3.1930e-02,  2.6296e-02, -2.2582e-02,\n",
       "                      -1.1591e-02,  1.6427e-02,  3.5351e-02, -1.1707e-02,  7.0909e-03,\n",
       "                      -1.9557e-02, -2.5019e-02,  1.0024e-02,  3.2447e-02, -1.0434e-02,\n",
       "                       1.2891e-02, -1.2656e-02, -5.4508e-03,  4.4544e-03, -2.7602e-02,\n",
       "                      -1.8236e-02, -2.2190e-02,  1.5683e-02,  1.0596e-02, -2.8897e-02,\n",
       "                      -3.1814e-02, -3.3080e-03, -1.5913e-02,  2.2824e-02, -2.6145e-02,\n",
       "                      -3.4641e-02, -2.4040e-02,  2.2893e-02, -3.2371e-02, -3.1022e-02,\n",
       "                       9.2267e-03,  1.8051e-02,  5.6143e-03, -1.1499e-03, -3.5363e-02,\n",
       "                       3.5915e-02,  2.4425e-02, -3.1824e-02, -1.2378e-02, -1.0774e-02,\n",
       "                       3.0640e-02, -2.9314e-02,  1.5194e-03,  6.7726e-03, -3.4149e-02,\n",
       "                      -2.8507e-02, -2.7903e-03, -3.3940e-02,  3.1073e-02, -1.7051e-02,\n",
       "                      -1.1282e-02,  3.0369e-02,  1.4968e-02, -1.0584e-04,  7.1688e-03,\n",
       "                      -2.2928e-02, -1.1511e-02,  3.1940e-02,  3.1334e-02, -8.0486e-03,\n",
       "                      -2.1196e-03, -1.4417e-02])),\n",
       "             ('swinViT.layers4.0.blocks.1.norm1.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers4.0.blocks.1.norm1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.relative_position_bias_table',\n",
       "              tensor([[ 0.0242,  0.0247, -0.0126,  ..., -0.0040,  0.0177, -0.0108],\n",
       "                      [-0.0060, -0.0096,  0.0226,  ...,  0.0417,  0.0102,  0.0256],\n",
       "                      [-0.0215, -0.0018, -0.0260,  ..., -0.0099, -0.0230, -0.0300],\n",
       "                      ...,\n",
       "                      [-0.0043, -0.0074, -0.0065,  ..., -0.0145, -0.0038, -0.0137],\n",
       "                      [ 0.0044,  0.0142, -0.0203,  ..., -0.0101, -0.0191,  0.0063],\n",
       "                      [ 0.0081, -0.0092,  0.0232,  ...,  0.0194, -0.0204, -0.0021]])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.relative_position_index',\n",
       "              tensor([[1098, 1097, 1096,  ...,    2,    1,    0],\n",
       "                      [1099, 1098, 1097,  ...,    3,    2,    1],\n",
       "                      [1100, 1099, 1098,  ...,    4,    3,    2],\n",
       "                      ...,\n",
       "                      [2194, 2193, 2192,  ..., 1098, 1097, 1096],\n",
       "                      [2195, 2194, 2193,  ..., 1099, 1098, 1097],\n",
       "                      [2196, 2195, 2194,  ..., 1100, 1099, 1098]])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.qkv.weight',\n",
       "              tensor([[ 0.0576, -0.0063, -0.0641,  ...,  0.0296, -0.0279,  0.0679],\n",
       "                      [ 0.0067, -0.0523,  0.0009,  ...,  0.0224,  0.0504,  0.0529],\n",
       "                      [-0.0535,  0.0326, -0.0581,  ...,  0.0396,  0.0268, -0.0478],\n",
       "                      ...,\n",
       "                      [ 0.0560,  0.0176, -0.0085,  ..., -0.0003,  0.0102,  0.0339],\n",
       "                      [ 0.0042,  0.0449, -0.0134,  ..., -0.0521,  0.0718, -0.0465],\n",
       "                      [-0.0208, -0.0721, -0.0138,  ...,  0.0531,  0.0104, -0.0519]])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.qkv.bias',\n",
       "              tensor([ 0.0403, -0.0278, -0.0085,  0.0359,  0.0419,  0.0239, -0.0642,  0.0027,\n",
       "                      -0.0098,  0.0581, -0.0686,  0.0654,  0.0008, -0.0434, -0.0429,  0.0261,\n",
       "                      -0.0601, -0.0464,  0.0331,  0.0404, -0.0608, -0.0551, -0.0314,  0.0706,\n",
       "                       0.0636, -0.0506,  0.0255,  0.0691, -0.0013,  0.0036, -0.0261, -0.0229,\n",
       "                       0.0603, -0.0301,  0.0384,  0.0593, -0.0028,  0.0685,  0.0469, -0.0190,\n",
       "                      -0.0258, -0.0020, -0.0621, -0.0057, -0.0597,  0.0109,  0.0273, -0.0583,\n",
       "                      -0.0209, -0.0563,  0.0126, -0.0024, -0.0708,  0.0330, -0.0431, -0.0054,\n",
       "                       0.0334, -0.0418,  0.0715,  0.0214, -0.0226,  0.0109, -0.0274, -0.0535,\n",
       "                       0.0303, -0.0447, -0.0468,  0.0433, -0.0297,  0.0412, -0.0665,  0.0335,\n",
       "                       0.0278,  0.0281,  0.0174, -0.0037,  0.0017, -0.0244, -0.0650,  0.0513,\n",
       "                      -0.0129,  0.0076, -0.0290,  0.0464,  0.0598, -0.0464,  0.0666,  0.0032,\n",
       "                      -0.0694,  0.0690, -0.0330, -0.0211,  0.0309, -0.0063,  0.0270, -0.0094,\n",
       "                      -0.0190,  0.0634,  0.0189,  0.0317,  0.0166,  0.0092, -0.0226,  0.0306,\n",
       "                       0.0404,  0.0164,  0.0181, -0.0326, -0.0453,  0.0569,  0.0721,  0.0308,\n",
       "                      -0.0415,  0.0062,  0.0162, -0.0382,  0.0272,  0.0369,  0.0290, -0.0471,\n",
       "                      -0.0199, -0.0719, -0.0666, -0.0160,  0.0168,  0.0433,  0.0346,  0.0685,\n",
       "                       0.0420, -0.0034,  0.0696,  0.0037,  0.0498, -0.0032,  0.0703,  0.0434,\n",
       "                      -0.0084,  0.0304, -0.0636, -0.0553, -0.0168,  0.0575, -0.0713, -0.0126,\n",
       "                       0.0307,  0.0591,  0.0505, -0.0530,  0.0546, -0.0305, -0.0179, -0.0381,\n",
       "                      -0.0365, -0.0285, -0.0715,  0.0084, -0.0483,  0.0341,  0.0692,  0.0275,\n",
       "                      -0.0104,  0.0159, -0.0024,  0.0679,  0.0040,  0.0159,  0.0474,  0.0193,\n",
       "                       0.0380,  0.0452,  0.0093,  0.0180,  0.0487, -0.0074, -0.0119,  0.0704,\n",
       "                      -0.0142,  0.0348, -0.0659, -0.0685, -0.0565, -0.0225, -0.0642,  0.0490,\n",
       "                      -0.0033, -0.0202,  0.0514,  0.0181, -0.0709,  0.0026,  0.0515, -0.0665,\n",
       "                       0.0214,  0.0699, -0.0355,  0.0371, -0.0605,  0.0301,  0.0627, -0.0001,\n",
       "                       0.0408,  0.0594,  0.0681, -0.0127, -0.0418,  0.0387,  0.0430, -0.0440,\n",
       "                       0.0660,  0.0398,  0.0299,  0.0389, -0.0518, -0.0544,  0.0051,  0.0583,\n",
       "                       0.0178,  0.0154, -0.0560,  0.0326, -0.0468,  0.0424,  0.0119,  0.0565,\n",
       "                       0.0074, -0.0694,  0.0387, -0.0255,  0.0051,  0.0621,  0.0092,  0.0090,\n",
       "                      -0.0373, -0.0368, -0.0544,  0.0056, -0.0274,  0.0632,  0.0403,  0.0237,\n",
       "                       0.0301, -0.0537, -0.0159, -0.0359, -0.0277, -0.0074, -0.0539, -0.0522,\n",
       "                       0.0674, -0.0181,  0.0014,  0.0508, -0.0123, -0.0499,  0.0193, -0.0296,\n",
       "                       0.0444,  0.0340, -0.0523,  0.0289, -0.0599,  0.0263, -0.0150, -0.0138,\n",
       "                       0.0280, -0.0323, -0.0249, -0.0591, -0.0584,  0.0275,  0.0427, -0.0310,\n",
       "                       0.0511, -0.0625, -0.0194, -0.0246, -0.0656, -0.0515,  0.0198, -0.0601,\n",
       "                       0.0533, -0.0435,  0.0692, -0.0234,  0.0516, -0.0385, -0.0285, -0.0100,\n",
       "                       0.0534,  0.0101,  0.0672,  0.0303,  0.0208,  0.0568,  0.0439, -0.0062,\n",
       "                       0.0710, -0.0495, -0.0247,  0.0393,  0.0011,  0.0350,  0.0419, -0.0044,\n",
       "                      -0.0250,  0.0042, -0.0203,  0.0097,  0.0661,  0.0587,  0.0263, -0.0335,\n",
       "                       0.0381, -0.0102,  0.0024,  0.0504,  0.0104, -0.0403,  0.0280, -0.0650,\n",
       "                      -0.0544,  0.0609, -0.0174, -0.0051,  0.0515,  0.0174,  0.0168, -0.0471,\n",
       "                       0.0028,  0.0366, -0.0214, -0.0091, -0.0080, -0.0235, -0.0541, -0.0502,\n",
       "                       0.0348,  0.0604, -0.0040, -0.0684,  0.0688, -0.0306, -0.0260,  0.0299,\n",
       "                      -0.0638, -0.0291,  0.0533,  0.0217, -0.0059,  0.0171, -0.0368, -0.0291,\n",
       "                       0.0510,  0.0612, -0.0481, -0.0622,  0.0636,  0.0201,  0.0520, -0.0422,\n",
       "                       0.0464,  0.0012,  0.0424, -0.0234, -0.0007, -0.0200, -0.0280,  0.0684,\n",
       "                       0.0206, -0.0481, -0.0341,  0.0281, -0.0614, -0.0168, -0.0019,  0.0047,\n",
       "                       0.0154, -0.0517, -0.0553,  0.0168,  0.0157, -0.0518, -0.0071, -0.0386,\n",
       "                      -0.0570,  0.0280,  0.0341,  0.0069,  0.0507,  0.0594,  0.0202,  0.0476,\n",
       "                       0.0620,  0.0481, -0.0221,  0.0391,  0.0038,  0.0199, -0.0260,  0.0065,\n",
       "                       0.0167,  0.0345, -0.0690, -0.0517, -0.0716,  0.0197,  0.0656, -0.0694,\n",
       "                      -0.0380, -0.0272,  0.0167,  0.0210,  0.0549,  0.0711, -0.0459, -0.0295,\n",
       "                       0.0445,  0.0371,  0.0189, -0.0123,  0.0517, -0.0246,  0.0224,  0.0202,\n",
       "                      -0.0335, -0.0568, -0.0149, -0.0162, -0.0085, -0.0444, -0.0617, -0.0674,\n",
       "                       0.0573, -0.0164, -0.0387, -0.0544, -0.0567,  0.0109,  0.0017,  0.0391,\n",
       "                       0.0136,  0.0534,  0.0345,  0.0592, -0.0103, -0.0195,  0.0537,  0.0036,\n",
       "                       0.0472, -0.0030,  0.0264,  0.0144, -0.0556, -0.0247, -0.0340,  0.0553,\n",
       "                       0.0182,  0.0121,  0.0094,  0.0087,  0.0457,  0.0029,  0.0133, -0.0338,\n",
       "                      -0.0548, -0.0234, -0.0536,  0.0660,  0.0421,  0.0364, -0.0017,  0.0426,\n",
       "                      -0.0669, -0.0024,  0.0067,  0.0324,  0.0098,  0.0059,  0.0337, -0.0119,\n",
       "                      -0.0503, -0.0089,  0.0162, -0.0705, -0.0163,  0.0694, -0.0516, -0.0178,\n",
       "                       0.0417,  0.0119, -0.0373,  0.0443,  0.0377,  0.0137,  0.0290,  0.0708,\n",
       "                       0.0124, -0.0621, -0.0148,  0.0657,  0.0223, -0.0591, -0.0453,  0.0518,\n",
       "                       0.0182, -0.0632, -0.0150,  0.0437, -0.0347,  0.0518, -0.0696,  0.0196,\n",
       "                       0.0629, -0.0115,  0.0504, -0.0509, -0.0639, -0.0361,  0.0340, -0.0004,\n",
       "                       0.0699, -0.0116,  0.0469, -0.0353, -0.0695, -0.0547, -0.0399, -0.0275,\n",
       "                       0.0617,  0.0104,  0.0425,  0.0289, -0.0626,  0.0278,  0.0231,  0.0293,\n",
       "                       0.0006, -0.0569,  0.0375, -0.0130, -0.0439, -0.0140, -0.0503,  0.0624,\n",
       "                      -0.0338, -0.0007,  0.0588,  0.0307, -0.0069,  0.0675,  0.0154,  0.0620,\n",
       "                      -0.0188, -0.0562, -0.0476, -0.0190,  0.0618, -0.0713,  0.0455, -0.0644,\n",
       "                       0.0150, -0.0325, -0.0425, -0.0129, -0.0687, -0.0589,  0.0326,  0.0393,\n",
       "                       0.0625, -0.0542,  0.0189, -0.0564, -0.0697, -0.0224,  0.0464,  0.0556])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.proj.weight',\n",
       "              tensor([[-0.0382,  0.0203, -0.0059,  ..., -0.0593, -0.0473, -0.0665],\n",
       "                      [-0.0458,  0.0146, -0.0712,  ...,  0.0486, -0.0490,  0.0036],\n",
       "                      [-0.0104, -0.0321, -0.0220,  ...,  0.0649, -0.0056, -0.0264],\n",
       "                      ...,\n",
       "                      [-0.0224,  0.0027,  0.0216,  ..., -0.0193, -0.0514,  0.0543],\n",
       "                      [-0.0045,  0.0337,  0.0287,  ..., -0.0320,  0.0108, -0.0534],\n",
       "                      [ 0.0701,  0.0504, -0.0286,  ..., -0.0256,  0.0328,  0.0155]])),\n",
       "             ('swinViT.layers4.0.blocks.1.attn.proj.bias',\n",
       "              tensor([-0.0660,  0.0503,  0.0620, -0.0220,  0.0004, -0.0321,  0.0173, -0.0482,\n",
       "                      -0.0362, -0.0180, -0.0168, -0.0440,  0.0465, -0.0076, -0.0330, -0.0222,\n",
       "                      -0.0140, -0.0488,  0.0083, -0.0317, -0.0674, -0.0102,  0.0679,  0.0617,\n",
       "                      -0.0223, -0.0420,  0.0223, -0.0568,  0.0615, -0.0412, -0.0457,  0.0196,\n",
       "                       0.0439,  0.0275, -0.0389, -0.0489,  0.0489, -0.0582,  0.0272, -0.0584,\n",
       "                      -0.0359,  0.0344, -0.0053, -0.0467, -0.0366, -0.0626,  0.0037, -0.0612,\n",
       "                       0.0454,  0.0308, -0.0546,  0.0589, -0.0028, -0.0516, -0.0098,  0.0057,\n",
       "                       0.0371, -0.0684, -0.0452,  0.0373,  0.0278,  0.0605,  0.0203, -0.0250,\n",
       "                       0.0013,  0.0560,  0.0198, -0.0257, -0.0076,  0.0675,  0.0536, -0.0593,\n",
       "                      -0.0349, -0.0370, -0.0368, -0.0703,  0.0060,  0.0278, -0.0643, -0.0569,\n",
       "                      -0.0255, -0.0059,  0.0311,  0.0642, -0.0622,  0.0172,  0.0081,  0.0586,\n",
       "                      -0.0259, -0.0321,  0.0275, -0.0527,  0.0450, -0.0568, -0.0090, -0.0585,\n",
       "                       0.0279, -0.0062, -0.0013,  0.0344,  0.0429, -0.0699,  0.0200,  0.0203,\n",
       "                      -0.0098,  0.0506, -0.0330,  0.0284, -0.0233,  0.0343, -0.0169,  0.0085,\n",
       "                       0.0175, -0.0647, -0.0314, -0.0659,  0.0446,  0.0528, -0.0513, -0.0712,\n",
       "                      -0.0703, -0.0672, -0.0713, -0.0352,  0.0680, -0.0410,  0.0627, -0.0165,\n",
       "                       0.0545, -0.0116, -0.0710, -0.0615,  0.0562,  0.0330, -0.0183, -0.0543,\n",
       "                      -0.0215, -0.0397,  0.0541, -0.0256,  0.0157, -0.0292,  0.0130,  0.0720,\n",
       "                      -0.0503,  0.0282, -0.0112,  0.0715,  0.0563, -0.0285, -0.0648,  0.0602,\n",
       "                       0.0684,  0.0129, -0.0067, -0.0708, -0.0146, -0.0045, -0.0434, -0.0347,\n",
       "                      -0.0130,  0.0377, -0.0165,  0.0330, -0.0019, -0.0162,  0.0224,  0.0631,\n",
       "                      -0.0031,  0.0024, -0.0674, -0.0091, -0.0185, -0.0282,  0.0542, -0.0034,\n",
       "                      -0.0161,  0.0050,  0.0611, -0.0409,  0.0687, -0.0640, -0.0513, -0.0253,\n",
       "                       0.0278, -0.0646, -0.0020,  0.0236,  0.0346,  0.0410,  0.0048, -0.0539])),\n",
       "             ('swinViT.layers4.0.blocks.1.norm2.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('swinViT.layers4.0.blocks.1.norm2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "             ('swinViT.layers4.0.blocks.1.mlp.linear1.weight',\n",
       "              tensor([[-0.0069,  0.0279, -0.0557,  ..., -0.0513, -0.0689, -0.0632],\n",
       "                      [-0.0141,  0.0557,  0.0235,  ...,  0.0555, -0.0626,  0.0619],\n",
       "                      [-0.0419, -0.0327, -0.0645,  ...,  0.0140,  0.0148,  0.0664],\n",
       "                      ...,\n",
       "                      [-0.0277,  0.0496,  0.0040,  ...,  0.0243,  0.0144,  0.0508],\n",
       "                      [-0.0342,  0.0664, -0.0102,  ...,  0.0310,  0.0473, -0.0139],\n",
       "                      [-0.0672, -0.0348, -0.0052,  ...,  0.0302,  0.0602,  0.0053]])),\n",
       "             ('swinViT.layers4.0.blocks.1.mlp.linear1.bias',\n",
       "              tensor([ 1.4808e-02, -1.0888e-02, -7.0422e-02, -4.3225e-02, -8.9300e-03,\n",
       "                       4.7775e-02,  1.7929e-02,  1.8407e-02,  1.0851e-02,  4.2538e-02,\n",
       "                       2.5572e-02, -5.1032e-02, -3.6745e-02,  5.7563e-02, -2.6301e-03,\n",
       "                       4.1756e-02,  4.8557e-02, -4.1034e-02,  5.7934e-02, -1.4146e-02,\n",
       "                       2.9658e-04,  1.8011e-02, -7.0377e-02,  1.5991e-02, -5.6663e-02,\n",
       "                      -5.3037e-02,  4.1919e-02,  3.8248e-02,  5.7165e-02,  6.8143e-02,\n",
       "                      -2.7015e-02,  6.6728e-02, -6.7682e-02, -9.6390e-03,  4.7581e-02,\n",
       "                      -1.5870e-02,  6.8653e-02,  3.5931e-02,  4.7677e-02, -6.5435e-02,\n",
       "                       2.5751e-02, -5.4609e-02, -5.6901e-02, -2.4710e-02, -5.5257e-02,\n",
       "                       6.9702e-02, -2.6878e-02, -3.4975e-02, -5.2537e-02,  9.3580e-03,\n",
       "                      -7.1479e-02,  5.8299e-02,  7.8243e-03, -2.4172e-02, -3.8734e-02,\n",
       "                       1.1667e-02,  4.5335e-02, -4.3303e-02, -5.6895e-02, -6.4580e-02,\n",
       "                       2.5383e-02,  2.1410e-02, -5.3877e-02, -7.2068e-02, -5.2296e-02,\n",
       "                      -5.2887e-02,  1.6394e-03,  5.1342e-02,  1.3626e-02,  8.7785e-03,\n",
       "                      -4.9644e-02,  1.9816e-02,  1.0792e-02, -5.9641e-02,  7.1319e-02,\n",
       "                       3.8699e-02,  5.7091e-02, -4.6875e-02, -3.6954e-03, -2.4288e-02,\n",
       "                       1.3313e-02,  6.1633e-02,  1.9487e-02, -4.7978e-02,  1.7105e-02,\n",
       "                      -4.2795e-02,  3.1691e-02,  4.2113e-02, -5.0302e-02,  1.5366e-02,\n",
       "                       6.1768e-02, -1.6899e-02,  2.2321e-02,  3.3416e-02, -1.8182e-03,\n",
       "                       5.4825e-02,  4.3496e-02, -6.3577e-02,  1.1834e-02, -1.2943e-02,\n",
       "                       2.9117e-02, -2.7605e-02, -2.0724e-02,  4.8962e-02,  1.4367e-02,\n",
       "                      -4.9628e-02, -5.3189e-02,  1.1140e-02,  2.1848e-02, -1.6962e-02,\n",
       "                      -6.8608e-03, -5.2546e-02,  3.3962e-02,  2.9099e-02,  4.0719e-02,\n",
       "                      -5.7313e-02, -7.1118e-02,  2.4578e-02, -1.3788e-02,  5.9970e-03,\n",
       "                       4.6700e-02, -6.2545e-02,  2.3351e-02, -7.1861e-02, -3.4791e-02,\n",
       "                      -1.6772e-02, -4.9705e-02, -3.7180e-03,  6.5082e-02,  3.5209e-02,\n",
       "                       6.7089e-02,  6.0760e-02,  9.9134e-03, -4.4819e-02, -4.3639e-02,\n",
       "                       1.3937e-02,  4.0918e-02,  2.1621e-02, -3.7918e-02,  4.7143e-02,\n",
       "                       9.1658e-03,  6.3722e-02,  3.5444e-02,  1.2086e-02, -3.9535e-02,\n",
       "                       5.6812e-02, -4.2328e-02, -3.4559e-02,  6.0792e-02,  9.6983e-03,\n",
       "                      -3.3641e-02,  5.9243e-03, -6.4143e-02, -6.6233e-02,  2.6639e-02,\n",
       "                       1.8079e-02,  1.3178e-02, -4.7901e-02,  1.0247e-03, -5.6539e-03,\n",
       "                       2.6937e-02, -3.7326e-02, -4.8604e-03,  7.1745e-02, -8.7752e-03,\n",
       "                       1.6448e-02, -5.0251e-03,  5.0559e-02,  7.5293e-03,  6.0029e-02,\n",
       "                       2.9879e-02,  5.6479e-02,  2.3273e-02,  6.9135e-02,  4.9606e-02,\n",
       "                       2.9779e-02, -2.0646e-02, -1.9651e-02,  3.6018e-02,  5.7584e-02,\n",
       "                      -2.2882e-02, -6.4414e-02, -9.5064e-03,  6.0557e-02, -2.0024e-02,\n",
       "                       7.2068e-02, -7.1109e-02, -5.1924e-02,  3.6024e-02, -6.0746e-02,\n",
       "                      -5.8685e-02,  2.4261e-03,  2.7613e-02,  1.9003e-02,  4.8009e-02,\n",
       "                      -4.7366e-02, -1.9306e-02, -4.2116e-02, -3.8954e-02,  9.2629e-03,\n",
       "                      -4.5078e-02, -2.1101e-03,  4.1822e-02, -1.2943e-02, -5.9490e-02,\n",
       "                       2.1556e-02,  4.9118e-02,  5.0093e-02,  3.5495e-02,  2.5637e-02,\n",
       "                       2.9419e-02, -5.5113e-03, -1.8186e-02, -4.8240e-02, -3.5640e-02,\n",
       "                      -2.0960e-02, -2.1536e-02, -3.7265e-02,  4.1517e-02,  7.1997e-02,\n",
       "                      -1.4661e-02,  2.6592e-02, -1.6214e-02, -2.1874e-02,  1.9783e-02,\n",
       "                      -2.9608e-02, -4.0680e-02,  6.0055e-02,  2.8562e-02,  3.2202e-02,\n",
       "                      -3.2066e-02, -5.9287e-02, -4.2244e-02, -4.8531e-02,  2.5102e-03,\n",
       "                      -3.0168e-02, -3.3349e-02,  5.5081e-02, -6.6268e-02,  1.8304e-02,\n",
       "                       2.0316e-02,  1.1079e-02, -4.5716e-02,  4.3966e-02, -2.0462e-03,\n",
       "                       6.5705e-02,  2.7536e-02, -1.5187e-02, -7.0242e-02, -3.3233e-02,\n",
       "                      -2.2356e-02, -1.8195e-02, -6.2290e-02,  1.4374e-03, -5.2994e-02,\n",
       "                      -5.0160e-03,  3.8875e-02,  6.4540e-02,  6.2114e-03,  1.7019e-02,\n",
       "                      -2.8513e-02, -6.1578e-02,  1.1765e-02,  2.7202e-02,  4.8835e-02,\n",
       "                       4.7831e-02, -1.3807e-02, -3.1673e-02,  1.0505e-02,  6.0107e-02,\n",
       "                      -3.1947e-02, -2.5762e-02,  5.3568e-02,  6.0973e-02, -2.1567e-02,\n",
       "                      -2.7010e-03,  2.2676e-02,  1.2187e-02,  6.3182e-02,  5.3814e-02,\n",
       "                      -2.1989e-02,  4.1658e-02, -4.6560e-03,  1.3494e-02, -1.7976e-02,\n",
       "                       8.7101e-03, -5.5417e-02, -5.5335e-02,  6.5372e-02, -1.8494e-02,\n",
       "                      -1.7507e-02,  6.7057e-02, -1.8867e-03,  5.2833e-02, -2.1836e-02,\n",
       "                       1.3564e-02, -5.7683e-02, -1.9461e-02,  6.7770e-02, -3.5334e-02,\n",
       "                      -1.9675e-02,  5.9141e-02, -5.0308e-02, -4.4872e-02, -3.4392e-02,\n",
       "                      -2.5360e-02,  4.3971e-02,  2.1415e-02,  5.3031e-02, -3.0421e-02,\n",
       "                      -4.2174e-02, -2.9523e-02,  6.4766e-02,  4.9581e-02,  1.6240e-04,\n",
       "                       6.9225e-02, -4.2398e-02,  6.6378e-02,  3.6851e-02, -5.0645e-02,\n",
       "                       1.0310e-02, -1.9208e-02,  2.7308e-02, -5.3746e-02,  5.1784e-02,\n",
       "                      -3.1790e-02, -5.3453e-03,  4.9755e-02, -4.3450e-02,  4.1728e-02,\n",
       "                       7.3226e-03,  7.2029e-02,  5.0122e-03, -7.1343e-02, -3.0465e-02,\n",
       "                      -3.6068e-02,  5.0052e-02, -6.7286e-02, -3.7011e-02, -2.4202e-03,\n",
       "                      -3.6214e-02,  3.7139e-02,  1.8644e-02,  1.1384e-02,  6.6331e-02,\n",
       "                       2.2682e-02, -6.3631e-02, -6.1682e-02,  2.0993e-02, -3.5746e-02,\n",
       "                      -2.3056e-02,  6.9572e-03, -2.9867e-02, -1.0691e-02,  3.1825e-02,\n",
       "                       5.5249e-02, -7.0335e-02, -4.0801e-02,  4.4885e-02, -5.2463e-02,\n",
       "                      -4.5391e-02, -1.5729e-02, -3.1742e-02, -4.9101e-02, -6.2599e-02,\n",
       "                       5.3249e-02, -5.3722e-02,  3.4327e-02,  5.5769e-02, -1.5574e-02,\n",
       "                       5.1854e-02, -5.5651e-02,  3.8740e-03,  5.3203e-02,  7.1060e-02,\n",
       "                      -4.1576e-02,  2.7392e-02,  3.3370e-03, -4.7767e-03,  6.9785e-02,\n",
       "                       4.0920e-02, -2.8117e-02,  5.2034e-02,  4.0090e-02, -4.2819e-02,\n",
       "                      -1.2527e-02, -4.9820e-02, -5.0089e-02,  2.4329e-02, -6.4156e-02,\n",
       "                      -5.8724e-02, -4.9688e-02, -9.7714e-03, -3.7980e-02, -5.9434e-02,\n",
       "                       3.5261e-02, -1.4353e-02, -5.2397e-02,  1.0635e-02,  3.9672e-02,\n",
       "                       5.5052e-02, -1.1253e-03, -2.7477e-03, -3.1549e-02, -6.4833e-02,\n",
       "                      -6.6439e-02,  3.9298e-02, -3.0421e-02, -6.3200e-02, -3.5041e-02,\n",
       "                       1.3156e-02,  3.2334e-04,  4.1343e-02,  5.5823e-03,  3.7687e-02,\n",
       "                      -2.0421e-02,  2.9545e-02, -5.3120e-02,  5.4065e-02, -3.8251e-02,\n",
       "                      -1.3083e-02, -6.5443e-02,  1.7738e-02, -6.6660e-02,  7.1402e-02,\n",
       "                       1.9268e-02, -1.8773e-02,  5.7433e-02,  1.6933e-02, -1.9399e-02,\n",
       "                       6.2205e-02,  3.5935e-02, -8.3325e-03,  3.7158e-02, -6.4398e-02,\n",
       "                       5.7976e-02, -6.7821e-02, -5.2447e-02, -1.0001e-02, -2.6626e-02,\n",
       "                      -5.0454e-02,  3.3180e-02, -7.0614e-02,  8.6959e-03, -3.8121e-03,\n",
       "                      -5.9281e-02, -4.9296e-02, -4.8360e-02, -6.5641e-02, -8.6542e-03,\n",
       "                       3.0985e-02,  3.2704e-02, -6.0135e-02,  6.6153e-02, -4.2793e-02,\n",
       "                      -4.5155e-02, -6.5987e-02,  6.7523e-02, -3.7418e-02,  6.4532e-02,\n",
       "                       5.0993e-02,  6.5313e-02, -1.6371e-02, -4.0407e-02, -3.7912e-02,\n",
       "                      -7.5178e-03, -2.9809e-02,  1.7734e-02,  1.1380e-02,  5.3439e-02,\n",
       "                       6.0805e-02, -5.9528e-02,  4.8103e-02,  3.5621e-02, -5.1103e-02,\n",
       "                      -6.4187e-02,  2.0395e-02, -4.6443e-02,  3.4812e-02,  6.8622e-02,\n",
       "                      -3.7365e-02, -3.8269e-02, -1.2278e-02,  3.0183e-02, -1.0348e-02,\n",
       "                       4.5991e-02,  1.1169e-02,  3.0352e-02,  6.5021e-02,  4.9668e-02,\n",
       "                       6.4826e-02,  3.3304e-02, -3.6548e-02,  3.7886e-02, -1.9845e-02,\n",
       "                       4.4528e-02,  5.5073e-02,  2.7790e-02,  1.6866e-02, -1.3176e-02,\n",
       "                       2.8626e-02, -1.7216e-02, -3.7956e-02,  4.1410e-02,  2.2497e-02,\n",
       "                       1.1820e-02, -5.2085e-02,  2.1362e-02, -4.7803e-02,  6.9422e-03,\n",
       "                      -5.5508e-02,  2.6239e-02,  4.4184e-02,  2.1110e-02,  4.4345e-02,\n",
       "                       9.5911e-03,  6.9672e-02,  6.5328e-03, -2.2961e-02,  6.3336e-02,\n",
       "                      -1.4222e-02,  4.8036e-02,  1.5429e-02, -8.6930e-03, -6.7039e-02,\n",
       "                      -7.1506e-02, -1.6713e-02, -2.0089e-02,  3.7365e-02, -5.5322e-02,\n",
       "                       1.2591e-02, -5.3492e-02,  5.5560e-02,  4.0546e-02,  3.4012e-02,\n",
       "                      -4.5121e-02,  6.3633e-02, -4.9614e-02,  3.4388e-02, -3.6009e-02,\n",
       "                      -2.9062e-02, -1.5379e-02,  6.5820e-02,  6.5402e-02, -3.5531e-02,\n",
       "                      -4.4932e-02, -2.2887e-02, -3.9721e-02,  1.3732e-02,  6.8545e-02,\n",
       "                      -3.8509e-02,  1.4873e-02,  3.0167e-02,  6.8036e-02, -6.5680e-02,\n",
       "                       3.3927e-02, -1.3708e-02,  6.0732e-02, -4.8827e-02, -6.3863e-02,\n",
       "                      -3.3893e-02,  1.1553e-02, -5.0737e-02,  3.7889e-03,  2.2696e-02,\n",
       "                      -3.2622e-03,  2.2141e-02,  3.6441e-02,  2.8791e-02, -6.8539e-02,\n",
       "                      -5.0752e-02,  2.7070e-02, -5.2119e-02, -5.3077e-02,  5.8774e-02,\n",
       "                      -2.2102e-02, -6.5585e-02, -4.7269e-02, -2.5526e-02, -5.4613e-02,\n",
       "                      -1.8087e-02,  3.4260e-02, -5.4032e-02,  4.0659e-02, -4.1677e-02,\n",
       "                       6.0120e-02,  3.4172e-03, -2.5673e-03, -2.1390e-02,  2.4384e-02,\n",
       "                      -4.4566e-02,  6.0538e-02,  6.2955e-02,  3.6683e-02,  3.1712e-02,\n",
       "                      -5.0422e-02, -5.3042e-03, -4.7207e-02,  5.3357e-02, -6.8839e-02,\n",
       "                       7.0657e-02, -3.4586e-02, -2.2364e-02, -3.4723e-02, -8.4238e-03,\n",
       "                      -3.6024e-02, -3.5210e-02,  6.5933e-02, -5.3693e-02,  2.5339e-02,\n",
       "                      -1.2795e-02, -2.9510e-02,  6.1450e-03,  3.2994e-02, -2.3958e-02,\n",
       "                       6.0269e-02,  4.6376e-02,  2.0489e-02, -4.2190e-02, -7.0002e-02,\n",
       "                      -3.1910e-03,  2.1116e-02, -4.3630e-02,  1.5705e-02, -3.7279e-02,\n",
       "                       5.3259e-03,  4.3655e-02,  2.6212e-02,  8.2466e-03, -7.6624e-04,\n",
       "                      -1.0118e-02,  3.9665e-02,  5.7812e-02, -5.3631e-02,  6.5255e-02,\n",
       "                      -3.3537e-02,  7.0222e-02,  1.6185e-02, -1.2113e-02, -6.3787e-02,\n",
       "                      -5.3304e-02,  5.3399e-03, -3.4837e-02,  6.5674e-02, -2.5720e-02,\n",
       "                      -2.2063e-02,  5.9823e-02,  1.0721e-05,  4.8814e-02, -5.1187e-02,\n",
       "                      -2.8676e-02, -6.2828e-02, -1.4346e-02,  1.1609e-02, -6.3538e-02,\n",
       "                       1.7926e-02,  5.3046e-02,  2.3724e-03, -1.6177e-02,  2.7860e-02,\n",
       "                      -5.3492e-02,  2.3693e-02, -6.5363e-02, -4.2022e-02, -5.6940e-02,\n",
       "                      -5.4462e-02,  6.4219e-02, -5.7876e-02,  5.0135e-02,  5.2876e-02,\n",
       "                      -3.5860e-02, -6.3455e-02, -6.6644e-02,  5.3714e-02,  7.0337e-02,\n",
       "                       1.7318e-02, -4.6940e-02,  3.8479e-02,  2.6749e-02, -6.5941e-02,\n",
       "                       1.3676e-02, -6.5339e-02,  5.8910e-03, -3.0666e-02,  4.7104e-02,\n",
       "                      -2.1676e-02, -6.7083e-02,  2.0216e-02, -7.1382e-02,  5.3303e-02,\n",
       "                       6.9136e-02, -4.9754e-04,  9.2668e-03,  1.3741e-02, -4.4744e-02,\n",
       "                       5.4940e-02,  5.2206e-02,  6.5950e-02, -6.4808e-02,  1.7126e-02,\n",
       "                      -2.4114e-02, -5.0187e-02,  1.1788e-02, -2.0628e-02, -3.8697e-02,\n",
       "                       1.7996e-02, -7.1336e-02,  3.8207e-02,  6.2613e-02, -1.1308e-02,\n",
       "                      -1.6280e-02,  6.8728e-02, -1.6064e-02, -8.3143e-04,  4.6925e-03,\n",
       "                       1.3806e-02,  5.7980e-02,  2.5654e-02,  2.6636e-02, -2.0520e-03,\n",
       "                       9.2607e-03,  2.7999e-02, -1.2207e-02, -5.1778e-03,  2.9033e-02,\n",
       "                      -4.1968e-02,  1.8623e-02, -3.8415e-02,  5.2893e-02,  5.8533e-02,\n",
       "                       2.0184e-02, -3.3148e-02, -5.6704e-03, -6.5638e-02, -4.3583e-02,\n",
       "                      -1.4702e-02,  2.8279e-02,  5.5147e-02,  3.7973e-02, -2.3240e-02,\n",
       "                      -4.1978e-02, -5.0505e-03,  6.2733e-02,  2.1500e-02,  3.1739e-02,\n",
       "                       1.9153e-02, -2.7999e-02,  2.5536e-02,  7.1747e-02, -5.0357e-02,\n",
       "                      -7.0676e-02,  6.1503e-02,  4.3149e-02, -1.6831e-02, -1.0128e-02,\n",
       "                       2.0208e-02,  4.2299e-02, -6.2009e-03,  1.4405e-02, -2.8174e-02,\n",
       "                       1.1924e-02,  4.8938e-02,  2.1969e-02,  4.8168e-02, -8.3167e-03,\n",
       "                      -5.7022e-02, -2.6870e-02, -3.2073e-02])),\n",
       "             ('swinViT.layers4.0.blocks.1.mlp.linear2.weight',\n",
       "              tensor([[ 0.0060,  0.0349, -0.0303,  ...,  0.0352, -0.0335, -0.0030],\n",
       "                      [-0.0272, -0.0270, -0.0065,  ...,  0.0338,  0.0205, -0.0310],\n",
       "                      [-0.0160, -0.0344, -0.0212,  ..., -0.0334,  0.0150, -0.0056],\n",
       "                      ...,\n",
       "                      [ 0.0092, -0.0233,  0.0118,  ...,  0.0056, -0.0312, -0.0046],\n",
       "                      [ 0.0356,  0.0286, -0.0145,  ...,  0.0041,  0.0093,  0.0033],\n",
       "                      [-0.0299, -0.0041,  0.0295,  ...,  0.0316,  0.0142,  0.0165]])),\n",
       "             ('swinViT.layers4.0.blocks.1.mlp.linear2.bias',\n",
       "              tensor([ 0.0162,  0.0075, -0.0082, -0.0170,  0.0017, -0.0243, -0.0361, -0.0045,\n",
       "                       0.0014,  0.0259, -0.0338,  0.0226,  0.0290, -0.0007, -0.0076,  0.0308,\n",
       "                       0.0070,  0.0274,  0.0119, -0.0339,  0.0011,  0.0079,  0.0089, -0.0152,\n",
       "                       0.0314, -0.0194,  0.0350,  0.0043, -0.0332, -0.0221,  0.0054,  0.0353,\n",
       "                      -0.0148, -0.0053,  0.0310, -0.0141, -0.0019, -0.0188, -0.0349,  0.0238,\n",
       "                       0.0240,  0.0143, -0.0073,  0.0017, -0.0261, -0.0073,  0.0159,  0.0078,\n",
       "                       0.0097,  0.0096, -0.0240,  0.0065, -0.0085, -0.0158,  0.0173, -0.0344,\n",
       "                       0.0245,  0.0353,  0.0166,  0.0270,  0.0207, -0.0048,  0.0191, -0.0209,\n",
       "                       0.0276,  0.0025,  0.0071, -0.0054,  0.0152,  0.0022,  0.0109, -0.0148,\n",
       "                       0.0225,  0.0160,  0.0205,  0.0342, -0.0319, -0.0324, -0.0016,  0.0108,\n",
       "                      -0.0358,  0.0111, -0.0240, -0.0331,  0.0306, -0.0243,  0.0238,  0.0032,\n",
       "                       0.0350,  0.0232, -0.0147, -0.0187,  0.0088,  0.0168, -0.0083, -0.0283,\n",
       "                      -0.0200,  0.0108, -0.0315, -0.0270, -0.0338, -0.0137, -0.0051,  0.0102,\n",
       "                       0.0290, -0.0261, -0.0285,  0.0179,  0.0300, -0.0265,  0.0144,  0.0254,\n",
       "                       0.0353, -0.0133, -0.0148,  0.0321, -0.0269,  0.0316,  0.0278, -0.0219,\n",
       "                      -0.0012,  0.0361, -0.0223, -0.0304, -0.0103,  0.0284,  0.0172, -0.0310,\n",
       "                      -0.0210,  0.0348,  0.0140, -0.0106,  0.0330, -0.0046, -0.0047,  0.0193,\n",
       "                      -0.0305,  0.0202,  0.0157, -0.0286, -0.0042, -0.0251, -0.0037, -0.0172,\n",
       "                      -0.0038,  0.0260, -0.0328,  0.0145,  0.0345,  0.0124, -0.0350, -0.0050,\n",
       "                      -0.0016, -0.0143, -0.0312,  0.0210, -0.0182,  0.0103,  0.0041,  0.0009,\n",
       "                      -0.0341, -0.0088, -0.0212, -0.0325, -0.0190, -0.0101, -0.0230,  0.0171,\n",
       "                      -0.0013, -0.0331, -0.0247,  0.0252, -0.0130, -0.0321, -0.0270,  0.0190,\n",
       "                       0.0359, -0.0355, -0.0313,  0.0261, -0.0350,  0.0142,  0.0238,  0.0272,\n",
       "                       0.0169,  0.0004,  0.0322,  0.0160, -0.0010,  0.0186, -0.0084,  0.0024])),\n",
       "             ('swinViT.layers4.0.downsample.reduction.weight',\n",
       "              tensor([[-2.1675e-02, -1.0500e-02,  1.5831e-02,  ...,  4.6324e-03,\n",
       "                        1.4396e-05, -1.0520e-03],\n",
       "                      [ 5.1571e-03,  5.6683e-03, -2.2636e-03,  ...,  1.3925e-03,\n",
       "                        7.5495e-03, -1.4387e-02],\n",
       "                      [-2.7196e-03, -8.6573e-03, -2.3704e-02,  ..., -1.3927e-02,\n",
       "                        1.9857e-02, -5.1872e-03],\n",
       "                      ...,\n",
       "                      [-2.7006e-04,  2.2518e-02, -1.2292e-02,  ...,  1.9473e-02,\n",
       "                        1.9686e-02,  1.4138e-03],\n",
       "                      [-4.1161e-03, -2.2357e-02, -8.0487e-03,  ...,  9.3850e-03,\n",
       "                        2.4393e-02, -1.7167e-02],\n",
       "                      [ 2.2698e-02, -1.9914e-02,  1.0831e-02,  ...,  2.0863e-02,\n",
       "                        1.7400e-02, -2.5200e-02]])),\n",
       "             ('swinViT.layers4.0.downsample.norm.weight',\n",
       "              tensor([1., 1., 1.,  ..., 1., 1., 1.])),\n",
       "             ('swinViT.layers4.0.downsample.norm.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.])),\n",
       "             ('head.weight',\n",
       "              tensor([[-1.2513e-02, -4.9686e-02, -4.8570e-02, -3.7443e-02,  6.2921e-03,\n",
       "                       -4.5218e-02, -4.1184e-03, -1.5958e-02, -2.2217e-02,  2.5087e-02,\n",
       "                       -2.6613e-02, -4.6102e-02,  6.3344e-03, -3.7902e-03, -4.4280e-03,\n",
       "                        1.1649e-02,  2.7986e-02, -2.3673e-02,  3.2362e-02, -2.1424e-03,\n",
       "                       -1.6655e-02, -1.6664e-02, -9.8884e-03,  9.3424e-04, -4.2975e-03,\n",
       "                        3.1949e-02, -1.6103e-02,  2.1403e-02, -8.6410e-03, -5.0007e-02,\n",
       "                        1.3386e-02, -4.7050e-02,  3.4076e-02,  4.1370e-02,  6.2664e-03,\n",
       "                       -3.9521e-02,  3.5355e-02, -3.2872e-04, -2.7722e-02, -2.7053e-02,\n",
       "                        2.6226e-02, -2.0092e-02,  4.5989e-02, -2.7308e-02,  2.8972e-02,\n",
       "                        2.0306e-02, -2.9370e-02, -4.8960e-02,  1.0603e-02,  4.1993e-02,\n",
       "                        4.1923e-02, -4.2228e-02,  5.6749e-04, -4.2078e-02,  1.4750e-02,\n",
       "                       -5.0462e-02, -2.3853e-02, -4.3615e-04,  2.5711e-02,  9.1015e-03,\n",
       "                       -4.2824e-02, -1.1487e-02, -1.5156e-02, -1.6937e-02, -1.1422e-03,\n",
       "                       -4.5524e-02, -3.2318e-02, -4.8953e-02,  2.8347e-02, -1.4273e-02,\n",
       "                       -1.8826e-02, -1.5174e-02, -1.0037e-02, -2.7549e-02,  2.4203e-02,\n",
       "                        2.8044e-02,  4.2635e-03,  4.6004e-02, -1.7659e-02, -2.5220e-02,\n",
       "                        1.9466e-02,  3.3721e-02, -1.8389e-02,  2.0980e-02, -3.2701e-02,\n",
       "                       -8.5812e-03, -5.5236e-04, -3.9590e-02,  1.4077e-02, -5.4208e-04,\n",
       "                        1.1172e-02,  4.4033e-02, -2.1818e-02, -5.0223e-02,  3.6071e-02,\n",
       "                       -4.3981e-02, -3.6108e-02, -2.6021e-02,  3.0260e-02,  2.8908e-02,\n",
       "                       -5.0228e-02, -1.7465e-02, -2.9595e-02, -3.6577e-02,  3.0983e-02,\n",
       "                        5.9506e-03,  1.2665e-03,  1.9605e-03, -1.8747e-02,  1.4130e-02,\n",
       "                       -2.1594e-02, -1.2553e-02,  3.6906e-02,  1.7758e-02,  3.9906e-02,\n",
       "                       -5.3952e-04, -3.2993e-02,  5.0144e-02, -8.8336e-04, -4.9900e-02,\n",
       "                       -2.6062e-02,  1.6483e-02,  2.6261e-02,  4.3665e-02,  1.5157e-02,\n",
       "                       -7.1960e-03, -2.5245e-02, -1.0680e-02, -3.1204e-02,  1.8268e-02,\n",
       "                       -3.4598e-03,  3.4419e-02, -1.9158e-02, -4.8375e-02,  2.4616e-02,\n",
       "                        2.4969e-02,  3.3391e-02,  2.5100e-02, -4.6540e-02, -2.5215e-02,\n",
       "                        1.0188e-02, -4.9629e-02, -1.9997e-02, -1.3412e-02,  3.9909e-03,\n",
       "                       -5.0920e-02,  1.0927e-02,  1.9568e-02,  6.6531e-03, -1.8379e-03,\n",
       "                        1.5536e-02, -2.4211e-02,  6.4407e-03, -2.4181e-02,  4.3270e-02,\n",
       "                        1.8273e-02, -2.8275e-02, -9.8912e-03,  1.5113e-03,  1.8444e-02,\n",
       "                       -3.3928e-02, -4.0966e-02, -2.4017e-02,  1.2843e-02, -2.7087e-02,\n",
       "                        2.5192e-02,  3.9341e-02,  2.8349e-02, -7.9727e-03,  4.0409e-02,\n",
       "                        2.5288e-02, -4.0789e-03, -9.0615e-03,  3.6878e-02, -5.0687e-02,\n",
       "                       -1.2064e-02, -1.1593e-03, -4.7855e-03,  6.8083e-04,  6.0697e-03,\n",
       "                        1.9831e-02, -9.7499e-03,  4.4038e-02, -7.7429e-03, -4.7363e-02,\n",
       "                        3.2440e-02, -2.1786e-02, -1.8683e-02,  2.1796e-02, -1.7616e-02,\n",
       "                        4.4024e-02, -2.7729e-02, -1.0317e-02, -3.7796e-02, -4.7008e-02,\n",
       "                        8.8955e-03,  2.8411e-02, -2.2222e-02, -1.2804e-02, -1.9293e-02,\n",
       "                       -3.8801e-02, -5.4638e-03,  4.1973e-02,  4.0524e-02,  2.0877e-02,\n",
       "                        3.9699e-02,  2.8171e-03,  3.0409e-02, -1.5150e-02,  3.4404e-02,\n",
       "                       -4.3082e-02,  2.3643e-02,  3.9579e-02, -5.8868e-03,  2.1981e-02,\n",
       "                        4.2018e-03, -3.8375e-02, -4.0560e-02, -4.1435e-02, -2.8046e-02,\n",
       "                       -3.5582e-02,  5.0585e-03,  3.4844e-02,  1.1772e-02,  6.8780e-03,\n",
       "                        3.2101e-02, -3.5739e-02,  2.0435e-02,  3.0500e-02,  1.3868e-02,\n",
       "                       -2.0462e-02, -4.5727e-02,  3.4099e-02,  1.3983e-02,  8.4243e-03,\n",
       "                        9.7337e-03, -4.8357e-02, -1.8392e-02,  5.9200e-04, -4.8428e-02,\n",
       "                        3.5231e-02, -2.2742e-02,  3.0142e-02,  4.8431e-02, -3.0154e-02,\n",
       "                        3.0667e-02,  2.0923e-03, -2.6317e-02,  1.1984e-02, -3.4260e-03,\n",
       "                        4.5161e-02,  4.0368e-02,  5.0879e-02, -1.6109e-02,  4.5792e-02,\n",
       "                        5.2637e-03,  5.0914e-02,  4.5665e-02, -1.0422e-02,  1.6575e-02,\n",
       "                       -4.3784e-03, -3.8268e-02,  2.2318e-02, -3.8514e-02,  1.4868e-03,\n",
       "                        2.9379e-02, -5.0656e-02,  3.6547e-02,  3.6440e-02, -3.9826e-02,\n",
       "                        9.1242e-03, -2.9945e-02, -4.3950e-02,  4.5681e-02,  3.5579e-02,\n",
       "                        2.2269e-02, -4.3789e-02, -3.4018e-02,  3.9640e-02,  1.6309e-02,\n",
       "                       -3.1104e-02, -2.8457e-02,  2.5816e-03, -1.5538e-02,  2.6454e-02,\n",
       "                       -3.7716e-02, -3.5863e-03, -2.9679e-03,  3.1757e-02,  2.7453e-02,\n",
       "                        3.3050e-02,  1.3648e-02,  8.2302e-03,  1.1116e-02, -2.1196e-02,\n",
       "                        2.1105e-02,  4.0708e-02, -2.7324e-02,  1.2237e-02,  2.8075e-02,\n",
       "                       -7.9203e-03, -4.0696e-02, -2.7925e-02, -4.4486e-02,  3.0839e-02,\n",
       "                       -2.9822e-02, -4.9938e-02, -4.1160e-02,  4.3184e-02, -9.7725e-03,\n",
       "                       -1.0792e-03,  3.7982e-02,  2.5053e-02,  4.6608e-02,  4.8736e-02,\n",
       "                        1.2754e-02,  1.6905e-02,  2.6281e-02,  4.9245e-02,  4.5885e-02,\n",
       "                        4.1595e-02,  3.8365e-02, -1.0624e-02,  3.6548e-02,  4.1427e-02,\n",
       "                       -4.4267e-02, -3.0817e-03,  4.7999e-02, -6.7350e-03,  2.5712e-02,\n",
       "                        1.5736e-02,  4.0216e-02,  1.8508e-02, -2.4946e-02,  4.4024e-02,\n",
       "                       -4.9438e-04, -4.9805e-02,  3.7858e-02,  6.3283e-03, -1.8085e-02,\n",
       "                        1.7869e-04,  9.8108e-03, -4.2931e-02,  2.9844e-02,  2.9258e-02,\n",
       "                        3.9556e-02, -1.1993e-02, -2.0558e-02, -2.5061e-02, -8.3407e-03,\n",
       "                       -2.7116e-02, -2.3306e-02, -2.4976e-02,  2.9459e-03, -4.2650e-03,\n",
       "                        2.1681e-02,  1.4219e-02, -2.5398e-03, -4.4409e-02,  1.9675e-02,\n",
       "                       -1.0755e-02,  2.9464e-02, -5.1754e-03,  1.5416e-02, -1.7453e-02,\n",
       "                        3.6204e-02, -3.9279e-02,  3.0968e-02,  4.5361e-02, -3.7277e-02,\n",
       "                        4.9905e-02, -3.2585e-02, -3.8866e-03,  2.5250e-02,  7.8890e-03,\n",
       "                        1.5738e-02, -9.2423e-03, -1.1759e-02,  4.0941e-02,  9.6396e-03,\n",
       "                        4.4932e-02, -7.3471e-03, -7.8430e-04,  4.3704e-02],\n",
       "                      [-1.1693e-02, -9.7208e-03, -3.0511e-02,  4.5841e-02, -3.9478e-02,\n",
       "                       -7.9956e-03,  7.4513e-03,  3.4764e-02,  3.9473e-02,  1.2947e-02,\n",
       "                        2.6874e-02,  2.4149e-02, -4.3650e-02, -2.0181e-02,  4.9714e-04,\n",
       "                        1.7283e-02,  5.7614e-03,  3.0405e-02,  7.8386e-03,  3.6140e-02,\n",
       "                       -2.4362e-02,  2.2477e-02,  1.2242e-02, -2.5835e-02, -8.7186e-03,\n",
       "                        1.8878e-02,  3.4580e-02,  2.7516e-03, -4.1279e-02, -2.8929e-02,\n",
       "                        3.2564e-02,  4.2571e-02,  4.9094e-02,  1.3987e-02, -3.3938e-02,\n",
       "                        1.4052e-02,  3.0825e-02, -8.0448e-05,  4.1730e-02, -3.3528e-02,\n",
       "                        1.9629e-02,  1.6449e-02, -1.5673e-02, -8.0037e-03, -3.8536e-02,\n",
       "                        1.4606e-03,  1.0274e-02, -2.0838e-02, -1.1597e-02, -2.1172e-02,\n",
       "                       -2.0636e-03,  1.0810e-02, -3.4940e-03, -2.7089e-02,  4.5039e-02,\n",
       "                        2.4722e-02, -4.8281e-02,  1.5957e-02, -3.2711e-02,  9.2486e-03,\n",
       "                        7.3671e-03, -2.4701e-02,  2.0872e-02,  2.8219e-02, -5.0117e-02,\n",
       "                       -3.1758e-02, -2.4906e-02, -3.5910e-02,  3.6936e-02,  3.7167e-02,\n",
       "                        1.9821e-02, -4.5568e-02, -4.7614e-02, -1.9062e-02,  3.9270e-02,\n",
       "                        1.0842e-03,  8.6980e-03, -1.4076e-02,  4.3638e-02, -4.2825e-02,\n",
       "                        3.5054e-02,  4.8850e-02, -2.8432e-02, -3.1699e-02,  4.5713e-02,\n",
       "                       -2.5888e-03, -3.9578e-02, -2.7872e-02,  2.6465e-02, -1.6799e-02,\n",
       "                        3.8085e-02, -3.1979e-02,  3.7569e-02,  5.6564e-03,  9.4105e-03,\n",
       "                       -3.4593e-02,  3.5467e-02, -3.7942e-02,  2.2428e-02,  1.5579e-02,\n",
       "                        2.5485e-02,  1.6584e-02,  1.3622e-02, -3.4627e-02, -3.7042e-02,\n",
       "                       -1.1142e-02, -1.5916e-02,  2.9731e-02,  4.3911e-02, -2.8808e-03,\n",
       "                       -3.1842e-03,  4.0889e-02, -4.6171e-02, -2.2840e-02, -1.8942e-02,\n",
       "                       -6.5502e-03,  3.9545e-02, -2.1164e-03, -2.9412e-02, -4.2562e-02,\n",
       "                        2.6974e-02, -1.3552e-02, -2.3959e-02,  6.4517e-03, -2.2405e-03,\n",
       "                       -4.8293e-02,  1.6326e-02, -1.3671e-02,  3.5470e-02, -3.0953e-02,\n",
       "                       -2.4056e-02,  1.9555e-02,  1.2967e-02,  3.0641e-02,  1.7246e-03,\n",
       "                        6.4759e-03, -9.1437e-03, -1.3473e-02,  1.6847e-02, -3.6770e-02,\n",
       "                       -1.6313e-02,  4.3452e-02,  3.2156e-02,  4.7806e-02,  1.8110e-02,\n",
       "                       -4.6977e-02,  2.8146e-02, -4.9598e-02,  2.3677e-02, -3.7388e-02,\n",
       "                       -2.1510e-02, -4.1811e-02, -1.9107e-02,  3.6919e-04,  3.5183e-02,\n",
       "                       -4.3597e-02,  3.6062e-02,  5.0682e-02,  1.9055e-02, -1.1454e-03,\n",
       "                        1.9765e-02, -3.1218e-02,  3.4946e-02, -1.2044e-02,  6.8383e-03,\n",
       "                       -1.3702e-02,  2.0674e-02, -1.3885e-02, -8.3531e-03, -4.1638e-02,\n",
       "                        1.4773e-02,  1.2988e-02, -1.5779e-02, -2.2318e-02,  3.5957e-02,\n",
       "                        2.1165e-02,  2.7897e-02, -4.8377e-02,  2.8218e-02, -3.9626e-02,\n",
       "                        3.1146e-02, -1.1826e-02, -2.5782e-02, -4.9328e-02,  2.6523e-02,\n",
       "                        1.4327e-02, -1.4249e-02, -3.0084e-02,  6.8091e-05,  2.7720e-02,\n",
       "                       -1.9676e-02,  3.5519e-03, -1.6154e-02,  3.0436e-02, -1.3269e-02,\n",
       "                        5.7685e-03, -1.0227e-02, -2.5300e-02,  5.0474e-02,  2.1748e-02,\n",
       "                        3.3359e-02,  1.6208e-02, -2.9288e-02, -2.8917e-02, -4.6367e-02,\n",
       "                       -2.9775e-02,  4.8589e-02, -2.0822e-03, -2.5343e-02,  1.4637e-03,\n",
       "                       -2.1003e-02, -1.3097e-02, -4.1215e-02, -2.4973e-02,  4.3831e-02,\n",
       "                        3.7186e-02, -3.9173e-02,  3.4090e-02, -3.5934e-05,  4.0797e-02,\n",
       "                       -3.9837e-02,  3.3965e-02,  1.8535e-02, -7.3402e-03,  1.0050e-02,\n",
       "                       -2.8140e-02, -9.7438e-03, -2.6342e-02, -3.6385e-02,  1.2278e-02,\n",
       "                       -9.4713e-04, -3.1789e-02, -4.7804e-02, -4.4939e-03, -5.7490e-03,\n",
       "                       -4.1682e-02, -3.0094e-02,  3.0033e-02,  2.8919e-02,  4.9858e-02,\n",
       "                       -2.3987e-03, -7.6555e-03, -3.0036e-02,  3.4443e-02, -2.0336e-02,\n",
       "                        3.3322e-02,  2.8255e-03, -2.6466e-02,  4.1903e-02,  3.0731e-02,\n",
       "                       -3.7834e-02,  4.5493e-02,  4.0855e-02, -1.9029e-02, -2.9976e-02,\n",
       "                        2.4491e-02,  4.3191e-03,  3.1296e-02, -6.9252e-03, -1.2180e-02,\n",
       "                        3.5595e-02,  4.1982e-02, -1.3981e-02, -9.1517e-03,  2.7693e-03,\n",
       "                        3.5943e-02, -2.0369e-02, -1.7213e-02, -4.4005e-02,  3.0481e-03,\n",
       "                       -9.2779e-04, -3.9222e-02, -9.7760e-03, -2.5766e-02, -1.9166e-02,\n",
       "                       -3.2170e-02,  2.4310e-02, -1.1815e-02, -4.9566e-02,  7.4108e-04,\n",
       "                       -2.8001e-02,  2.5302e-02,  8.6906e-03, -1.7059e-02,  4.6847e-02,\n",
       "                        3.2651e-02,  4.3430e-03,  2.1479e-02,  3.7117e-02,  2.9823e-02,\n",
       "                        2.7934e-02,  2.5833e-03, -8.3076e-03,  2.7114e-02,  5.0909e-02,\n",
       "                       -5.6930e-03, -3.3754e-02, -1.4994e-02,  9.1483e-03, -1.1060e-02,\n",
       "                       -4.5807e-02,  1.7050e-02, -1.7107e-04,  4.8798e-02, -4.8673e-02,\n",
       "                       -4.0890e-02, -4.3956e-02, -4.1736e-02,  3.8100e-02, -1.5067e-02,\n",
       "                        3.1324e-02, -3.7968e-02,  2.2646e-02,  1.4170e-03,  4.8319e-02,\n",
       "                       -3.9933e-02, -3.0918e-02,  4.9422e-02, -1.7374e-02, -1.6579e-02,\n",
       "                        2.6226e-02, -9.4927e-03,  9.7931e-03, -3.2959e-02,  4.8838e-02,\n",
       "                       -7.8716e-03, -2.7361e-02, -4.0145e-02, -1.8868e-02,  2.2712e-02,\n",
       "                       -4.4280e-02,  2.8527e-02, -3.9791e-03,  3.3665e-02, -6.5862e-03,\n",
       "                        9.5674e-03,  4.5982e-02, -2.6507e-02,  2.8964e-02, -1.0968e-02,\n",
       "                        2.4550e-02,  3.4747e-03,  3.3912e-02,  3.8866e-02, -1.7645e-02,\n",
       "                        6.5210e-03, -3.4244e-02, -3.6214e-02,  2.9818e-03,  3.3548e-02,\n",
       "                       -1.1693e-03,  2.0400e-02,  4.4523e-02,  4.0464e-02,  1.5222e-02,\n",
       "                       -4.5865e-02, -4.4598e-02, -2.8755e-02,  4.5944e-03, -2.0974e-02,\n",
       "                        4.2499e-02,  6.4008e-04, -2.1580e-02, -2.4426e-02, -2.3062e-02,\n",
       "                        1.2886e-02,  1.1441e-02, -2.6819e-02, -2.6574e-02,  4.8933e-02,\n",
       "                       -3.9306e-04,  9.8062e-03, -1.4597e-02, -3.1849e-02, -3.5094e-02,\n",
       "                        1.0983e-02,  2.2619e-02,  2.6307e-02,  3.6573e-03,  1.6418e-02,\n",
       "                       -4.4540e-03,  4.5695e-02, -3.9160e-02,  3.8031e-02]])),\n",
       "             ('head.bias', tensor([-0.0491, -0.0081]))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema_model = ema.ema_model\n",
    "ema_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "# your neural network as a pytorch module\n",
    "\n",
    "net = torch.nn.Linear(512, 512)\n",
    "\n",
    "# wrap your neural network, specify the decay (beta)\n",
    "\n",
    "ema = EMA(\n",
    "    net,\n",
    "    beta = 0.9999,              # exponential moving average factor\n",
    "    update_after_step = 1,    # only after this number of .update() calls will it start updating\n",
    "    update_every = 10,          # how often to actually update, to save on compute (updates every 10th .update() call)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ema.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ema.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ•°æ®é¢„å¤„ç†\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# åŠ è½½è®­ç»ƒé›†\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# åŠ è½½æµ‹è¯•é›†\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# ç±»åˆ«\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', \n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„å·ç§¯ç¥žç»ç½‘ç»œ\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.to('cuda')\n",
    "\n",
    "# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.115\n",
      "tensor([[-1.1270, -0.9944,  0.4766,  0.9668,  0.7682,  0.9278,  0.3272,  0.9337,\n",
      "         -1.6743, -0.7875],\n",
      "        [-0.9816, -0.7856,  0.2214,  1.4703, -0.5245,  2.3286, -1.0866,  0.5096,\n",
      "         -1.0007, -1.8022],\n",
      "        [ 1.7981,  1.2103, -0.0755, -0.8556, -1.3757, -1.3051, -2.9969, -0.9202,\n",
      "          2.4162,  1.2604],\n",
      "        [-0.6375,  1.0634, -0.3466,  0.1160, -0.8362, -0.0192, -0.6238, -0.1938,\n",
      "          0.3141,  0.6908]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([7, 5, 0, 8], device='cuda:0') original outputs\n",
      "tensor([[-0.6143, -0.3419,  0.2176,  0.4729,  0.4133,  0.3112,  0.3957,  0.3465,\n",
      "         -0.7501, -0.4329],\n",
      "        [ 0.0059, -0.0371,  0.0498,  0.2252, -0.4461,  0.6110, -0.5353,  0.0489,\n",
      "         -0.0685, -0.6344],\n",
      "        [ 0.7811,  0.2420,  0.0217, -0.3846, -0.3281, -0.2207, -0.9067, -0.2976,\n",
      "          0.6728,  0.2047],\n",
      "        [-0.4445,  0.0251,  0.1067,  0.2231, -0.0158,  0.2486,  0.1590,  0.1596,\n",
      "         -0.3575, -0.1517]], device='cuda:0') tensor([7, 5, 0, 8], device='cuda:0') ema outputs no eval\n",
      "tensor([[-0.6143, -0.3419,  0.2176,  0.4729,  0.4133,  0.3112,  0.3957,  0.3465,\n",
      "         -0.7501, -0.4329],\n",
      "        [ 0.0059, -0.0371,  0.0498,  0.2252, -0.4461,  0.6110, -0.5353,  0.0489,\n",
      "         -0.0685, -0.6344],\n",
      "        [ 0.7811,  0.2420,  0.0217, -0.3846, -0.3281, -0.2207, -0.9067, -0.2976,\n",
      "          0.6728,  0.2047],\n",
      "        [-0.4445,  0.0251,  0.1067,  0.2231, -0.0158,  0.2486,  0.1590,  0.1596,\n",
      "         -0.3575, -0.1517]], device='cuda:0') tensor([7, 5, 0, 8], device='cuda:0') ema outputs eval\n",
      "[1,  4000] loss: 1.805\n",
      "tensor([[ 0.3748,  0.7043, -0.4185, -0.3182, -0.5603, -1.1013, -0.6547, -0.4163,\n",
      "          0.6767,  1.6867],\n",
      "        [ 1.3670,  2.8465, -0.6668, -0.9913, -1.1109, -1.8502, -1.8595, -1.6997,\n",
      "          1.2136,  1.5365],\n",
      "        [-1.4309, -2.9893,  2.1259,  1.3348,  2.5610,  1.3689,  1.1232,  2.0911,\n",
      "         -3.3399, -3.1203],\n",
      "        [-0.9005,  0.1693,  0.2269,  1.0247, -0.2515,  0.1227,  0.9178, -0.3973,\n",
      "         -0.9007,  0.1763]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([0, 1, 5, 5], device='cuda:0') original outputs\n",
      "tensor([[ 2.4621e-01,  5.8604e-01, -2.1298e-01, -4.3301e-01, -2.9146e-01,\n",
      "         -7.7885e-01, -6.2341e-01, -8.4675e-04,  5.3377e-01,  1.0471e+00],\n",
      "        [-4.0325e-01,  4.0550e-01, -9.1100e-02,  5.3307e-01, -4.2969e-01,\n",
      "          1.9315e-01, -2.2740e-01, -6.2283e-02, -2.9599e-01, -9.0795e-02],\n",
      "        [-1.0667e+00, -1.1912e+00,  8.9360e-01,  5.6186e-01,  1.2198e+00,\n",
      "          6.5285e-01,  9.6625e-01,  7.8549e-01, -1.7679e+00, -1.0914e+00],\n",
      "        [-7.8981e-01,  3.3232e-01,  2.4901e-01,  3.8134e-01,  6.2067e-02,\n",
      "          2.2659e-02,  6.8097e-01, -2.0519e-01, -6.6855e-01,  1.9954e-01]],\n",
      "       device='cuda:0') tensor([0, 1, 5, 5], device='cuda:0') ema outputs no eval\n",
      "tensor([[ 2.4621e-01,  5.8604e-01, -2.1298e-01, -4.3301e-01, -2.9146e-01,\n",
      "         -7.7885e-01, -6.2341e-01, -8.4675e-04,  5.3377e-01,  1.0471e+00],\n",
      "        [-4.0325e-01,  4.0550e-01, -9.1100e-02,  5.3307e-01, -4.2969e-01,\n",
      "          1.9315e-01, -2.2740e-01, -6.2283e-02, -2.9599e-01, -9.0795e-02],\n",
      "        [-1.0667e+00, -1.1912e+00,  8.9360e-01,  5.6186e-01,  1.2198e+00,\n",
      "          6.5285e-01,  9.6625e-01,  7.8549e-01, -1.7679e+00, -1.0914e+00],\n",
      "        [-7.8981e-01,  3.3232e-01,  2.4901e-01,  3.8134e-01,  6.2067e-02,\n",
      "          2.2659e-02,  6.8097e-01, -2.0519e-01, -6.6855e-01,  1.9954e-01]],\n",
      "       device='cuda:0') tensor([0, 1, 5, 5], device='cuda:0') ema outputs eval\n",
      "[1,  6000] loss: 1.630\n",
      "tensor([[ 1.6163, -2.6899,  0.8062,  1.1058, -0.5593,  1.1993, -1.6923, -0.2831,\n",
      "         -0.0934, -0.9906],\n",
      "        [ 2.7682,  0.1888,  0.0617, -2.2499, -0.2347, -2.4634, -3.5380,  2.2370,\n",
      "          0.5462,  3.2770],\n",
      "        [ 0.9700,  0.3704, -1.4079,  0.0039, -2.3260, -0.6279, -1.9296, -0.1503,\n",
      "          2.2852,  2.8145],\n",
      "        [-0.3115, -0.1758, -0.5644,  0.3390, -0.7126,  0.0426, -0.3058,  0.9538,\n",
      "         -0.6713,  1.6598]], device='cuda:0', grad_fn=<AddmmBackward0>) tensor([5, 2, 8, 9], device='cuda:0') original outputs\n",
      "tensor([[ 0.8301, -1.2613,  0.7820,  0.8999, -0.2945,  0.5553, -0.8256, -0.5700,\n",
      "         -0.0834, -1.2720],\n",
      "        [ 2.1089, -0.3848,  0.3336, -1.5186,  0.2481, -1.7281, -2.7094,  1.1734,\n",
      "          1.5202,  1.5418],\n",
      "        [ 0.9753,  1.2578, -0.8056, -0.7092, -1.0928, -0.7762, -1.8406, -0.6905,\n",
      "          1.9782,  1.3557],\n",
      "        [-0.5894, -0.2349, -0.0164,  0.2332,  0.0836,  0.1909,  0.1945,  0.5088,\n",
      "         -0.5765,  0.4055]], device='cuda:0') tensor([5, 2, 8, 9], device='cuda:0') ema outputs no eval\n",
      "tensor([[ 0.8301, -1.2613,  0.7820,  0.8999, -0.2945,  0.5553, -0.8256, -0.5700,\n",
      "         -0.0834, -1.2720],\n",
      "        [ 2.1089, -0.3848,  0.3336, -1.5186,  0.2481, -1.7281, -2.7094,  1.1734,\n",
      "          1.5202,  1.5418],\n",
      "        [ 0.9753,  1.2578, -0.8056, -0.7092, -1.0928, -0.7762, -1.8406, -0.6905,\n",
      "          1.9782,  1.3557],\n",
      "        [-0.5894, -0.2349, -0.0164,  0.2332,  0.0836,  0.1909,  0.1945,  0.5088,\n",
      "         -0.5765,  0.4055]], device='cuda:0') tensor([5, 2, 8, 9], device='cuda:0') ema outputs eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 22\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m eam\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# æ‰“å°ç»Ÿè®¡ä¿¡æ¯\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\optim\\sgd.py:151\u001b[0m, in \u001b[0;36mSGD.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m             momentum_buffer_list\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmomentum_buffer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 151\u001b[0m \u001b[43msgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdampening\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnesterov\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# update momentum_buffers in state\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, momentum_buffer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_with_grad, momentum_buffer_list):\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\optim\\sgd.py:202\u001b[0m, in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_sgd\n\u001b[1;32m--> 202\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m     \u001b[49m\u001b[43md_p_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdampening\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdampening\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m     \u001b[49m\u001b[43mnesterov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnesterov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_sparse_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\optim\\sgd.py:238\u001b[0m, in \u001b[0;36m_single_tensor_sgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[0;32m    236\u001b[0m     momentum_buffer_list[i] \u001b[38;5;241m=\u001b[39m buf\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     \u001b[43mbuf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(d_p, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dampening)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov:\n\u001b[0;32m    241\u001b[0m     d_p \u001b[38;5;241m=\u001b[39m d_p\u001b[38;5;241m.\u001b[39madd(buf, alpha\u001b[38;5;241m=\u001b[39mmomentum)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eam = EMA(\n",
    "    net,\n",
    "    beta = 0.9999,              # exponential moving average factor\n",
    "    update_after_step = 1,    # only after this number of .update() calls will it start updating\n",
    "    update_every = 10,          # how often to actually update, to save on compute (updates every 10th .update() call)\n",
    ")\n",
    "\n",
    "# è®­ç»ƒç½‘ç»œ\n",
    "for epoch in range(2):  # å¤šæ¬¡å¾ªçŽ¯éåŽ†æ•°æ®é›†\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # èŽ·å–è¾“å…¥æ•°æ®\n",
    "        inputs, labels = data[0].to('cuda'), data[1].to('cuda')\n",
    "\n",
    "        # æ¢¯åº¦æ¸…é›¶\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # å‰å‘ + åå‘ + ä¼˜åŒ–\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        eam.update()\n",
    "\n",
    "\n",
    "        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # æ¯2000ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            print(outputs,labels,'original outputs')\n",
    "            eam_model = eam.ema_model\n",
    "            eam_output = eam_model(inputs)\n",
    "            print(eam_output,labels,'ema outputs no eval')\n",
    "            eam_model.eval()\n",
    "            eam_output = eam_model(inputs)\n",
    "            print(eam_output,labels,'ema outputs eval')\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 2.4853e-01,  1.4136e-01,  8.8376e-02,  6.4345e-02,  1.0191e-01],\n",
       "                        [ 2.1640e-01,  1.2899e-01,  1.9218e-01,  2.2545e-01,  1.9552e-01],\n",
       "                        [ 1.1442e-01,  2.1091e-01,  1.2708e-01,  7.5423e-02,  1.7802e-01],\n",
       "                        [ 3.4655e-02, -3.0641e-03, -2.8502e-02, -7.6196e-03, -3.0069e-02],\n",
       "                        [-3.3238e-02,  3.6624e-02,  1.2331e-01, -7.1538e-02,  1.2122e-01]],\n",
       "              \n",
       "                       [[-5.6797e-02, -1.3049e-01,  4.7797e-03,  1.7290e-02, -9.6059e-02],\n",
       "                        [-1.7487e-01, -1.8724e-01, -5.5656e-02, -1.1662e-01,  3.9790e-02],\n",
       "                        [-2.4075e-02, -1.0720e-01,  3.6000e-02, -9.7040e-02, -1.1319e-01],\n",
       "                        [ 5.8036e-03, -1.0937e-01, -4.7860e-02,  4.9619e-02, -1.0796e-01],\n",
       "                        [-6.5905e-02, -1.2742e-01, -4.9430e-02, -5.8437e-02, -1.0034e-01]],\n",
       "              \n",
       "                       [[-2.0091e-01,  1.2914e-02, -2.2845e-02, -1.5160e-01,  1.6410e-02],\n",
       "                        [-1.5453e-01, -9.2637e-02, -7.0168e-02, -1.2037e-01, -6.7707e-02],\n",
       "                        [-1.1135e-01, -5.4181e-02, -3.9595e-02, -7.5960e-02, -1.1188e-01],\n",
       "                        [-1.0890e-01, -2.8207e-02,  2.7402e-02,  3.3073e-02,  1.4088e-01],\n",
       "                        [ 8.2276e-02, -1.7392e-02,  1.1349e-01,  9.3062e-02,  1.6589e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-6.7741e-02, -2.1070e-03, -1.5322e-02, -2.2943e-02, -8.2735e-03],\n",
       "                        [-4.5422e-04, -2.0570e-01,  5.4038e-02,  1.0626e-01,  6.6961e-02],\n",
       "                        [-2.4626e-01, -8.9318e-02,  3.3848e-02,  1.8925e-01,  8.4539e-02],\n",
       "                        [-2.1935e-01, -8.1486e-02,  1.9662e-01,  2.3716e-01,  1.5665e-02],\n",
       "                        [-6.1026e-02,  6.3163e-02,  2.5543e-02,  2.9314e-01,  1.5050e-01]],\n",
       "              \n",
       "                       [[-7.0427e-02, -6.0610e-02,  3.6044e-03,  8.8337e-02,  1.2918e-01],\n",
       "                        [-1.1550e-02, -7.5094e-02,  4.0016e-02,  1.7072e-01,  1.6133e-01],\n",
       "                        [-2.2537e-01, -3.3230e-02,  2.1072e-01,  2.0150e-01,  3.9085e-03],\n",
       "                        [-7.6495e-02, -1.7916e-01,  2.6368e-01,  2.4701e-01,  1.2625e-01],\n",
       "                        [ 4.7448e-02,  6.1581e-02,  5.9312e-02,  2.1549e-01,  3.1437e-01]],\n",
       "              \n",
       "                       [[-8.6678e-02, -5.7395e-02, -5.9653e-02, -1.4233e-01, -5.2888e-02],\n",
       "                        [-2.2553e-01, -2.7422e-01, -9.6072e-02,  5.2720e-02, -6.2285e-02],\n",
       "                        [-4.3981e-01, -2.9545e-01,  1.2633e-01,  1.5974e-01,  5.9814e-02],\n",
       "                        [-3.7212e-01, -1.3200e-01, -6.4177e-02,  1.8760e-01,  3.1927e-02],\n",
       "                        [-1.8442e-01, -1.5270e-01, -8.2634e-02,  9.9337e-02,  2.5464e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9106e-01, -2.2478e-01, -2.3133e-01, -2.5169e-01, -2.6664e-01],\n",
       "                        [-1.9814e-01, -1.7953e-01, -2.5778e-01, -1.8920e-01, -1.2472e-01],\n",
       "                        [-2.3167e-02, -1.0367e-02,  4.5334e-02,  2.3127e-02, -1.8467e-01],\n",
       "                        [-9.2125e-02,  6.0207e-02,  2.9776e-02, -4.1112e-02, -8.1102e-02],\n",
       "                        [ 3.3669e-02,  3.1935e-02,  1.3580e-01,  1.0731e-01,  1.9449e-01]],\n",
       "              \n",
       "                       [[-3.6195e-02, -1.3964e-01, -1.7086e-01,  2.6815e-02, -1.4532e-01],\n",
       "                        [-9.2487e-02, -9.7965e-02, -1.6527e-01, -5.7930e-03, -3.7511e-02],\n",
       "                        [-1.0660e-01, -1.0141e-01, -1.7975e-02, -6.7180e-02,  9.5686e-02],\n",
       "                        [ 1.3561e-01, -4.0017e-02,  8.4585e-03,  6.4513e-02,  1.6100e-01],\n",
       "                        [-1.3291e-02,  1.5055e-01,  7.9785e-02,  1.3777e-01,  1.1141e-01]],\n",
       "              \n",
       "                       [[-7.5869e-02, -3.9549e-02,  1.1261e-02, -3.8504e-02, -5.6192e-02],\n",
       "                        [ 7.7013e-02,  8.9458e-02, -3.6365e-02, -2.9261e-02,  3.4719e-03],\n",
       "                        [-7.1140e-03,  9.1347e-02,  1.9444e-02,  3.3870e-02, -3.5872e-02],\n",
       "                        [ 1.1921e-01,  4.4153e-02,  2.2377e-01,  1.6381e-01,  2.1105e-01],\n",
       "                        [ 4.2600e-02,  1.8967e-01,  1.8066e-01,  1.6486e-01,  1.9971e-01]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.9708e-02, -1.1649e-01, -6.3350e-03,  6.1657e-02, -8.2703e-02],\n",
       "                        [-7.3170e-02, -4.0620e-02, -1.4335e-01, -6.9173e-02,  3.3504e-02],\n",
       "                        [-1.0994e-01, -1.1497e-01, -5.1951e-02,  3.4859e-02, -6.1152e-02],\n",
       "                        [-1.6263e-01,  1.2158e-02, -2.2093e-02, -7.9069e-02,  3.1284e-02],\n",
       "                        [-2.1512e-02,  1.8443e-02,  6.4862e-02,  1.2808e-01,  7.3909e-02]],\n",
       "              \n",
       "                       [[-6.4121e-03, -8.5728e-03,  2.0640e-02, -9.1643e-02, -5.4362e-02],\n",
       "                        [-1.2095e-01,  1.4316e-03, -8.2774e-02, -1.1366e-02, -3.2819e-02],\n",
       "                        [-1.1786e-01, -1.6475e-01, -1.8905e-02,  7.5082e-02, -1.2839e-01],\n",
       "                        [-5.1713e-02,  3.0312e-02,  4.6817e-02, -4.9898e-02,  7.6214e-02],\n",
       "                        [-4.3866e-02, -1.5050e-02,  2.8894e-02, -1.3996e-02,  9.2295e-02]],\n",
       "              \n",
       "                       [[ 2.3712e-03,  1.1292e-01,  1.6132e-01,  1.7494e-01,  1.8165e-01],\n",
       "                        [ 1.5140e-01,  1.3888e-01,  2.1243e-01,  9.6985e-02,  2.2486e-01],\n",
       "                        [ 7.7690e-02,  5.4401e-02,  1.4861e-01,  1.5992e-01,  2.0064e-01],\n",
       "                        [ 6.8959e-02,  1.1991e-01,  1.9691e-01,  1.3750e-01,  1.6509e-01],\n",
       "                        [-5.3996e-03,  2.7583e-02,  1.1998e-01,  2.1019e-01,  1.9372e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 1.8276e-01,  1.2131e-01,  1.7456e-01,  7.1361e-03, -3.2895e-03],\n",
       "                        [-7.0218e-03, -1.4209e-02,  7.9407e-02, -7.1346e-02, -3.6729e-02],\n",
       "                        [ 2.7870e-02, -1.0479e-01, -1.2542e-01, -1.5829e-01,  9.9300e-02],\n",
       "                        [-3.0404e-02,  6.3772e-02, -1.6954e-01, -2.1887e-01, -1.5161e-01],\n",
       "                        [ 1.0698e-01,  2.1453e-02, -1.2215e-01, -3.3891e-02, -1.3084e-01]],\n",
       "              \n",
       "                       [[-9.4650e-02, -1.2770e-01, -1.2901e-01, -1.7339e-01, -2.2199e-01],\n",
       "                        [-1.1405e-01, -7.0351e-02, -5.0257e-02, -9.7161e-02, -1.5795e-01],\n",
       "                        [-2.7595e-02, -9.1149e-02, -2.4214e-01, -7.9546e-02, -1.1339e-02],\n",
       "                        [ 2.4184e-02, -7.3344e-02,  1.9479e-03, -1.6095e-01, -2.7891e-02],\n",
       "                        [ 1.9177e-01,  6.3955e-02,  1.1937e-02, -9.5034e-02,  1.3230e-01]],\n",
       "              \n",
       "                       [[ 9.5960e-02, -4.9659e-02,  8.5978e-02, -5.7611e-02, -1.0447e-01],\n",
       "                        [ 1.7238e-01, -1.0986e-01, -1.1374e-01,  2.0957e-03,  6.2972e-02],\n",
       "                        [ 2.0920e-01, -9.5236e-02, -1.4778e-02, -1.0234e-01,  8.3278e-02],\n",
       "                        [ 9.6506e-02, -2.8396e-02, -7.0817e-02, -9.3497e-03, -1.7425e-02],\n",
       "                        [ 2.5006e-01,  2.3109e-01, -4.2998e-02, -5.5505e-02, -8.8335e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-4.7981e-02,  5.1146e-02,  1.5782e-01,  3.8062e-02, -2.0256e-02],\n",
       "                        [-6.2862e-02, -2.3019e-01, -1.7708e-01, -1.3125e-01, -1.7621e-01],\n",
       "                        [-1.0737e-01, -1.5189e-01, -2.0577e-01, -2.5794e-01, -2.3467e-01],\n",
       "                        [-1.2438e-01,  7.2512e-02, -4.0532e-03,  1.5072e-01,  8.4722e-03],\n",
       "                        [ 2.4768e-01,  1.5873e-01,  2.8493e-01,  3.6365e-01,  3.2087e-01]],\n",
       "              \n",
       "                       [[ 9.8709e-02,  2.3737e-01,  3.0853e-01,  1.4651e-01,  2.0287e-01],\n",
       "                        [-5.1682e-02, -4.9846e-02,  5.4922e-02, -9.4516e-02, -1.5429e-02],\n",
       "                        [-8.0054e-02, -2.1281e-01, -2.9223e-01, -2.0156e-01, -7.3463e-02],\n",
       "                        [-8.8151e-02,  2.9805e-04, -9.9712e-02, -4.1399e-02, -6.2069e-02],\n",
       "                        [ 5.5118e-02,  1.1867e-01,  2.4506e-01,  5.1921e-02,  1.9662e-01]],\n",
       "              \n",
       "                       [[ 1.9404e-01,  1.9708e-01,  2.3317e-01,  2.1530e-01,  1.9697e-01],\n",
       "                        [-1.4924e-02,  9.9221e-02,  4.7297e-02,  1.3879e-02, -9.8444e-05],\n",
       "                        [-1.3780e-01, -1.5718e-01, -2.1605e-01, -1.2029e-01, -1.4783e-01],\n",
       "                        [ 8.1898e-05, -1.7059e-02, -1.3533e-01, -1.3656e-01,  6.0794e-03],\n",
       "                        [-1.0462e-01,  7.2264e-02,  7.1663e-02,  1.5861e-01,  1.0302e-01]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.4378,  0.2656, -0.1286,  0.4061, -0.0948, -0.2052], device='cuda:0')),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-3.9591e-02, -3.4232e-02, -3.4935e-02, -6.6773e-02, -5.2440e-02],\n",
       "                        [ 3.9722e-02, -2.4565e-02, -4.4690e-02,  3.0432e-02, -9.2245e-02],\n",
       "                        [-7.6476e-02,  3.7545e-02,  4.6113e-03,  6.3319e-02,  5.0970e-02],\n",
       "                        [ 6.4330e-02, -3.7798e-02, -1.0256e-02, -2.7293e-02, -3.9857e-02],\n",
       "                        [-4.3690e-02, -7.9479e-02, -1.9759e-02,  5.2697e-02, -3.6251e-02]],\n",
       "              \n",
       "                       [[-8.6571e-02, -4.0963e-02, -5.2234e-02, -7.5158e-02, -8.2695e-02],\n",
       "                        [-3.9032e-03,  1.6205e-02, -6.2574e-02, -3.5327e-02, -6.7022e-02],\n",
       "                        [ 4.8291e-02, -2.6510e-03,  5.7193e-02,  7.6133e-03, -5.6075e-02],\n",
       "                        [ 1.0159e-01,  1.5932e-02, -1.7846e-02,  7.8556e-02, -6.6073e-02],\n",
       "                        [ 1.0469e-02, -5.3924e-02, -4.9798e-02, -2.0231e-02,  4.1251e-02]],\n",
       "              \n",
       "                       [[ 2.9983e-02, -2.1101e-02, -9.4062e-02, -7.4336e-02, -1.7055e-02],\n",
       "                        [-6.3864e-02,  5.3981e-02, -4.5227e-02,  4.0088e-02, -5.7441e-02],\n",
       "                        [ 7.0795e-02,  1.1792e-02,  6.9995e-02,  4.4223e-02,  3.0065e-02],\n",
       "                        [ 8.0683e-02,  5.6083e-03,  7.2916e-02, -4.6517e-02, -3.1827e-02],\n",
       "                        [-6.7775e-02, -7.5731e-02,  1.3436e-02, -3.9909e-02, -1.1232e-02]],\n",
       "              \n",
       "                       [[ 4.0056e-03,  1.1573e-01,  1.2362e-01,  8.4373e-02,  1.3748e-01],\n",
       "                        [ 3.8094e-02, -1.5083e-02,  8.6609e-02,  3.8785e-02, -3.0397e-02],\n",
       "                        [-6.4346e-02, -8.7897e-02,  1.8635e-04, -2.4913e-02,  5.0265e-02],\n",
       "                        [ 3.5926e-02,  2.5532e-02,  3.2287e-02,  3.5442e-02, -6.5435e-02],\n",
       "                        [-8.5703e-02, -6.5099e-02, -8.5705e-02, -8.0260e-02, -4.1659e-02]],\n",
       "              \n",
       "                       [[-4.9696e-02,  1.7170e-02,  4.4427e-02, -3.6147e-02,  2.8050e-02],\n",
       "                        [-7.9947e-02,  3.3705e-02, -6.6211e-02, -4.9968e-02, -2.3996e-02],\n",
       "                        [-5.6040e-02,  3.3170e-02,  7.2586e-02,  2.4452e-02,  4.4432e-02],\n",
       "                        [ 6.4489e-03,  4.0541e-02,  8.0527e-02,  6.7915e-02,  7.7436e-02],\n",
       "                        [-8.0245e-02, -1.3680e-01, -6.4935e-02, -2.4389e-03, -4.9497e-03]],\n",
       "              \n",
       "                       [[-3.3169e-02, -5.4489e-02, -6.0236e-02,  3.2016e-02, -7.5150e-02],\n",
       "                        [ 2.5206e-02,  4.9961e-03,  4.3527e-02,  5.2809e-02, -7.9455e-02],\n",
       "                        [ 3.4720e-02,  1.0719e-01, -2.0154e-02,  3.7399e-02, -6.9450e-02],\n",
       "                        [ 7.1104e-02,  5.8287e-02,  1.0821e-01,  7.9730e-02,  9.1446e-02],\n",
       "                        [ 3.2306e-03,  6.2431e-02,  4.3495e-02, -6.4620e-02, -3.1206e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.1079e-02,  4.4524e-02,  5.0039e-02,  8.7808e-02,  7.6757e-02],\n",
       "                        [ 3.2258e-02, -4.4342e-02,  3.0648e-02,  6.5512e-03,  8.3621e-02],\n",
       "                        [-4.5529e-03,  9.3578e-02, -4.0945e-02, -5.6969e-02,  6.8772e-02],\n",
       "                        [-4.7963e-02,  8.3299e-02,  2.0098e-02, -2.6772e-02, -4.0268e-02],\n",
       "                        [ 7.1578e-02, -6.2511e-02,  6.9807e-02, -4.2518e-03,  9.9039e-02]],\n",
       "              \n",
       "                       [[ 1.9080e-02,  3.2745e-02, -1.0067e-02, -8.3006e-03, -9.5188e-02],\n",
       "                        [-5.2441e-02,  3.6303e-03,  4.6038e-02,  3.6748e-03, -1.3185e-02],\n",
       "                        [-4.2995e-02,  3.5747e-02, -8.9884e-02, -5.5186e-02, -1.3976e-02],\n",
       "                        [ 2.9354e-02, -9.8744e-02,  2.3184e-02,  8.4402e-03,  9.6870e-02],\n",
       "                        [-6.4740e-02, -1.3208e-01, -1.0311e-01,  6.2757e-02,  9.2304e-02]],\n",
       "              \n",
       "                       [[-1.4062e-02,  2.3226e-04, -8.9149e-02, -8.9873e-02, -9.1391e-02],\n",
       "                        [ 5.9848e-02, -5.2789e-02,  4.5670e-03, -3.6780e-02,  3.5161e-04],\n",
       "                        [ 1.0953e-01, -2.6691e-02, -7.9893e-02, -7.6813e-02, -9.0747e-02],\n",
       "                        [ 8.9091e-03, -5.3251e-02, -5.4965e-02, -7.4305e-02,  7.3220e-02],\n",
       "                        [-8.1788e-02, -5.5450e-02,  3.9070e-02,  5.6516e-02,  7.4698e-04]],\n",
       "              \n",
       "                       [[ 1.2491e-01,  3.2311e-02,  3.1480e-02, -7.8404e-02, -4.9063e-02],\n",
       "                        [ 9.5978e-02,  4.2626e-02, -5.5531e-02, -5.9609e-02, -1.0544e-01],\n",
       "                        [ 3.9797e-02,  9.7341e-02,  4.1507e-02, -3.0795e-02, -8.9614e-02],\n",
       "                        [ 1.0018e-01,  6.3521e-02,  4.0777e-02, -8.1969e-02,  3.8073e-02],\n",
       "                        [ 1.1370e-01, -3.8434e-02, -2.1293e-02,  6.4110e-02,  8.4137e-02]],\n",
       "              \n",
       "                       [[ 5.4783e-02,  7.8839e-02, -7.0871e-02, -1.9978e-02, -1.3201e-03],\n",
       "                        [-1.7050e-02,  4.4287e-02, -3.4287e-02, -9.6519e-02, -2.3925e-02],\n",
       "                        [ 4.8583e-02, -6.5753e-02, -1.5666e-02, -9.6832e-04, -3.1487e-02],\n",
       "                        [-6.0369e-02,  3.2484e-02, -5.9227e-02, -5.1958e-02,  9.3157e-03],\n",
       "                        [-2.0725e-02,  2.0076e-02,  2.4794e-02,  7.2151e-02, -4.5115e-02]],\n",
       "              \n",
       "                       [[-5.1489e-02, -7.9071e-02, -1.5226e-02, -3.3219e-03,  9.3451e-03],\n",
       "                        [ 7.1204e-02, -4.4921e-02, -1.5437e-02, -8.1073e-02, -5.6938e-02],\n",
       "                        [-2.0194e-02,  3.8732e-02, -1.1947e-02, -5.9788e-02,  4.9221e-02],\n",
       "                        [ 4.1413e-02,  5.3806e-02, -7.1168e-02,  1.9847e-02,  3.8977e-03],\n",
       "                        [ 3.9322e-02, -6.5738e-02, -4.2342e-02, -2.8109e-02,  2.2560e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[-2.9895e-02,  3.4760e-02,  1.0899e-01,  8.7989e-02,  1.3136e-02],\n",
       "                        [ 3.3092e-02,  1.1749e-01,  1.2988e-01,  1.3105e-01,  1.3428e-01],\n",
       "                        [ 4.4430e-03,  1.6879e-03,  7.5437e-02,  1.2540e-01,  6.9848e-02],\n",
       "                        [-4.8331e-02, -5.4223e-02, -5.4879e-02, -1.2063e-02,  3.3823e-02],\n",
       "                        [ 7.0462e-02,  1.8302e-02,  2.5538e-02, -4.6498e-03, -2.0324e-02]],\n",
       "              \n",
       "                       [[-6.2538e-02,  2.0335e-02,  7.5645e-02, -6.8878e-02,  7.1818e-02],\n",
       "                        [-4.3581e-02,  8.8049e-02, -8.0232e-02, -2.4101e-02,  7.9811e-02],\n",
       "                        [ 2.7967e-02, -7.0410e-03, -6.9889e-02,  3.2319e-02,  1.7870e-02],\n",
       "                        [ 8.1937e-02, -9.4862e-05, -1.1735e-01, -2.7827e-02,  1.3869e-02],\n",
       "                        [ 4.5273e-02, -5.3580e-02, -4.0039e-02,  5.8939e-02,  2.0774e-03]],\n",
       "              \n",
       "                       [[ 8.1339e-02,  7.9248e-02,  5.6391e-03, -8.9104e-02, -4.7095e-03],\n",
       "                        [ 8.8804e-02,  6.1694e-04,  7.9022e-02,  3.2941e-02, -9.9862e-03],\n",
       "                        [ 4.6741e-02,  3.7678e-02,  7.9536e-02, -2.6570e-03, -1.0685e-04],\n",
       "                        [ 6.2309e-03, -3.1145e-03, -2.7289e-02,  3.6044e-03,  5.3510e-03],\n",
       "                        [-6.1524e-03, -3.2128e-03,  1.6627e-02, -4.0505e-02,  5.4811e-02]],\n",
       "              \n",
       "                       [[-1.0366e-01, -9.1793e-02, -5.6683e-02, -9.3017e-02, -1.6566e-02],\n",
       "                        [-2.9419e-03, -9.7220e-03, -2.1670e-02, -1.0964e-02,  3.8094e-02],\n",
       "                        [ 5.5289e-02, -1.0393e-02,  1.3662e-02,  2.4560e-02,  1.3181e-02],\n",
       "                        [-6.4423e-02,  9.4199e-02, -1.0391e-02,  1.1055e-02, -4.3646e-02],\n",
       "                        [-4.7511e-02,  8.3642e-02,  3.6789e-02, -3.3383e-02,  4.9718e-02]],\n",
       "              \n",
       "                       [[ 9.8271e-02,  4.1136e-02,  5.8438e-03,  5.4852e-02,  4.8364e-03],\n",
       "                        [-4.6460e-02, -3.9237e-02, -1.4092e-02, -4.2083e-02, -3.0173e-02],\n",
       "                        [-1.7147e-02, -1.9722e-04,  6.2494e-02,  1.0591e-01, -7.2290e-02],\n",
       "                        [-6.2484e-02, -8.4141e-02,  7.7173e-02, -9.7553e-03, -4.9081e-02],\n",
       "                        [ 2.9335e-02, -5.7769e-02,  4.3525e-02,  3.8578e-02, -2.7311e-02]],\n",
       "              \n",
       "                       [[-5.0268e-02,  4.7180e-02, -7.5816e-02,  4.0427e-02, -4.7398e-02],\n",
       "                        [ 2.3314e-02,  2.8965e-02,  8.2797e-03, -5.6090e-02,  1.3025e-02],\n",
       "                        [-4.1264e-02,  4.0107e-02,  2.8144e-02,  2.8074e-02, -5.9130e-02],\n",
       "                        [-8.3228e-02,  3.9345e-02,  5.2115e-03, -9.2600e-02, -6.6144e-02],\n",
       "                        [ 3.0303e-02, -8.3412e-02, -1.0485e-01, -1.1565e-01, -1.0112e-01]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 1.3064e-01,  1.0816e-01,  1.9518e-02,  4.3616e-02,  1.0259e-01],\n",
       "                        [ 6.6121e-02,  1.0267e-02,  1.1939e-02,  9.9403e-03,  1.9107e-02],\n",
       "                        [ 1.0926e-02,  7.6957e-02,  5.9093e-02, -7.7981e-03, -1.0331e-01],\n",
       "                        [-6.9046e-02, -2.7768e-02, -7.7368e-02,  3.7760e-02, -1.0864e-01],\n",
       "                        [-5.8476e-02, -6.9770e-02, -2.4454e-02, -2.0793e-02, -6.8901e-02]],\n",
       "              \n",
       "                       [[ 1.1306e-01,  1.9210e-02, -4.4024e-02, -7.4617e-02, -7.1121e-02],\n",
       "                        [ 7.4085e-02,  1.8703e-01,  8.4691e-02, -4.9368e-02, -7.8928e-02],\n",
       "                        [-2.4945e-02,  3.8134e-02,  1.4636e-01, -3.4757e-02,  4.7227e-02],\n",
       "                        [-1.0184e-02, -6.1692e-02,  4.9501e-02,  1.4172e-01,  1.2828e-01],\n",
       "                        [-9.0202e-02, -1.5985e-01, -7.8771e-02,  3.3850e-02,  4.1831e-02]],\n",
       "              \n",
       "                       [[-1.9467e-02,  2.1181e-02,  1.6596e-02, -3.2109e-02, -9.8817e-02],\n",
       "                        [-2.2019e-02, -1.2472e-01, -6.4762e-02, -6.0838e-02, -5.1459e-02],\n",
       "                        [ 5.7316e-03,  2.5309e-02, -6.8483e-02,  1.3199e-02, -8.7265e-02],\n",
       "                        [ 4.1262e-02,  2.6553e-02,  1.4572e-02, -5.0810e-02, -5.2790e-02],\n",
       "                        [-2.7241e-02,  1.3489e-01,  4.1325e-02,  6.0757e-02,  1.8221e-02]],\n",
       "              \n",
       "                       [[-8.4980e-02, -5.5572e-02, -3.2138e-02, -6.8903e-02, -1.1503e-01],\n",
       "                        [-2.8812e-03,  3.6770e-02,  1.0915e-01, -2.7911e-02, -6.1974e-02],\n",
       "                        [-3.8890e-02, -1.4665e-02, -2.8665e-03, -3.9441e-02,  6.7463e-02],\n",
       "                        [-6.9459e-02, -7.8561e-02, -6.0768e-02, -6.1130e-02, -7.2336e-02],\n",
       "                        [ 3.4781e-02, -5.5231e-02, -5.6598e-02,  1.9936e-02, -2.6625e-02]],\n",
       "              \n",
       "                       [[ 1.1440e-02, -1.3646e-01, -1.0559e-01,  3.2619e-02,  8.4149e-02],\n",
       "                        [-4.7561e-02, -2.9214e-02, -7.3687e-02, -6.1788e-02,  1.2068e-01],\n",
       "                        [ 8.2627e-02,  5.4670e-02,  9.9399e-03, -1.3818e-01,  1.2836e-02],\n",
       "                        [ 1.5545e-03, -4.5692e-02, -1.8682e-02,  5.9663e-02, -3.9457e-03],\n",
       "                        [-6.8980e-02,  1.0597e-01,  1.5778e-01,  4.8708e-03,  7.4858e-03]],\n",
       "              \n",
       "                       [[-8.2873e-02,  5.3136e-02, -7.1138e-02,  4.0824e-03, -1.0362e-01],\n",
       "                        [-5.3458e-02,  3.9328e-02,  8.1725e-03,  1.5913e-02,  5.1049e-02],\n",
       "                        [ 7.3448e-02,  9.5505e-02,  5.4562e-02,  5.4868e-02,  8.1404e-02],\n",
       "                        [-1.1020e-02,  1.0066e-01,  1.1565e-01,  1.1001e-01,  3.7378e-02],\n",
       "                        [-4.0438e-02,  1.5904e-01,  1.4684e-01,  1.7013e-01, -5.7745e-03]]],\n",
       "              \n",
       "              \n",
       "                      [[[-7.9482e-02, -9.9878e-02, -9.0043e-02, -1.0750e-02,  4.3462e-02],\n",
       "                        [-4.1166e-02,  9.1333e-02,  5.2623e-02,  9.0164e-02,  6.1844e-02],\n",
       "                        [ 3.3352e-02,  9.7636e-02,  1.5318e-01,  7.7805e-02, -1.2804e-02],\n",
       "                        [ 4.7528e-02,  1.2892e-01,  2.4740e-02,  1.5484e-01,  1.3032e-01],\n",
       "                        [ 3.6883e-02,  1.4427e-03, -3.2352e-02,  5.9610e-02, -4.4739e-02]],\n",
       "              \n",
       "                       [[ 1.0890e-02,  5.2279e-02,  4.6532e-02,  1.0052e-01, -1.1708e-02],\n",
       "                        [ 1.1796e-01,  1.8309e-02, -2.7865e-02, -1.5772e-02,  2.0749e-02],\n",
       "                        [ 2.6607e-02,  2.9313e-02, -8.0052e-02, -3.9546e-02, -6.4830e-02],\n",
       "                        [-9.2788e-02,  4.0925e-02,  1.9238e-02, -7.4553e-02,  1.0303e-02],\n",
       "                        [-9.5026e-02, -3.2040e-02, -2.3440e-02, -7.0404e-02, -9.7805e-02]],\n",
       "              \n",
       "                       [[ 9.0573e-02,  8.9451e-02,  1.3764e-01,  3.4057e-02,  7.3464e-02],\n",
       "                        [ 7.2788e-02,  4.4964e-02,  9.1499e-02,  1.8662e-02,  2.8841e-02],\n",
       "                        [ 1.6012e-02,  3.4471e-02, -2.9791e-02, -3.9358e-02, -8.8183e-02],\n",
       "                        [-7.9173e-02, -3.8245e-02,  4.1921e-03, -3.1198e-02, -8.4080e-02],\n",
       "                        [-7.8741e-02, -3.7766e-02, -9.2002e-02, -4.8659e-02,  4.7479e-03]],\n",
       "              \n",
       "                       [[ 2.2608e-02,  1.7462e-01,  1.6426e-01, -1.4538e-02,  6.8301e-02],\n",
       "                        [ 4.7716e-02, -4.2002e-02, -5.6873e-03,  4.2620e-02, -2.0154e-02],\n",
       "                        [-4.6968e-02, -1.1008e-02,  3.3255e-02, -3.0222e-02, -5.5569e-02],\n",
       "                        [-7.6246e-02,  5.5959e-02, -6.1145e-02, -1.6811e-02,  5.5430e-02],\n",
       "                        [-7.6296e-02,  5.1768e-03,  1.3072e-02, -3.0008e-02, -3.9790e-02]],\n",
       "              \n",
       "                       [[-1.2680e-02, -3.4818e-03, -8.5580e-02, -9.1010e-02, -1.6402e-01],\n",
       "                        [-5.6571e-03,  1.7920e-02, -5.5309e-03, -4.6609e-02, -1.2101e-01],\n",
       "                        [ 4.8760e-02,  1.2871e-03,  1.1994e-01,  4.8177e-02, -3.6631e-02],\n",
       "                        [ 5.5931e-02,  4.1282e-02,  4.7738e-02,  6.5717e-02, -3.6737e-02],\n",
       "                        [ 1.8134e-01,  1.0057e-01, -2.6487e-02,  6.2962e-02, -6.0749e-02]],\n",
       "              \n",
       "                       [[ 9.4374e-02,  1.3779e-01,  7.1677e-02,  4.6616e-02,  1.1712e-01],\n",
       "                        [ 3.0377e-02,  2.1898e-02,  5.9694e-02,  4.5703e-02,  1.9946e-02],\n",
       "                        [ 8.5354e-03,  1.8356e-02, -1.9931e-02,  8.7140e-02,  6.5627e-02],\n",
       "                        [-6.5981e-02, -1.2138e-01, -3.6963e-02, -4.9390e-02, -1.9555e-02],\n",
       "                        [-1.0699e-01, -3.4613e-02, -2.7726e-02, -2.5084e-02, -9.6163e-02]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 4.9115e-02,  1.0166e-02, -2.6003e-02, -4.4373e-02,  3.8276e-02],\n",
       "                        [-3.6579e-02, -6.6707e-02,  1.5461e-02, -3.4889e-03,  2.0288e-02],\n",
       "                        [-1.0587e-02, -7.6825e-02, -1.6077e-02,  5.9204e-02, -2.2884e-02],\n",
       "                        [-3.7324e-03, -7.2301e-02, -6.4134e-02,  4.5583e-02, -2.4420e-02],\n",
       "                        [-4.9568e-02,  2.1966e-02,  2.3617e-02,  4.1587e-02,  6.5022e-02]],\n",
       "              \n",
       "                       [[-9.4805e-02, -6.7301e-02,  4.9006e-02,  1.9300e-01,  1.0635e-01],\n",
       "                        [-8.1991e-02, -1.7019e-01,  2.0174e-02,  1.0436e-01,  1.1480e-01],\n",
       "                        [-9.1300e-02, -1.4627e-01, -3.7507e-02,  9.3969e-02,  1.6808e-02],\n",
       "                        [-1.6901e-01, -9.0641e-02,  3.8843e-02,  5.8239e-02, -2.7813e-02],\n",
       "                        [-1.6015e-01, -1.6411e-01,  7.5467e-02,  1.0334e-01,  3.6281e-02]],\n",
       "              \n",
       "                       [[-4.6237e-02, -1.0055e-01, -1.2002e-02,  1.0487e-02,  7.2374e-03],\n",
       "                        [ 3.8625e-02,  2.3790e-02, -9.3856e-02, -4.7468e-03,  3.3130e-02],\n",
       "                        [ 9.2240e-02, -1.0855e-01,  1.1988e-02, -4.9517e-02, -9.8351e-02],\n",
       "                        [-6.4817e-02,  2.3832e-02, -2.0684e-02, -4.0525e-02, -2.8088e-02],\n",
       "                        [-1.3960e-02, -7.7397e-02,  8.1227e-04, -2.4588e-02, -7.9649e-02]],\n",
       "              \n",
       "                       [[-2.5372e-02, -2.7016e-02, -3.6685e-02,  4.5232e-02,  2.5609e-02],\n",
       "                        [-3.9507e-02, -5.9029e-02, -8.0082e-03,  9.6256e-02,  1.3131e-01],\n",
       "                        [-1.2596e-02, -9.7108e-02, -4.5058e-02,  7.9619e-02,  1.8540e-02],\n",
       "                        [ 9.7984e-02, -7.8869e-02, -7.3257e-02,  9.6142e-02,  3.6172e-02],\n",
       "                        [-1.9250e-02, -4.7639e-02, -9.3758e-02,  4.7705e-02,  4.2552e-02]],\n",
       "              \n",
       "                       [[ 7.1429e-02,  4.6144e-02, -1.3547e-02, -2.4450e-02, -1.3947e-01],\n",
       "                        [ 4.8622e-02,  7.3477e-02, -3.5717e-03,  7.2270e-03, -1.0343e-01],\n",
       "                        [-5.3948e-03,  9.6391e-02,  1.2764e-01, -2.5344e-02, -1.6562e-01],\n",
       "                        [-1.0995e-01,  1.5000e-01,  9.9885e-02, -3.3637e-02, -8.0216e-02],\n",
       "                        [-8.2165e-02,  1.2335e-01,  2.5632e-02, -1.0885e-01, -9.1023e-02]],\n",
       "              \n",
       "                       [[ 2.6096e-02,  2.3758e-02, -1.9003e-03, -5.6457e-02, -4.2902e-02],\n",
       "                        [-9.1769e-03, -1.1162e-01,  3.1594e-02,  4.4663e-02, -7.9885e-03],\n",
       "                        [-2.7703e-02, -1.1657e-01,  2.4159e-02, -2.3096e-02, -3.4452e-02],\n",
       "                        [-5.2686e-02, -1.0135e-01, -6.0387e-02, -9.8791e-02, -8.7204e-02],\n",
       "                        [ 1.0107e-02, -6.8945e-02, -9.9404e-02,  2.7035e-02, -9.7333e-02]]]],\n",
       "                     device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0697, -0.0043, -0.1450,  0.0270, -0.1294,  0.1031, -0.0298, -0.1058,\n",
       "                       0.0464,  0.2657, -0.2746,  0.0514, -0.0529, -0.0330, -0.2068,  0.0325],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0144,  0.0492,  0.0158,  ...,  0.0337, -0.0160,  0.0419],\n",
       "                      [-0.0253,  0.0012, -0.0458,  ..., -0.0121,  0.0290, -0.0083],\n",
       "                      [-0.0241,  0.0415, -0.0187,  ..., -0.0025, -0.0594, -0.0065],\n",
       "                      ...,\n",
       "                      [-0.0498,  0.0343, -0.0252,  ...,  0.0228,  0.0373, -0.0161],\n",
       "                      [-0.0282,  0.0102,  0.0189,  ...,  0.0545,  0.0530,  0.0210],\n",
       "                      [-0.0417, -0.0039,  0.0313,  ..., -0.0406,  0.0300, -0.0311]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 8.4014e-02,  4.1246e-02, -2.1304e-02,  7.2434e-02,  1.4103e-02,\n",
       "                      -1.0099e-01, -1.1937e-02,  1.1666e-02, -1.0945e-02,  5.0268e-02,\n",
       "                       5.0898e-02,  2.8714e-02,  9.8045e-02, -2.0867e-02,  5.7868e-03,\n",
       "                      -1.1982e-02,  1.9822e-02, -4.3051e-02,  4.5801e-03, -2.1910e-02,\n",
       "                      -7.1668e-02, -1.9373e-02,  7.8352e-02, -2.6107e-02,  7.7232e-02,\n",
       "                      -4.4725e-02, -9.9024e-02, -6.2476e-02,  1.9848e-02, -8.2848e-02,\n",
       "                      -2.1750e-02, -7.0061e-03, -2.9500e-02,  4.7071e-03,  2.3010e-02,\n",
       "                       3.0656e-02,  7.9080e-03, -3.5740e-02, -6.9717e-02, -1.3269e-02,\n",
       "                       3.7496e-04,  4.7290e-02,  2.2736e-02, -3.7120e-02,  4.5868e-02,\n",
       "                       1.1418e-02,  4.4020e-03,  6.5944e-02,  3.5092e-02, -5.8815e-02,\n",
       "                      -2.5923e-02, -7.6373e-02,  5.3009e-03, -3.2287e-03,  2.9193e-02,\n",
       "                       8.0547e-02, -1.2761e-02, -4.9733e-03,  2.1681e-03, -6.3579e-03,\n",
       "                       3.8504e-03,  3.5034e-02, -6.2103e-05, -1.6221e-02,  4.6189e-02,\n",
       "                       1.2208e-02,  3.3725e-02,  3.3790e-02, -1.2628e-02,  2.3713e-02,\n",
       "                      -1.3064e-02,  3.4713e-02, -3.2966e-02,  2.3190e-02, -2.6784e-02,\n",
       "                       1.0337e-02,  1.1051e-01,  3.2558e-02,  5.7917e-03, -1.6199e-02,\n",
       "                      -3.3852e-02, -1.8150e-02, -1.0038e-02, -7.5470e-03,  3.1960e-03,\n",
       "                      -1.6696e-02,  8.8764e-03, -7.0273e-02,  8.8653e-02, -3.8244e-02,\n",
       "                      -3.3798e-02,  1.5792e-02,  5.5374e-03, -3.6031e-02,  5.7021e-02,\n",
       "                       1.3792e-02,  1.4791e-02,  6.6484e-03,  3.3091e-02,  3.1009e-02,\n",
       "                      -8.8647e-02,  2.6779e-02,  2.0518e-02, -2.2284e-03,  2.7170e-02,\n",
       "                       4.1441e-02,  6.9077e-02, -4.1621e-02,  4.1960e-02,  4.8460e-02,\n",
       "                       8.7333e-02, -3.3616e-02, -4.3693e-03, -8.0991e-02, -1.0746e-02,\n",
       "                       4.4304e-02, -3.8986e-02, -4.6800e-02,  1.8885e-02, -4.5754e-02],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.0035,  0.0288,  0.1024,  ..., -0.0530, -0.0652,  0.0741],\n",
       "                      [-0.0628,  0.0071,  0.0681,  ...,  0.0597, -0.0214, -0.0652],\n",
       "                      [-0.0035, -0.0628,  0.0524,  ...,  0.0310,  0.0333, -0.0545],\n",
       "                      ...,\n",
       "                      [-0.0336,  0.0770, -0.0085,  ..., -0.0389,  0.0103, -0.0712],\n",
       "                      [ 0.0779,  0.0114,  0.0429,  ...,  0.0407, -0.0782,  0.0113],\n",
       "                      [ 0.0890, -0.0252,  0.0265,  ..., -0.0695,  0.0455, -0.0060]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0986, -0.0985,  0.0712,  0.1537,  0.0683, -0.0377,  0.0408,  0.0616,\n",
       "                      -0.1036,  0.1640,  0.0123,  0.0574, -0.0686, -0.0081,  0.1424, -0.0559,\n",
       "                       0.0907,  0.0052,  0.0160,  0.2426, -0.0632, -0.0551, -0.0437,  0.0844,\n",
       "                      -0.0927,  0.0467, -0.0200,  0.0599, -0.0757,  0.0578, -0.0607, -0.0890,\n",
       "                      -0.0151, -0.0426,  0.0646, -0.0429,  0.1789,  0.1503,  0.0879, -0.0769,\n",
       "                      -0.0227, -0.0290,  0.1215,  0.0478, -0.1246,  0.0951,  0.0767, -0.0565,\n",
       "                       0.0287, -0.0488, -0.0696, -0.0909,  0.0272,  0.0628,  0.1028,  0.1170,\n",
       "                      -0.0875, -0.1421, -0.1547,  0.1496, -0.1127,  0.0423, -0.0095,  0.0440,\n",
       "                       0.2543,  0.0819,  0.0171,  0.0206, -0.0242, -0.0932, -0.0450,  0.0886,\n",
       "                      -0.0811,  0.1282, -0.0057,  0.0274,  0.0881, -0.0727, -0.0065, -0.0160,\n",
       "                      -0.0903, -0.1173, -0.0593,  0.2260], device='cuda:0')),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 4.1367e-02,  6.7605e-02,  1.9387e-01,  2.6176e-02,  8.7152e-02,\n",
       "                        2.8776e-02, -8.7682e-02,  6.0964e-02,  8.7893e-02, -9.1072e-02,\n",
       "                       -1.3559e-01, -1.2122e-01, -5.7737e-03, -1.7068e-02,  2.5794e-01,\n",
       "                        3.0048e-02, -1.7329e-01,  8.6222e-03,  2.6571e-01,  1.4706e-01,\n",
       "                       -1.2780e-01, -2.0034e-02,  1.0223e-01, -8.2627e-04, -1.0765e-01,\n",
       "                        1.6037e-01,  3.7851e-02, -9.1195e-02, -6.4546e-02, -1.1270e-01,\n",
       "                        1.8065e-02,  7.4935e-02,  2.0439e-01,  1.1496e-02, -2.5507e-02,\n",
       "                        1.1802e-01, -1.6661e-01,  3.1101e-02,  3.0612e-02,  9.1462e-03,\n",
       "                       -7.2644e-02,  6.8903e-02, -9.2119e-03,  1.5368e-01, -7.1022e-02,\n",
       "                       -4.7094e-02, -7.7793e-02,  1.2402e-02, -5.0466e-02,  5.7677e-02,\n",
       "                       -8.0259e-02,  6.9585e-02, -5.4962e-02,  5.7943e-02,  1.4025e-02,\n",
       "                        1.1525e-01, -4.5051e-02, -3.2643e-02, -4.4478e-02, -5.3781e-02,\n",
       "                        8.3786e-02,  1.2867e-01, -5.6481e-02, -3.7490e-02, -2.7583e-01,\n",
       "                       -9.2038e-02,  4.7864e-02,  2.0753e-02,  7.7528e-02, -1.5455e-01,\n",
       "                       -4.2365e-02,  2.3896e-02,  1.1661e-02,  2.1059e-01,  3.3849e-02,\n",
       "                       -9.5506e-02,  1.1120e-01,  4.6684e-02,  3.6390e-02, -5.4524e-02,\n",
       "                        4.2801e-02, -6.1908e-03, -6.6399e-02, -3.6251e-02],\n",
       "                      [ 1.1315e-01,  3.3247e-01,  1.0298e-02, -2.2307e-01, -2.0170e-01,\n",
       "                        1.8191e-02, -6.8927e-02, -1.2228e-01,  2.1787e-01, -2.1578e-01,\n",
       "                        1.1603e-01, -8.4712e-02,  1.0868e-01,  4.3903e-02,  1.2627e-01,\n",
       "                        3.1568e-02, -2.6958e-01,  3.6179e-04, -1.4950e-01, -2.0020e-01,\n",
       "                       -1.3509e-01,  6.1986e-02,  9.9959e-03, -2.0197e-01, -9.0702e-02,\n",
       "                       -7.1846e-02, -6.6574e-02, -6.5604e-02,  6.1317e-02,  2.6086e-03,\n",
       "                       -3.8067e-02, -8.8751e-02, -4.5490e-03,  2.8485e-01, -5.2330e-02,\n",
       "                       -1.1657e-01,  2.4085e-02, -9.0061e-02, -2.2606e-02,  1.7254e-01,\n",
       "                       -2.0944e-01,  1.0815e-01,  1.3979e-02,  6.3779e-02,  4.9977e-03,\n",
       "                       -8.9919e-02, -1.4782e-01,  9.4452e-02, -4.3495e-03, -2.3420e-02,\n",
       "                        1.7088e-01,  2.5394e-02, -7.7732e-02, -9.5750e-03, -1.1475e-01,\n",
       "                       -4.4464e-02, -2.5432e-02,  2.1804e-01,  2.5520e-01,  7.6013e-02,\n",
       "                       -2.5999e-02,  8.9999e-02, -1.4793e-03,  5.8009e-02, -2.2091e-01,\n",
       "                       -1.3404e-02,  4.2208e-02,  5.4960e-02, -1.1856e-01,  1.5973e-01,\n",
       "                        8.4619e-02, -6.8527e-03, -7.9792e-03,  7.7766e-02, -1.1517e-01,\n",
       "                       -3.3597e-02, -7.5942e-02,  4.3097e-02, -1.0659e-02, -8.5962e-02,\n",
       "                        1.0645e-01, -4.8539e-02,  1.8021e-01,  3.6649e-03],\n",
       "                      [ 9.8081e-02, -7.3224e-02, -6.8003e-02,  2.7683e-01,  1.5841e-01,\n",
       "                        8.6982e-02, -1.0328e-01,  1.2635e-01, -1.1449e-01,  1.0711e-02,\n",
       "                       -1.3525e-02,  1.6648e-01,  4.0631e-02, -8.9058e-02, -5.1522e-02,\n",
       "                        3.6054e-02, -7.0135e-02,  3.6172e-02,  2.4675e-01,  1.3142e-01,\n",
       "                        4.8696e-02, -1.4252e-02, -2.8080e-02,  1.5251e-01, -9.4448e-02,\n",
       "                        1.6517e-01, -5.5823e-02,  7.3319e-02,  1.4454e-03,  7.2087e-02,\n",
       "                        2.7352e-02, -4.4092e-02,  7.4378e-02,  9.1737e-03, -8.6819e-02,\n",
       "                        2.1945e-03,  6.5348e-02,  7.7871e-02,  1.6354e-02, -6.2424e-02,\n",
       "                        3.3618e-02, -6.6117e-02, -1.2802e-02, -1.5238e-01, -1.2747e-01,\n",
       "                        1.0178e-01, -1.6238e-01,  6.8626e-02,  2.5866e-02,  9.6245e-02,\n",
       "                       -1.3325e-01,  1.1791e-02,  5.9152e-02, -5.5632e-02,  1.0553e-01,\n",
       "                        6.5079e-02,  1.0061e-01,  2.0416e-02,  7.8001e-02,  1.2365e-01,\n",
       "                        4.6758e-02, -1.1189e-01, -1.0359e-01,  5.9308e-02,  1.0553e-01,\n",
       "                        4.2918e-03, -9.8210e-02,  1.0063e-01,  9.5499e-02, -1.2852e-01,\n",
       "                        3.0828e-02,  3.5236e-02, -2.8623e-03, -5.3002e-02, -7.0348e-03,\n",
       "                        1.2189e-01, -7.5645e-02, -1.3202e-01,  7.0546e-02, -7.8215e-02,\n",
       "                        7.8920e-02, -5.3042e-02, -4.9083e-02,  1.3150e-01],\n",
       "                      [-1.4532e-01, -1.5281e-01, -1.5257e-01, -4.7600e-02, -3.9674e-02,\n",
       "                        7.5360e-02,  1.0565e-01,  6.4379e-02, -1.1313e-01,  2.8811e-01,\n",
       "                       -6.2009e-02,  4.6586e-02,  3.3934e-02, -1.2976e-02, -1.3571e-01,\n",
       "                       -6.2968e-02,  1.6341e-01,  2.8681e-02, -1.1200e-01,  7.7960e-02,\n",
       "                       -1.2865e-02, -8.3248e-02, -9.7686e-02, -2.3157e-02,  8.5423e-02,\n",
       "                       -1.0790e-01,  1.0498e-01, -2.0350e-02, -5.3813e-03,  3.1184e-02,\n",
       "                        5.5570e-02,  7.8772e-02, -5.6085e-02, -1.6307e-01,  1.2407e-02,\n",
       "                        1.1709e-01, -1.5973e-02, -3.0314e-02,  5.7776e-02,  8.2913e-03,\n",
       "                        1.4457e-01,  4.0885e-02,  5.0049e-02, -1.7015e-01, -5.2836e-02,\n",
       "                        1.1438e-03,  9.5934e-02, -3.0761e-02,  1.9087e-02,  5.1824e-02,\n",
       "                       -8.2851e-02, -1.0391e-02, -5.7323e-02, -9.7882e-02,  6.7026e-02,\n",
       "                       -4.8018e-02,  9.0170e-02,  6.5736e-02, -7.5602e-02,  1.9712e-02,\n",
       "                        3.1327e-02, -1.0510e-02,  1.6356e-01, -6.8878e-02,  2.5338e-02,\n",
       "                        1.7560e-01,  1.1093e-03,  1.3709e-01,  7.0373e-03,  3.1966e-02,\n",
       "                       -1.4941e-02,  1.6206e-02,  2.2627e-02, -1.0075e-01, -1.1429e-01,\n",
       "                        2.1358e-01,  2.6786e-02, -1.3337e-02,  7.3037e-02, -9.4929e-02,\n",
       "                       -2.1344e-02,  1.2837e-03,  1.3327e-01,  5.7723e-03],\n",
       "                      [-1.0466e-02, -4.7372e-02,  2.9756e-02,  1.3366e-01,  4.2297e-02,\n",
       "                        5.4913e-02,  2.5891e-02,  8.1278e-04, -1.7639e-01, -1.0034e-01,\n",
       "                        1.4535e-01,  5.2618e-02, -3.7404e-02, -1.1808e-01, -5.8078e-02,\n",
       "                        3.5978e-03,  1.1189e-01,  1.3419e-01,  5.8104e-02,  1.0813e-01,\n",
       "                       -8.8892e-02,  6.0615e-02, -4.9475e-02,  1.2332e-01,  6.2726e-02,\n",
       "                       -2.2681e-02, -3.5566e-02,  1.1794e-01,  1.6607e-02,  8.3143e-02,\n",
       "                        4.2043e-02, -1.6364e-02,  5.3228e-02,  1.0373e-02, -4.3972e-02,\n",
       "                       -8.3495e-02,  2.2206e-01,  1.0462e-01,  6.7771e-02, -1.2250e-01,\n",
       "                       -5.2461e-02, -9.0987e-02,  1.7780e-01, -3.4391e-02, -1.4798e-01,\n",
       "                        6.4554e-02, -1.1310e-01, -2.0196e-02,  1.1048e-01, -7.2135e-02,\n",
       "                       -1.4112e-01,  8.4571e-02,  7.3796e-02,  1.0523e-01, -4.7907e-02,\n",
       "                        1.2377e-01,  7.9510e-02, -1.8608e-02,  3.6253e-02,  2.0479e-01,\n",
       "                        4.3002e-02, -3.8114e-02, -3.1663e-02, -1.9290e-02,  2.9696e-01,\n",
       "                       -7.6458e-02,  9.1157e-02, -8.0819e-02,  1.2646e-01, -1.3209e-01,\n",
       "                       -8.0233e-02, -1.4304e-01, -1.7340e-02,  3.6410e-02,  1.3487e-01,\n",
       "                       -2.1241e-02, -8.2192e-02, -7.1532e-02,  6.7613e-02, -1.5628e-02,\n",
       "                       -2.4675e-02, -1.0861e-01, -1.7954e-01,  2.5805e-01],\n",
       "                      [-3.0690e-02, -1.3892e-01, -2.3464e-01, -4.2267e-02,  1.8717e-01,\n",
       "                       -1.0382e-01, -2.6579e-02,  1.0118e-01, -7.5878e-02,  1.5467e-01,\n",
       "                        3.8823e-03,  9.2332e-02, -6.8585e-03,  1.1344e-01, -2.3632e-01,\n",
       "                       -4.4610e-02,  1.8109e-01,  4.5312e-02, -1.2397e-01, -1.2906e-02,\n",
       "                       -2.4557e-02, -9.9091e-03,  9.1200e-02,  5.9798e-02,  6.8266e-02,\n",
       "                       -1.3786e-01, -6.2554e-02, -1.5228e-01,  3.8506e-02,  3.8752e-02,\n",
       "                       -5.5093e-02, -8.5093e-02,  3.3855e-03, -7.8255e-02, -1.0281e-01,\n",
       "                        2.5704e-02, -1.9027e-02, -1.1651e-01, -3.1290e-02,  1.9534e-02,\n",
       "                        1.3391e-01,  4.3350e-02, -5.1055e-02, -1.2494e-01, -1.0700e-01,\n",
       "                        1.8398e-01,  1.0940e-01,  6.4536e-02, -5.2793e-02, -7.8954e-02,\n",
       "                       -1.5461e-01,  9.1328e-02,  1.1505e-01, -1.6576e-01,  3.0209e-02,\n",
       "                       -2.3198e-02, -3.7952e-02,  1.6757e-02, -2.4054e-02, -1.0748e-01,\n",
       "                        3.8726e-02, -1.4970e-01, -1.3024e-02, -3.0412e-02,  1.4003e-01,\n",
       "                       -3.5852e-02, -4.4426e-02,  1.1850e-01,  3.0132e-02,  1.3378e-01,\n",
       "                       -5.2333e-02,  6.5945e-02, -1.1753e-01, -1.3524e-01, -1.3642e-01,\n",
       "                        2.0297e-01,  2.9630e-02, -3.9463e-02,  8.4551e-02,  8.8263e-02,\n",
       "                        1.9459e-02,  9.4656e-02,  5.8690e-02, -9.4056e-02],\n",
       "                      [ 1.9725e-01, -1.0790e-01, -1.4196e-01, -6.8549e-02, -3.8507e-02,\n",
       "                       -1.4759e-02,  3.5695e-02, -8.0040e-02, -1.2622e-01,  1.5289e-01,\n",
       "                        1.4262e-01,  1.7167e-01, -1.8585e-02, -9.3788e-02, -1.5225e-01,\n",
       "                       -6.8604e-02,  5.5158e-02, -1.0989e-01, -3.9302e-02,  1.8672e-01,\n",
       "                       -1.9283e-02, -7.5351e-02, -7.5264e-02, -1.3782e-01,  7.9739e-02,\n",
       "                        4.3035e-02, -5.4352e-02,  5.3199e-04,  7.8098e-02, -1.4170e-02,\n",
       "                       -4.2570e-02,  5.2092e-02, -4.8223e-02, -1.8015e-01,  7.0335e-02,\n",
       "                        5.1699e-02,  2.9291e-01,  1.9366e-01, -2.1165e-01, -5.0193e-02,\n",
       "                       -9.6733e-02,  7.7972e-02,  5.0454e-02, -1.6538e-01, -7.7248e-02,\n",
       "                        4.7087e-02, -9.9155e-03, -1.0539e-01, -7.7132e-02, -6.8070e-02,\n",
       "                        2.0625e-01, -1.3387e-01, -7.1882e-02, -1.4381e-01,  1.5469e-01,\n",
       "                       -1.0604e-01, -3.5755e-03, -4.2248e-02, -1.3520e-01,  2.1293e-01,\n",
       "                       -9.2630e-02, -1.3681e-01,  1.3337e-01, -5.9575e-02,  2.2427e-01,\n",
       "                        2.0907e-01, -8.2154e-03, -1.8471e-01, -1.0657e-01,  2.0637e-02,\n",
       "                       -6.3356e-02, -1.7364e-01,  2.2399e-02, -9.1585e-02,  2.3057e-02,\n",
       "                       -7.0788e-02, -2.5408e-01,  1.2555e-01, -2.2676e-02,  7.3544e-02,\n",
       "                        7.8469e-02, -1.5972e-01, -4.2760e-02,  2.0225e-01],\n",
       "                      [-9.9589e-02, -1.7094e-01,  5.4924e-02,  9.0209e-02, -5.2040e-02,\n",
       "                       -2.0697e-02,  2.6648e-02, -9.0184e-02, -6.9399e-02, -1.6679e-01,\n",
       "                        2.5637e-02, -1.1058e-01, -1.9923e-02,  4.5173e-02, -2.9399e-01,\n",
       "                        1.6890e-02,  2.2015e-01,  1.4176e-01,  1.2865e-02, -1.9473e-01,\n",
       "                        1.9031e-01,  3.8523e-02,  9.3570e-02,  1.4697e-01,  1.3807e-02,\n",
       "                       -1.2434e-01, -5.5466e-02,  1.5722e-01, -6.1573e-02,  4.0583e-02,\n",
       "                       -8.4409e-02,  3.0851e-02, -1.1244e-01,  1.2292e-02,  2.1860e-02,\n",
       "                       -6.5060e-02, -1.7320e-02, -1.4589e-01,  5.0375e-02,  3.1330e-02,\n",
       "                        1.2629e-01, -1.1517e-01,  5.4997e-03, -1.7623e-01,  2.0089e-01,\n",
       "                        3.2376e-02,  2.8791e-04, -5.2601e-02, -7.0643e-02, -7.5819e-02,\n",
       "                       -6.2505e-02,  9.8644e-02,  1.9033e-01,  2.3109e-01, -1.5493e-01,\n",
       "                       -1.2772e-01,  1.2087e-02, -5.5287e-02,  2.0720e-01, -6.5947e-02,\n",
       "                        2.0216e-01, -7.6614e-02, -4.8716e-02, -1.0395e-01,  1.1725e-01,\n",
       "                       -1.7251e-01, -1.8018e-02, -8.6292e-02,  1.8551e-01,  1.3668e-01,\n",
       "                       -4.9868e-02, -5.5710e-03,  4.3332e-02, -1.1746e-01,  4.9710e-02,\n",
       "                        1.7119e-01, -3.3445e-02,  5.4970e-03,  5.1862e-02,  1.1366e-02,\n",
       "                       -1.2655e-03,  6.6179e-02, -1.6720e-02, -1.9130e-01],\n",
       "                      [-7.1983e-02, -1.2768e-02,  3.2601e-01,  1.0217e-01, -1.1205e-01,\n",
       "                        1.7955e-03, -5.0551e-02, -1.2805e-01, -5.0740e-03,  7.8779e-02,\n",
       "                       -1.1413e-01, -5.8079e-02,  3.5420e-02,  8.9876e-02,  1.8581e-01,\n",
       "                       -6.1444e-02, -1.1500e-01, -1.6899e-01, -2.9369e-03,  5.1563e-02,\n",
       "                       -1.2475e-02,  1.0024e-02,  6.8019e-02, -1.0582e-01,  8.7573e-02,\n",
       "                        1.2658e-01,  1.5083e-03, -8.4423e-02,  5.4015e-02, -7.7698e-03,\n",
       "                        1.2900e-01, -7.0008e-02,  1.2552e-01,  1.3287e-01,  2.7627e-01,\n",
       "                        1.1395e-02, -1.7201e-01, -1.1033e-01,  3.4623e-02, -3.6308e-02,\n",
       "                       -1.5589e-01, -2.1293e-02, -6.3386e-02,  7.8554e-02, -1.5824e-01,\n",
       "                       -1.2059e-01,  1.2429e-01,  3.8891e-02, -7.3362e-03, -9.6063e-02,\n",
       "                       -7.2675e-02, -1.2913e-01, -1.5397e-01, -4.0756e-02,  7.4809e-02,\n",
       "                       -4.0833e-02,  8.6166e-02, -1.0490e-02, -2.2906e-01, -1.5642e-01,\n",
       "                       -8.0759e-02,  1.9222e-01, -2.0107e-01,  1.6297e-02, -3.0599e-01,\n",
       "                       -1.3302e-01,  9.0629e-02, -1.1807e-01, -9.4464e-02, -4.7249e-02,\n",
       "                        9.5643e-02,  1.5632e-01,  8.7356e-02,  3.0886e-01,  1.4604e-01,\n",
       "                       -2.2577e-01,  6.4838e-02, -1.1754e-01, -9.1015e-04,  1.0114e-01,\n",
       "                       -7.0345e-02,  2.5429e-02,  1.6968e-01, -1.2712e-01],\n",
       "                      [-8.3022e-02,  1.2503e-01,  1.8040e-01, -3.3320e-02, -6.4170e-02,\n",
       "                        6.4234e-02,  5.6891e-02, -1.4786e-01,  3.1754e-01, -8.3484e-02,\n",
       "                        8.3622e-02, -1.3753e-01,  4.7608e-02, -1.3348e-01, -2.6093e-02,\n",
       "                        5.2906e-02,  2.2660e-02, -1.1641e-02, -5.5047e-02, -1.7383e-01,\n",
       "                        6.2541e-02, -1.2852e-02,  9.3267e-02, -1.7149e-01, -1.0704e-02,\n",
       "                        5.7662e-02, -1.4188e-02,  1.5739e-01,  6.0656e-02,  4.8350e-03,\n",
       "                        1.0685e-01, -3.9875e-02, -1.6405e-02,  1.8376e-01,  1.7945e-01,\n",
       "                       -2.6920e-02, -8.3277e-02, -3.9871e-02, -9.3920e-02,  5.6950e-02,\n",
       "                       -1.2407e-01, -6.3313e-02, -1.5319e-01,  1.4392e-01,  1.9688e-01,\n",
       "                       -8.7454e-02,  1.0150e-02,  4.8440e-02,  5.5843e-02,  5.9848e-02,\n",
       "                        1.5016e-01, -4.7689e-02, -1.6306e-01, -5.9788e-02, -6.1483e-02,\n",
       "                       -1.8264e-01,  4.5298e-02,  1.3435e-01,  4.0893e-02,  5.4741e-02,\n",
       "                        1.6583e-01, -8.7418e-02,  1.6423e-01, -4.5823e-02, -2.9133e-01,\n",
       "                       -7.6180e-02, -6.4323e-02, -1.2117e-01, -1.0216e-01,  8.1860e-02,\n",
       "                        1.0864e-01, -1.0560e-01, -8.9116e-02,  5.9175e-03, -8.5916e-02,\n",
       "                       -1.2851e-01,  4.8941e-02,  1.1586e-01, -1.5594e-02, -4.9109e-02,\n",
       "                       -4.6591e-02,  1.4840e-01, -5.4408e-03, -1.6765e-01]], device='cuda:0')),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.0472, -0.2898,  0.1666,  0.0727,  0.3128, -0.1652,  0.1177, -0.1012,\n",
       "                       0.2390, -0.2131], device='cuda:0'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.weight',\n",
       "              tensor([[[[ 0.1967,  0.0823,  0.0303,  0.0031,  0.0756],\n",
       "                        [ 0.1601,  0.0802,  0.1588,  0.1865,  0.1881],\n",
       "                        [ 0.0938,  0.2008,  0.1104,  0.0381,  0.1547],\n",
       "                        [ 0.0342, -0.0173, -0.0483, -0.0230, -0.0326],\n",
       "                        [-0.0240,  0.0265,  0.1234, -0.0687,  0.1412]],\n",
       "              \n",
       "                       [[-0.0414, -0.1244,  0.0024,  0.0062, -0.0857],\n",
       "                        [-0.1563, -0.1609, -0.0257, -0.0971,  0.0781],\n",
       "                        [ 0.0122, -0.0597,  0.0665, -0.0912, -0.1002],\n",
       "                        [ 0.0413, -0.0874, -0.0417,  0.0542, -0.0955],\n",
       "                        [-0.0381, -0.1183, -0.0379, -0.0519, -0.0811]],\n",
       "              \n",
       "                       [[-0.1762,  0.0305, -0.0139, -0.1512,  0.0315],\n",
       "                        [-0.1243, -0.0507, -0.0267, -0.0890, -0.0237],\n",
       "                        [-0.0731, -0.0007, -0.0058, -0.0699, -0.1055],\n",
       "                        [-0.0954, -0.0260,  0.0109,  0.0110,  0.1228],\n",
       "                        [ 0.0637, -0.0528,  0.0778,  0.0512,  0.1374]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0876,  0.0019, -0.0099, -0.0175, -0.0036],\n",
       "                        [ 0.0184, -0.1699,  0.0487,  0.0967,  0.0843],\n",
       "                        [-0.1950, -0.0449, -0.0121,  0.1318,  0.0825],\n",
       "                        [-0.1896, -0.0576,  0.1363,  0.1566, -0.0099],\n",
       "                        [-0.1093,  0.0339, -0.0214,  0.2255,  0.1003]],\n",
       "              \n",
       "                       [[-0.0717, -0.0463,  0.0095,  0.0832,  0.1148],\n",
       "                        [ 0.0231, -0.0310,  0.0342,  0.1511,  0.1614],\n",
       "                        [-0.1501,  0.0274,  0.1736,  0.1456, -0.0013],\n",
       "                        [-0.0121, -0.1315,  0.2183,  0.1738,  0.1058],\n",
       "                        [ 0.0475,  0.0682,  0.0374,  0.1642,  0.2788]],\n",
       "              \n",
       "                       [[-0.0461, -0.0139, -0.0372, -0.1385, -0.0576],\n",
       "                        [-0.1534, -0.2051, -0.0881,  0.0392, -0.0569],\n",
       "                        [-0.3179, -0.2026,  0.1116,  0.1194,  0.0680],\n",
       "                        [-0.2477, -0.0390, -0.0784,  0.1348,  0.0300],\n",
       "                        [-0.1109, -0.0891, -0.0669,  0.0700,  0.0127]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.2496, -0.1887, -0.1854, -0.2036, -0.2285],\n",
       "                        [-0.1483, -0.1297, -0.2048, -0.1387, -0.0859],\n",
       "                        [ 0.0115,  0.0202,  0.0679,  0.0366, -0.1726],\n",
       "                        [-0.0775,  0.0620,  0.0167, -0.0637, -0.0945],\n",
       "                        [ 0.0347,  0.0276,  0.1106,  0.0731,  0.1674]],\n",
       "              \n",
       "                       [[-0.0120, -0.1229, -0.1443,  0.0534, -0.1284],\n",
       "                        [-0.0604, -0.0680, -0.1321,  0.0238, -0.0187],\n",
       "                        [-0.0851, -0.0869, -0.0123, -0.0708,  0.0933],\n",
       "                        [ 0.1424, -0.0472, -0.0148,  0.0323,  0.1420],\n",
       "                        [-0.0116,  0.1460,  0.0540,  0.1031,  0.0877]],\n",
       "              \n",
       "                       [[-0.0615, -0.0292,  0.0302, -0.0231, -0.0543],\n",
       "                        [ 0.0925,  0.1066, -0.0158, -0.0146,  0.0027],\n",
       "                        [ 0.0006,  0.0956,  0.0152,  0.0187, -0.0532],\n",
       "                        [ 0.1188,  0.0344,  0.1986,  0.1285,  0.1883],\n",
       "                        [ 0.0433,  0.1881,  0.1596,  0.1348,  0.1803]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0314, -0.1089, -0.0131,  0.0370, -0.1008],\n",
       "                        [-0.0469, -0.0341, -0.1610, -0.0947,  0.0258],\n",
       "                        [-0.0937, -0.1191, -0.0600,  0.0131, -0.0710],\n",
       "                        [-0.1538,  0.0241, -0.0125, -0.0993,  0.0214],\n",
       "                        [-0.0125,  0.0263,  0.0664,  0.1081,  0.0515]],\n",
       "              \n",
       "                       [[ 0.0380,  0.0257,  0.0424, -0.0868, -0.0376],\n",
       "                        [-0.0699,  0.0315, -0.0762, -0.0123, -0.0108],\n",
       "                        [-0.0743, -0.1449, -0.0044,  0.0756, -0.1105],\n",
       "                        [-0.0087,  0.0708,  0.0808, -0.0472,  0.0966],\n",
       "                        [ 0.0107,  0.0328,  0.0649, -0.0025,  0.1074]],\n",
       "              \n",
       "                       [[ 0.0042,  0.1018,  0.1358,  0.1339,  0.1545],\n",
       "                        [ 0.1620,  0.1249,  0.1718,  0.0499,  0.2030],\n",
       "                        [ 0.0839,  0.0335,  0.1187,  0.1160,  0.1774],\n",
       "                        [ 0.0827,  0.1278,  0.1949,  0.1055,  0.1546],\n",
       "                        [ 0.0329,  0.0565,  0.1338,  0.2002,  0.0158]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1164,  0.0695,  0.1379, -0.0199, -0.0376],\n",
       "                        [-0.0659, -0.0367,  0.0838, -0.0760, -0.0755],\n",
       "                        [-0.0348, -0.1167, -0.1033, -0.1560,  0.0670],\n",
       "                        [-0.0714,  0.0540, -0.1392, -0.1903, -0.1423],\n",
       "                        [ 0.0989,  0.0331, -0.0796,  0.0141, -0.0920]],\n",
       "              \n",
       "                       [[-0.0917, -0.1057, -0.0895, -0.1266, -0.1850],\n",
       "                        [-0.1298, -0.0470,  0.0021, -0.0533, -0.1478],\n",
       "                        [-0.0728, -0.0831, -0.1926, -0.0448, -0.0123],\n",
       "                        [-0.0184, -0.0805,  0.0444, -0.1179, -0.0053],\n",
       "                        [ 0.1686,  0.0647,  0.0496, -0.0505,  0.1662]],\n",
       "              \n",
       "                       [[ 0.0361, -0.0907,  0.0609, -0.0751, -0.1323],\n",
       "                        [ 0.0937, -0.1459, -0.1189, -0.0083,  0.0184],\n",
       "                        [ 0.1075, -0.1361, -0.0081, -0.1056,  0.0422],\n",
       "                        [ 0.0031, -0.0784, -0.0637,  0.0049, -0.0289],\n",
       "                        [ 0.1774,  0.1903, -0.0411, -0.0432, -0.0103]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0773,  0.0085,  0.0982, -0.0100, -0.0559],\n",
       "                        [-0.0282, -0.1991, -0.1567, -0.1027, -0.1435],\n",
       "                        [-0.0420, -0.0665, -0.1229, -0.1842, -0.1855],\n",
       "                        [-0.1232,  0.0769, -0.0210,  0.1254, -0.0341],\n",
       "                        [ 0.1985,  0.0910,  0.1906,  0.2751,  0.2376]],\n",
       "              \n",
       "                       [[ 0.0342,  0.1571,  0.2096,  0.0570,  0.1275],\n",
       "                        [-0.0506, -0.0533,  0.0401, -0.1012, -0.0154],\n",
       "                        [-0.0254, -0.1363, -0.2180, -0.1359, -0.0313],\n",
       "                        [-0.0673,  0.0251, -0.0947, -0.0432, -0.0830],\n",
       "                        [ 0.0419,  0.0874,  0.1890,  0.0030,  0.1531]],\n",
       "              \n",
       "                       [[ 0.1330,  0.1247,  0.1507,  0.1443,  0.1411],\n",
       "                        [-0.0293,  0.0820,  0.0253,  0.0059,  0.0051],\n",
       "                        [-0.0971, -0.0934, -0.1503, -0.0555, -0.0983],\n",
       "                        [ 0.0273,  0.0193, -0.1122, -0.1139,  0.0127],\n",
       "                        [-0.0930,  0.0737,  0.0553,  0.1548,  0.1033]]]], device='cuda:0')),\n",
       "             ('conv1.bias',\n",
       "              tensor([-0.1972,  0.2252, -0.0707,  0.2938, -0.0072, -0.1401], device='cuda:0')),\n",
       "             ('conv2.weight',\n",
       "              tensor([[[[-0.0334, -0.0272, -0.0282, -0.0602, -0.0451],\n",
       "                        [ 0.0436, -0.0201, -0.0403,  0.0357, -0.0859],\n",
       "                        [-0.0703,  0.0422,  0.0079,  0.0670,  0.0553],\n",
       "                        [ 0.0734, -0.0301, -0.0040, -0.0222, -0.0343],\n",
       "                        [-0.0368, -0.0735, -0.0153,  0.0556, -0.0315]],\n",
       "              \n",
       "                       [[-0.0769, -0.0178, -0.0269, -0.0596, -0.0677],\n",
       "                        [-0.0129,  0.0191, -0.0580, -0.0323, -0.0508],\n",
       "                        [ 0.0262, -0.0209,  0.0533,  0.0086, -0.0412],\n",
       "                        [ 0.0789, -0.0138, -0.0294,  0.0786, -0.0619],\n",
       "                        [-0.0040, -0.0687, -0.0591, -0.0321,  0.0429]],\n",
       "              \n",
       "                       [[ 0.0469, -0.0066, -0.0849, -0.0692, -0.0131],\n",
       "                        [-0.0600,  0.0576, -0.0382,  0.0493, -0.0509],\n",
       "                        [ 0.0632,  0.0036,  0.0729,  0.0568,  0.0409],\n",
       "                        [ 0.0682, -0.0152,  0.0578, -0.0518, -0.0322],\n",
       "                        [-0.0604, -0.0719,  0.0133, -0.0420, -0.0149]],\n",
       "              \n",
       "                       [[-0.0166,  0.0902,  0.0986,  0.0578,  0.1097],\n",
       "                        [ 0.0367, -0.0218,  0.0769,  0.0301, -0.0369],\n",
       "                        [-0.0582, -0.0855,  0.0011, -0.0205,  0.0579],\n",
       "                        [ 0.0442,  0.0298,  0.0359,  0.0450, -0.0512],\n",
       "                        [-0.0652, -0.0454, -0.0662, -0.0618, -0.0225]],\n",
       "              \n",
       "                       [[-0.0498,  0.0190,  0.0530, -0.0217,  0.0388],\n",
       "                        [-0.0876,  0.0255, -0.0699, -0.0418, -0.0183],\n",
       "                        [-0.0684,  0.0257,  0.0649,  0.0209,  0.0442],\n",
       "                        [ 0.0070,  0.0503,  0.0825,  0.0596,  0.0671],\n",
       "                        [-0.0661, -0.1140, -0.0483,  0.0051, -0.0016]],\n",
       "              \n",
       "                       [[-0.0341, -0.0508, -0.0536,  0.0371, -0.0751],\n",
       "                        [ 0.0235,  0.0069,  0.0567,  0.0700, -0.0738],\n",
       "                        [ 0.0179,  0.0919, -0.0282,  0.0424, -0.0591],\n",
       "                        [ 0.0631,  0.0432,  0.0985,  0.0719,  0.0824],\n",
       "                        [-0.0066,  0.0514,  0.0421, -0.0636, -0.0359]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0312,  0.0389,  0.0453,  0.0835,  0.0722],\n",
       "                        [ 0.0251, -0.0476,  0.0310,  0.0067,  0.0823],\n",
       "                        [-0.0129,  0.0892, -0.0424, -0.0596,  0.0635],\n",
       "                        [-0.0569,  0.0783,  0.0162, -0.0317, -0.0464],\n",
       "                        [ 0.0628, -0.0694,  0.0608, -0.0152,  0.0893]],\n",
       "              \n",
       "                       [[ 0.0118,  0.0387, -0.0077,  0.0089, -0.0674],\n",
       "                        [-0.0574,  0.0093,  0.0623,  0.0325,  0.0123],\n",
       "                        [-0.0467,  0.0392, -0.0693, -0.0236, -0.0092],\n",
       "                        [ 0.0387, -0.0789,  0.0492,  0.0225,  0.0700],\n",
       "                        [-0.0438, -0.1026, -0.0839,  0.0612,  0.0709]],\n",
       "              \n",
       "                       [[-0.0192, -0.0018, -0.0842, -0.0809, -0.0775],\n",
       "                        [ 0.0592, -0.0499,  0.0098, -0.0249,  0.0170],\n",
       "                        [ 0.0923, -0.0328, -0.0719, -0.0623, -0.0840],\n",
       "                        [-0.0013, -0.0551, -0.0496, -0.0777,  0.0609],\n",
       "                        [-0.0762, -0.0492,  0.0412,  0.0460, -0.0075]],\n",
       "              \n",
       "                       [[ 0.1007,  0.0219,  0.0317, -0.0705, -0.0311],\n",
       "                        [ 0.0748,  0.0337, -0.0545, -0.0472, -0.0812],\n",
       "                        [ 0.0167,  0.0885,  0.0474, -0.0096, -0.0661],\n",
       "                        [ 0.0785,  0.0588,  0.0519, -0.0615,  0.0520],\n",
       "                        [ 0.0919, -0.0411, -0.0149,  0.0720,  0.0868]],\n",
       "              \n",
       "                       [[ 0.0519,  0.0797, -0.0632, -0.0075,  0.0087],\n",
       "                        [-0.0216,  0.0435, -0.0321, -0.0920, -0.0263],\n",
       "                        [ 0.0448, -0.0671, -0.0185, -0.0026, -0.0348],\n",
       "                        [-0.0540,  0.0409, -0.0616, -0.0568,  0.0127],\n",
       "                        [-0.0130,  0.0293,  0.0297,  0.0774, -0.0260]],\n",
       "              \n",
       "                       [[-0.0511, -0.0783, -0.0089,  0.0079,  0.0224],\n",
       "                        [ 0.0654, -0.0477, -0.0118, -0.0774, -0.0556],\n",
       "                        [-0.0301,  0.0370, -0.0048, -0.0548,  0.0441],\n",
       "                        [ 0.0548,  0.0651, -0.0681,  0.0128,  0.0016],\n",
       "                        [ 0.0511, -0.0596, -0.0442, -0.0291,  0.0312]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0364,  0.0230,  0.0976,  0.0749,  0.0012],\n",
       "                        [ 0.0209,  0.1000,  0.1148,  0.1116,  0.1185],\n",
       "                        [-0.0120, -0.0174,  0.0617,  0.1070,  0.0524],\n",
       "                        [-0.0630, -0.0695, -0.0644, -0.0237,  0.0226],\n",
       "                        [ 0.0669,  0.0159,  0.0274, -0.0066, -0.0212]],\n",
       "              \n",
       "                       [[-0.0625, -0.0021,  0.0597, -0.0802,  0.0383],\n",
       "                        [-0.0644,  0.0547, -0.0801, -0.0259,  0.0442],\n",
       "                        [-0.0168, -0.0626, -0.0562,  0.0375,  0.0071],\n",
       "                        [ 0.0410, -0.0271, -0.0869, -0.0172,  0.0256],\n",
       "                        [ 0.0289, -0.0362,  0.0129,  0.0716,  0.0170]],\n",
       "              \n",
       "                       [[ 0.0776,  0.0742,  0.0155, -0.0725, -0.0004],\n",
       "                        [ 0.0910, -0.0161,  0.0750,  0.0434,  0.0076],\n",
       "                        [ 0.0497,  0.0184,  0.0655, -0.0047,  0.0131],\n",
       "                        [ 0.0079, -0.0009, -0.0126,  0.0187,  0.0153],\n",
       "                        [ 0.0016,  0.0115,  0.0476, -0.0100,  0.0787]],\n",
       "              \n",
       "                       [[-0.0909, -0.0776, -0.0400, -0.0708,  0.0028],\n",
       "                        [-0.0021, -0.0133, -0.0170,  0.0003,  0.0447],\n",
       "                        [ 0.0468, -0.0283,  0.0095,  0.0316,  0.0192],\n",
       "                        [-0.0739,  0.0790, -0.0089,  0.0194, -0.0359],\n",
       "                        [-0.0491,  0.0792,  0.0423, -0.0293,  0.0589]],\n",
       "              \n",
       "                       [[ 0.0911,  0.0369,  0.0107,  0.0621,  0.0117],\n",
       "                        [-0.0337, -0.0262, -0.0108, -0.0449, -0.0167],\n",
       "                        [ 0.0026,  0.0286,  0.0676,  0.1050, -0.0550],\n",
       "                        [-0.0400, -0.0552,  0.0871, -0.0017, -0.0267],\n",
       "                        [ 0.0502, -0.0429,  0.0378,  0.0376, -0.0114]],\n",
       "              \n",
       "                       [[-0.0418,  0.0535, -0.0581,  0.0628, -0.0303],\n",
       "                        [ 0.0277,  0.0235,  0.0091, -0.0441,  0.0342],\n",
       "                        [-0.0394,  0.0398,  0.0359,  0.0394, -0.0509],\n",
       "                        [-0.0742,  0.0555,  0.0345, -0.0650, -0.0486],\n",
       "                        [ 0.0454, -0.0680, -0.0871, -0.0912, -0.0772]]],\n",
       "              \n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1257,  0.1042,  0.0211,  0.0453,  0.1036],\n",
       "                        [ 0.0773,  0.0153,  0.0241,  0.0318,  0.0446],\n",
       "                        [ 0.0291,  0.0892,  0.0695,  0.0110, -0.0756],\n",
       "                        [-0.0444, -0.0084, -0.0609,  0.0558, -0.0865],\n",
       "                        [-0.0375, -0.0460, -0.0026, -0.0004, -0.0503]],\n",
       "              \n",
       "                       [[ 0.0985,  0.0028, -0.0030, -0.0002, -0.0086],\n",
       "                        [ 0.0549,  0.1060,  0.0430,  0.0128, -0.0278],\n",
       "                        [-0.0393, -0.0180,  0.0824, -0.0193,  0.0615],\n",
       "                        [ 0.0105, -0.0486,  0.0534,  0.1033,  0.1056],\n",
       "                        [-0.0180, -0.1064, -0.0380,  0.0344,  0.0380]],\n",
       "              \n",
       "                       [[ 0.0336,  0.0596,  0.0460, -0.0049, -0.0705],\n",
       "                        [ 0.0087, -0.0932, -0.0391, -0.0288, -0.0072],\n",
       "                        [-0.0192,  0.0328, -0.0418,  0.0345, -0.0747],\n",
       "                        [ 0.0211,  0.0274,  0.0322, -0.0402, -0.0708],\n",
       "                        [ 0.0111,  0.1322,  0.0156,  0.0434,  0.0243]],\n",
       "              \n",
       "                       [[-0.0879, -0.0801, -0.0604, -0.0859, -0.0978],\n",
       "                        [-0.0043,  0.0057,  0.0652, -0.0528, -0.0582],\n",
       "                        [-0.0339, -0.0318, -0.0318, -0.0558,  0.0609],\n",
       "                        [-0.0548, -0.0659, -0.0479, -0.0487, -0.0612],\n",
       "                        [ 0.0710, -0.0134, -0.0146,  0.0554,  0.0046]],\n",
       "              \n",
       "                       [[ 0.0286, -0.1180, -0.0917,  0.0304,  0.0764],\n",
       "                        [-0.0645, -0.0209, -0.0526, -0.0808,  0.0915],\n",
       "                        [ 0.0720,  0.0640,  0.0397, -0.1253,  0.0081],\n",
       "                        [ 0.0260, -0.0306, -0.0089,  0.0862,  0.0254],\n",
       "                        [-0.0119,  0.1228,  0.1549,  0.0236,  0.0513]],\n",
       "              \n",
       "                       [[-0.0654,  0.0624, -0.0588,  0.0212, -0.0853],\n",
       "                        [-0.0682,  0.0397,  0.0189,  0.0203,  0.0533],\n",
       "                        [ 0.0304,  0.0659,  0.0478,  0.0431,  0.0626],\n",
       "                        [-0.0096,  0.0813,  0.0767,  0.0767,  0.0318],\n",
       "                        [-0.0077,  0.1344,  0.0873,  0.1213, -0.0144]]],\n",
       "              \n",
       "              \n",
       "                      [[[-0.0699, -0.0911, -0.0829, -0.0009,  0.0558],\n",
       "                        [-0.0534,  0.0725,  0.0330,  0.0728,  0.0509],\n",
       "                        [ 0.0103,  0.0650,  0.1230,  0.0530, -0.0269],\n",
       "                        [ 0.0322,  0.1015, -0.0007,  0.1333,  0.1184],\n",
       "                        [ 0.0357, -0.0103, -0.0467,  0.0499, -0.0445]],\n",
       "              \n",
       "                       [[-0.0083,  0.0524,  0.0096,  0.0715,  0.0013],\n",
       "                        [ 0.0793,  0.0184, -0.0400, -0.0175,  0.0388],\n",
       "                        [ 0.0054,  0.0254, -0.0684, -0.0108, -0.0498],\n",
       "                        [-0.0933,  0.0206,  0.0264, -0.0493,  0.0391],\n",
       "                        [-0.0792, -0.0446, -0.0121, -0.0377, -0.0600]],\n",
       "              \n",
       "                       [[ 0.0794,  0.0675,  0.1131,  0.0271,  0.0824],\n",
       "                        [ 0.0640,  0.0319,  0.0856,  0.0253,  0.0435],\n",
       "                        [ 0.0268,  0.0426, -0.0225, -0.0238, -0.0726],\n",
       "                        [-0.0550, -0.0177,  0.0197, -0.0122, -0.0617],\n",
       "                        [-0.0509, -0.0138, -0.0720, -0.0317,  0.0188]],\n",
       "              \n",
       "                       [[-0.0076,  0.1498,  0.1471, -0.0208,  0.0652],\n",
       "                        [ 0.0242, -0.0513, -0.0044,  0.0527, -0.0109],\n",
       "                        [-0.0664, -0.0246,  0.0313, -0.0262, -0.0524],\n",
       "                        [-0.0751,  0.0488, -0.0601, -0.0127,  0.0563],\n",
       "                        [-0.0674,  0.0053,  0.0108, -0.0239, -0.0379]],\n",
       "              \n",
       "                       [[ 0.0093,  0.0084, -0.0598, -0.0461, -0.1223],\n",
       "                        [-0.0010,  0.0123, -0.0024, -0.0309, -0.1042],\n",
       "                        [ 0.0367, -0.0126,  0.1051,  0.0385, -0.0273],\n",
       "                        [ 0.0472,  0.0449,  0.0520,  0.0650, -0.0215],\n",
       "                        [ 0.1629,  0.1138, -0.0129,  0.0649, -0.0502]],\n",
       "              \n",
       "                       [[ 0.0520,  0.0801,  0.0124, -0.0021,  0.0695],\n",
       "                        [ 0.0165,  0.0035,  0.0425,  0.0303, -0.0070],\n",
       "                        [ 0.0438,  0.0418, -0.0166,  0.0761,  0.0512],\n",
       "                        [-0.0294, -0.0893, -0.0073, -0.0235, -0.0015],\n",
       "                        [-0.0752, -0.0055, -0.0066, -0.0081, -0.0846]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0564,  0.0198, -0.0267, -0.0542,  0.0344],\n",
       "                        [-0.0308, -0.0570,  0.0165, -0.0132,  0.0167],\n",
       "                        [-0.0048, -0.0703, -0.0219,  0.0462, -0.0285],\n",
       "                        [ 0.0044, -0.0658, -0.0688,  0.0377, -0.0276],\n",
       "                        [-0.0382,  0.0304,  0.0209,  0.0362,  0.0626]],\n",
       "              \n",
       "                       [[-0.0519, -0.0110,  0.0478,  0.1389,  0.1082],\n",
       "                        [-0.0421, -0.1018,  0.0172,  0.0426,  0.1131],\n",
       "                        [-0.0481, -0.0741, -0.0474,  0.0384,  0.0210],\n",
       "                        [-0.1217, -0.0258,  0.0299,  0.0106, -0.0218],\n",
       "                        [-0.1059, -0.0950,  0.0832,  0.0716,  0.0404]],\n",
       "              \n",
       "                       [[-0.0518, -0.0871, -0.0046, -0.0010, -0.0185],\n",
       "                        [ 0.0148,  0.0330, -0.0782, -0.0030,  0.0216],\n",
       "                        [ 0.0714, -0.0955,  0.0319, -0.0386, -0.0932],\n",
       "                        [-0.0711,  0.0369, -0.0031, -0.0273, -0.0138],\n",
       "                        [-0.0107, -0.0641,  0.0192, -0.0055, -0.0609]],\n",
       "              \n",
       "                       [[-0.0329, -0.0258, -0.0473,  0.0088, -0.0178],\n",
       "                        [-0.0489, -0.0471, -0.0069,  0.0639,  0.0862],\n",
       "                        [-0.0272, -0.0821, -0.0364,  0.0575, -0.0209],\n",
       "                        [ 0.0829, -0.0674, -0.0657,  0.0791,  0.0073],\n",
       "                        [-0.0394, -0.0409, -0.0836,  0.0379,  0.0244]],\n",
       "              \n",
       "                       [[ 0.0676,  0.0342, -0.0077,  0.0134, -0.1028],\n",
       "                        [ 0.0543,  0.0480, -0.0046,  0.0535, -0.0518],\n",
       "                        [ 0.0115,  0.0663,  0.1239,  0.0164, -0.1186],\n",
       "                        [-0.0897,  0.1216,  0.0863,  0.0022, -0.0292],\n",
       "                        [-0.0595,  0.1058,  0.0183, -0.0829, -0.0474]],\n",
       "              \n",
       "                       [[ 0.0361,  0.0432,  0.0123, -0.0505, -0.0330],\n",
       "                        [-0.0139, -0.0990,  0.0429,  0.0518, -0.0021],\n",
       "                        [-0.0199, -0.0974,  0.0385, -0.0190, -0.0273],\n",
       "                        [-0.0472, -0.0928, -0.0502, -0.0924, -0.0820],\n",
       "                        [ 0.0210, -0.0569, -0.0906,  0.0341, -0.0924]]]], device='cuda:0')),\n",
       "             ('conv2.bias',\n",
       "              tensor([ 0.0727, -0.0119, -0.0877,  0.0235, -0.0783,  0.0705, -0.0275, -0.0555,\n",
       "                       0.0256,  0.1763, -0.1805,  0.0361, -0.0029,  0.0046, -0.1154,  0.0525],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.weight',\n",
       "              tensor([[-0.0199,  0.0411,  0.0079,  ...,  0.0326, -0.0233,  0.0318],\n",
       "                      [-0.0246,  0.0014, -0.0444,  ..., -0.0122,  0.0336, -0.0040],\n",
       "                      [-0.0220,  0.0444, -0.0164,  ...,  0.0012, -0.0536, -0.0071],\n",
       "                      ...,\n",
       "                      [-0.0501,  0.0345, -0.0261,  ...,  0.0226,  0.0392, -0.0144],\n",
       "                      [-0.0323,  0.0014,  0.0113,  ...,  0.0454,  0.0515,  0.0296],\n",
       "                      [-0.0416, -0.0038,  0.0315,  ..., -0.0407,  0.0300, -0.0303]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0660,  0.0407, -0.0262,  0.0618,  0.0084, -0.0706, -0.0125,  0.0222,\n",
       "                      -0.0152,  0.0375,  0.0258,  0.0302,  0.0736, -0.0224,  0.0125, -0.0073,\n",
       "                       0.0203, -0.0374,  0.0059, -0.0059, -0.0666, -0.0247,  0.0633, -0.0261,\n",
       "                       0.0637, -0.0421, -0.0758, -0.0557, -0.0013, -0.0583, -0.0193, -0.0051,\n",
       "                      -0.0224,  0.0054,  0.0257,  0.0307,  0.0089, -0.0346, -0.0464,  0.0044,\n",
       "                      -0.0119,  0.0418,  0.0073, -0.0391,  0.0226,  0.0210,  0.0044,  0.0514,\n",
       "                       0.0234, -0.0447, -0.0270, -0.0651,  0.0029, -0.0122,  0.0172,  0.0647,\n",
       "                      -0.0144, -0.0091,  0.0013, -0.0039,  0.0040,  0.0396,  0.0149, -0.0078,\n",
       "                       0.0303,  0.0105,  0.0282,  0.0234, -0.0126,  0.0243, -0.0144,  0.0288,\n",
       "                      -0.0250,  0.0210, -0.0261,  0.0155,  0.0784,  0.0253,  0.0082, -0.0053,\n",
       "                      -0.0188, -0.0241, -0.0008, -0.0087,  0.0021, -0.0226,  0.0096, -0.0502,\n",
       "                       0.0612, -0.0372, -0.0399,  0.0032,  0.0008, -0.0189,  0.0398,  0.0064,\n",
       "                       0.0095, -0.0081,  0.0323,  0.0318, -0.0663,  0.0153,  0.0023, -0.0049,\n",
       "                       0.0285,  0.0278,  0.0625, -0.0358,  0.0368,  0.0366,  0.0682, -0.0308,\n",
       "                      -0.0039, -0.0552, -0.0050,  0.0406, -0.0392, -0.0444,  0.0145, -0.0451],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.0006,  0.0290,  0.0983,  ..., -0.0546, -0.0591,  0.0741],\n",
       "                      [-0.0493,  0.0093,  0.0691,  ...,  0.0620, -0.0065, -0.0650],\n",
       "                      [-0.0014, -0.0626,  0.0495,  ...,  0.0372,  0.0273, -0.0547],\n",
       "                      ...,\n",
       "                      [-0.0308,  0.0763, -0.0090,  ..., -0.0387,  0.0163, -0.0710],\n",
       "                      [ 0.0683,  0.0133,  0.0489,  ...,  0.0357, -0.0679,  0.0113],\n",
       "                      [ 0.0780, -0.0253,  0.0291,  ..., -0.0690,  0.0422, -0.0063]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([ 0.0843, -0.0831,  0.0494,  0.1312,  0.0615, -0.0350,  0.0413,  0.0589,\n",
       "                      -0.0675,  0.1408, -0.0030,  0.0203, -0.0667, -0.0036,  0.1106, -0.0488,\n",
       "                       0.0966,  0.0123,  0.0095,  0.1940, -0.0479, -0.0529, -0.0436,  0.0783,\n",
       "                      -0.0925,  0.0262, -0.0231,  0.0565, -0.0770,  0.0494, -0.0693, -0.0869,\n",
       "                      -0.0370, -0.0342,  0.0414, -0.0475,  0.1284,  0.1224,  0.0797, -0.0700,\n",
       "                      -0.0055, -0.0293,  0.0948,  0.0431, -0.0868,  0.0936,  0.0772, -0.0588,\n",
       "                       0.0242, -0.0482, -0.0648, -0.0898,  0.0391,  0.0703,  0.0761,  0.0972,\n",
       "                      -0.0874, -0.1223, -0.1288,  0.1106, -0.0959,  0.0165,  0.0003,  0.0424,\n",
       "                       0.1987,  0.0715,  0.0180,  0.0286, -0.0275, -0.0624, -0.0466,  0.0764,\n",
       "                      -0.0789,  0.0919, -0.0371,  0.0431,  0.0922, -0.0679, -0.0065, -0.0207,\n",
       "                      -0.0907, -0.1000, -0.0560,  0.1618], device='cuda:0')),\n",
       "             ('fc3.weight',\n",
       "              tensor([[ 0.0283,  0.0659,  0.1795,  0.0274,  0.0698,  0.0261, -0.0888,  0.0542,\n",
       "                        0.0758, -0.0874, -0.1210, -0.1003, -0.0054,  0.0007,  0.1957,  0.0281,\n",
       "                       -0.1421,  0.0377,  0.2194,  0.1190, -0.1176, -0.0235,  0.1021,  0.0389,\n",
       "                       -0.1077,  0.1309,  0.0317, -0.0942, -0.0725, -0.1029,  0.0200,  0.0750,\n",
       "                        0.1787, -0.0183, -0.0420,  0.1079, -0.1452,  0.0310,  0.0309,  0.0028,\n",
       "                       -0.0587,  0.0737,  0.0040,  0.1446, -0.0560, -0.0201, -0.0537,  0.0094,\n",
       "                       -0.0601,  0.0585, -0.0831,  0.0824, -0.0172,  0.0329,  0.0146,  0.0898,\n",
       "                       -0.0456, -0.0406, -0.0697, -0.0498,  0.0962,  0.1009, -0.0430, -0.0421,\n",
       "                       -0.2248, -0.0942,  0.0508,  0.0162,  0.0831, -0.1474, -0.0687,  0.0089,\n",
       "                        0.0085,  0.1806,  0.0052, -0.0771,  0.1156,  0.0669,  0.0364, -0.0530,\n",
       "                        0.0430, -0.0033, -0.0799, -0.0220],\n",
       "                      [ 0.0994,  0.2600,  0.0145, -0.1696, -0.1660,  0.0177, -0.0683, -0.0984,\n",
       "                        0.1821, -0.1845,  0.0889, -0.0715,  0.1088,  0.0430,  0.1047,  0.0357,\n",
       "                       -0.2264, -0.0181, -0.1194, -0.1666, -0.1060,  0.0656,  0.0100, -0.1767,\n",
       "                       -0.0907, -0.0519, -0.0689, -0.0487,  0.0477,  0.0052, -0.0416, -0.0920,\n",
       "                       -0.0010,  0.2311, -0.0178, -0.1032,  0.0175, -0.0879, -0.0128,  0.1338,\n",
       "                       -0.1816,  0.1094,  0.0188,  0.0485,  0.0164, -0.0713, -0.1293,  0.0943,\n",
       "                        0.0027, -0.0246,  0.1423,  0.0248, -0.0866,  0.0044, -0.1030, -0.0288,\n",
       "                       -0.0254,  0.1758,  0.2002,  0.0640, -0.0354,  0.0707, -0.0083,  0.0513,\n",
       "                       -0.2078, -0.0079,  0.0451,  0.0717, -0.0942,  0.1272,  0.0952,  0.0059,\n",
       "                       -0.0114,  0.0815, -0.0910, -0.0099, -0.0705,  0.0246, -0.0107, -0.0850,\n",
       "                        0.1066, -0.0327,  0.1666, -0.0053],\n",
       "                      [ 0.1052, -0.0712, -0.0541,  0.1991,  0.1227,  0.0884, -0.1016,  0.0759,\n",
       "                       -0.0914,  0.0150,  0.0243,  0.1362,  0.0420, -0.0866, -0.0448,  0.0389,\n",
       "                       -0.0743,  0.0283,  0.2104,  0.1020,  0.0510, -0.0092, -0.0283,  0.0781,\n",
       "                       -0.0946,  0.1531, -0.0582,  0.0700,  0.0070,  0.0533,  0.0343, -0.0434,\n",
       "                        0.0811,  0.0250, -0.0533,  0.0113,  0.0619,  0.0600, -0.0122, -0.0628,\n",
       "                        0.0214, -0.0663, -0.0235, -0.1200, -0.1090,  0.0829, -0.1344,  0.0697,\n",
       "                        0.0271,  0.0952, -0.0915,  0.0006,  0.0536, -0.0710,  0.0979,  0.0306,\n",
       "                        0.1008,  0.0181,  0.0552,  0.1114,  0.0454, -0.0972, -0.0676,  0.0680,\n",
       "                        0.0777,  0.0036, -0.1014,  0.0546,  0.0830, -0.1020,  0.0503,  0.0275,\n",
       "                       -0.0088, -0.0377, -0.0189,  0.1013, -0.0848, -0.0915,  0.0705, -0.0830,\n",
       "                        0.0792, -0.0529, -0.0287,  0.1108],\n",
       "                      [-0.1244, -0.1325, -0.1354, -0.0368, -0.0343,  0.0781,  0.1054,  0.0541,\n",
       "                       -0.1002,  0.2244, -0.0664,  0.0248,  0.0351, -0.0251, -0.0889, -0.0707,\n",
       "                        0.1084,  0.0297, -0.0959,  0.0833, -0.0103, -0.0854, -0.0982, -0.0141,\n",
       "                        0.0855, -0.0869,  0.1037, -0.0138,  0.0014,  0.0143,  0.0670,  0.0785,\n",
       "                       -0.0445, -0.1490,  0.0082,  0.1099,  0.0096, -0.0162,  0.0552,  0.0175,\n",
       "                        0.1284,  0.0413,  0.0652, -0.1451, -0.0535, -0.0163,  0.0697, -0.0319,\n",
       "                        0.0216,  0.0497, -0.0751, -0.0208, -0.0635, -0.0700,  0.0633, -0.0462,\n",
       "                        0.0888,  0.0758, -0.0694,  0.0489,  0.0451,  0.0092,  0.1428, -0.0727,\n",
       "                       -0.0006,  0.1600,  0.0019,  0.1127,  0.0146,  0.0229, -0.0089, -0.0136,\n",
       "                        0.0342, -0.0836, -0.1027,  0.1843,  0.0252, -0.0192,  0.0730, -0.0940,\n",
       "                       -0.0213, -0.0093,  0.1076,  0.0241],\n",
       "                      [-0.0278, -0.0455,  0.0151,  0.1058,  0.0102,  0.0523,  0.0253, -0.0145,\n",
       "                       -0.1685, -0.0919,  0.1131,  0.0559, -0.0385, -0.1039, -0.0480,  0.0051,\n",
       "                        0.0884,  0.0925,  0.0650,  0.1098, -0.0911,  0.0620, -0.0505,  0.1043,\n",
       "                        0.0629, -0.0176, -0.0335,  0.0926,  0.0201,  0.0665,  0.0429, -0.0141,\n",
       "                        0.0584,  0.0181, -0.0429, -0.0774,  0.1843,  0.0944,  0.0479, -0.1121,\n",
       "                       -0.0647, -0.0855,  0.1400, -0.0353, -0.1424,  0.0315, -0.1045, -0.0194,\n",
       "                        0.1032, -0.0716, -0.1186,  0.0849,  0.0152,  0.0897, -0.0333,  0.1022,\n",
       "                        0.0805, -0.0158,  0.0261,  0.1816,  0.0320, -0.0232, -0.0148, -0.0165,\n",
       "                        0.2261, -0.0509,  0.0866, -0.1118,  0.0973, -0.1218, -0.0776, -0.1439,\n",
       "                       -0.0174,  0.0435,  0.1262, -0.0506, -0.0845, -0.0777,  0.0676, -0.0114,\n",
       "                       -0.0250, -0.1072, -0.1594,  0.1997],\n",
       "                      [-0.0172, -0.1160, -0.1883, -0.0222,  0.1690, -0.1021, -0.0265,  0.0871,\n",
       "                       -0.0356,  0.1571,  0.0057,  0.0741, -0.0085,  0.0973, -0.2031, -0.0388,\n",
       "                        0.1693,  0.0444, -0.1115,  0.0024, -0.0372, -0.0059,  0.0914,  0.0486,\n",
       "                        0.0683, -0.1220, -0.0624, -0.1246,  0.0419,  0.0315, -0.0477, -0.0838,\n",
       "                        0.0117, -0.0430, -0.0763,  0.0137, -0.0137, -0.0952, -0.0343,  0.0284,\n",
       "                        0.1017,  0.0402, -0.0442, -0.1058, -0.0794,  0.1640,  0.0952,  0.0638,\n",
       "                       -0.0500, -0.0771, -0.1409,  0.0918,  0.1065, -0.1195,  0.0330, -0.0173,\n",
       "                       -0.0379,  0.0320, -0.0216, -0.0964,  0.0247, -0.1257, -0.0172, -0.0261,\n",
       "                        0.1148, -0.0396, -0.0421,  0.1069,  0.0212,  0.1211, -0.0416,  0.0833,\n",
       "                       -0.1164, -0.1201, -0.1155,  0.1742,  0.0459, -0.0275,  0.0846,  0.0870,\n",
       "                        0.0196,  0.0951,  0.0744, -0.0978],\n",
       "                      [ 0.1660, -0.1096, -0.1118, -0.0615, -0.0152, -0.0146,  0.0361, -0.0599,\n",
       "                       -0.1210,  0.1448,  0.1203,  0.1565, -0.0176, -0.0880, -0.1418, -0.0680,\n",
       "                        0.0812, -0.0704, -0.0362,  0.1347, -0.0055, -0.0755, -0.0739, -0.1264,\n",
       "                        0.0798,  0.0386, -0.0558, -0.0045,  0.0784, -0.0044, -0.0381,  0.0533,\n",
       "                       -0.0382, -0.1687,  0.0654,  0.0646,  0.2296,  0.1454, -0.1868, -0.0466,\n",
       "                       -0.0749,  0.0746,  0.0337, -0.1692, -0.0823,  0.0515, -0.0119, -0.1049,\n",
       "                       -0.0817, -0.0679,  0.1643, -0.1079, -0.0399, -0.1290,  0.1102, -0.0915,\n",
       "                       -0.0039, -0.0364, -0.0809,  0.1876, -0.0581, -0.1212,  0.1127, -0.0583,\n",
       "                        0.2104,  0.1578, -0.0081, -0.1270, -0.0839,  0.0326, -0.0615, -0.1440,\n",
       "                        0.0156, -0.0820,  0.0026, -0.0439, -0.2202,  0.0955, -0.0227,  0.0725,\n",
       "                        0.0778, -0.1313, -0.0344,  0.1658],\n",
       "                      [-0.0900, -0.1293,  0.0598,  0.1089, -0.0378, -0.0198,  0.0263, -0.0766,\n",
       "                       -0.0673, -0.1634,  0.0230, -0.1030, -0.0210,  0.0402, -0.2357,  0.0184,\n",
       "                        0.1638,  0.1049,  0.0360, -0.1626,  0.1542,  0.0357,  0.0935,  0.1440,\n",
       "                        0.0137, -0.1135, -0.0526,  0.1260, -0.0677,  0.0582, -0.0847,  0.0309,\n",
       "                       -0.1110,  0.0307,  0.0257, -0.0831, -0.0261, -0.1323,  0.0513,  0.0517,\n",
       "                        0.0951, -0.1092, -0.0008, -0.1403,  0.1477,  0.0361, -0.0222, -0.0532,\n",
       "                       -0.0656, -0.0769, -0.0654,  0.0768,  0.1685,  0.1806, -0.1486, -0.0932,\n",
       "                        0.0122, -0.0459,  0.1903, -0.0672,  0.1739, -0.0838, -0.0659, -0.1014,\n",
       "                        0.1144, -0.1647, -0.0220, -0.0755,  0.1768,  0.1152, -0.0434,  0.0033,\n",
       "                        0.0443, -0.0858,  0.0686,  0.1413, -0.0486, -0.0190,  0.0519,  0.0154,\n",
       "                       -0.0012,  0.0526, -0.0110, -0.1601],\n",
       "                      [-0.0753, -0.0081,  0.2588,  0.0844, -0.1069, -0.0017, -0.0523, -0.1096,\n",
       "                        0.0043,  0.0662, -0.0917, -0.0428,  0.0352,  0.0579,  0.1211, -0.0665,\n",
       "                       -0.0937, -0.1254, -0.0047,  0.0229, -0.0141,  0.0084,  0.0681, -0.1015,\n",
       "                        0.0876,  0.0942,  0.0035, -0.0674,  0.0545,  0.0103,  0.0951, -0.0714,\n",
       "                        0.1104,  0.1429,  0.2098,  0.0090, -0.1385, -0.0998,  0.0414, -0.0296,\n",
       "                       -0.1348, -0.0299, -0.0456,  0.0436, -0.1333, -0.1151,  0.0985,  0.0405,\n",
       "                        0.0010, -0.0969, -0.0805, -0.1058, -0.1387, -0.0251,  0.0540, -0.0517,\n",
       "                        0.0866,  0.0069, -0.1264, -0.1404, -0.0389,  0.1539, -0.1817,  0.0244,\n",
       "                       -0.2568, -0.1298,  0.0921, -0.0908, -0.0864,  0.0076,  0.0912,  0.1425,\n",
       "                        0.0913,  0.2360,  0.1256, -0.1764,  0.0317, -0.1021, -0.0009,  0.0995,\n",
       "                       -0.0702,  0.0303,  0.1391, -0.1000],\n",
       "                      [-0.0555,  0.1076,  0.1601, -0.0214, -0.0445,  0.0667,  0.0582, -0.1271,\n",
       "                        0.2647, -0.0526,  0.0956, -0.1123,  0.0478, -0.1076, -0.0431,  0.0512,\n",
       "                        0.0517, -0.0190, -0.0626, -0.1239,  0.0572, -0.0167,  0.0934, -0.1538,\n",
       "                       -0.0107,  0.0632, -0.0077,  0.1571,  0.0683,  0.0067,  0.1115, -0.0406,\n",
       "                       -0.0223,  0.1545,  0.1722, -0.0187, -0.0492, -0.0251, -0.0826,  0.0434,\n",
       "                       -0.1048, -0.0659, -0.1394,  0.0954,  0.1529, -0.0573,  0.0216,  0.0499,\n",
       "                        0.0503,  0.0628,  0.1486, -0.0667, -0.1385, -0.0718, -0.0209, -0.1627,\n",
       "                        0.0458,  0.1261,  0.0052,  0.0685,  0.1274, -0.0837,  0.1481, -0.0585,\n",
       "                       -0.2381, -0.0449, -0.0631, -0.1162, -0.1110,  0.0468,  0.0817, -0.1070,\n",
       "                       -0.0872,  0.0092, -0.0714, -0.1090,  0.0504,  0.1128, -0.0156, -0.0520,\n",
       "                       -0.0467,  0.1184,  0.0078, -0.1304]], device='cuda:0')),\n",
       "             ('fc3.bias',\n",
       "              tensor([-0.0224, -0.2128,  0.1264,  0.0707,  0.2093, -0.0901,  0.0636, -0.0390,\n",
       "                       0.1448, -0.1583], device='cuda:0'))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eam_model.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CILM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
