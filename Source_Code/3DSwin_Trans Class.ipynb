{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sparse\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Copyright (c) MONAI Consortium\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from __future__ import annotations\n","import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","import itertools\n","from collections.abc import Sequence\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from torch.nn import LayerNorm\n","from typing_extensions import Final\n","\n","from monai.networks.blocks import MLPBlock as Mlp\n","from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n","from monai.networks.layers import DropPath, trunc_normal_\n","from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n","from monai.utils.deprecate_utils import deprecated_arg\n","\n","rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n","\n","__all__ = [\n","    \"SwinUNETR\",\n","    \"window_partition\",\n","    \"window_reverse\",\n","    \"WindowAttention\",\n","    \"SwinTransformerBlock\",\n","    \"PatchMerging\",\n","    \"PatchMergingV2\",\n","    \"MERGING_MODE\",\n","    \"BasicLayer\",\n","    \"SwinTransformer\",\n","]\n","\n","\n","\n","class SwinUNETR(nn.Module):\n","    \"\"\"\n","    Swin UNETR based on: \"Hatamizadeh et al.,\n","    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n","    <https://arxiv.org/abs/2201.01266>\"\n","    \"\"\"\n","\n","    patch_size: Final[int] = 2\n","\n","    @deprecated_arg(\n","        name=\"img_size\",\n","        since=\"1.3\",\n","        removed=\"1.5\",\n","        msg_suffix=\"The img_size argument is not required anymore and \"\n","        \"checks on the input size are run during forward().\",\n","    )\n","    def __init__(\n","        self,\n","        img_size: Sequence[int] | int,\n","        in_channels: int,\n","        out_channels: int,\n","        depths: Sequence[int] = (2, 2, 2, 2),\n","        num_heads: Sequence[int] = (3, 6, 12, 24),\n","        feature_size: int = 24,\n","        norm_name: tuple | str = \"instance\",\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        dropout_path_rate: float = 0.0,\n","        normalize: bool = True,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","        num_classes = 2\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            img_size: spatial dimension of input image.\n","                This argument is only used for checking that the input image size is divisible by the patch size.\n","                The tensor passed to forward() can have a dynamic shape as long as its spatial dimensions are divisible by 2**5.\n","                It will be removed in an upcoming version.\n","            in_channels: dimension of input channels.\n","            out_channels: dimension of output channels.\n","            feature_size: dimension of network feature size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            norm_name: feature normalization type and arguments.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            dropout_path_rate: drop path rate.\n","            normalize: normalize output intermediate features in each stage.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: number of spatial dims.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beggining of each swin stage.\n","            num_class: number of classes for classification.\n","        Examples::\n","\n","            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n","            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n","\n","            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n","            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n","\n","            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n","            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n","\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        img_size = ensure_tuple_rep(img_size, spatial_dims)\n","        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n","        window_size = ensure_tuple_rep(7, spatial_dims)\n","        #print(patch_sizes,'666')\n","\n","\n","        if spatial_dims not in (2, 3):\n","            raise ValueError(\"spatial dimension should be 2 or 3.\")\n","\n","        self._check_input_size(img_size)\n","\n","        if not (0 <= drop_rate <= 1):\n","            raise ValueError(\"dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= attn_drop_rate <= 1):\n","            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= dropout_path_rate <= 1):\n","            raise ValueError(\"drop path rate should be between 0 and 1.\")\n","\n","        if feature_size % 12 != 0:\n","            raise ValueError(\"feature_size should be divisible by 12.\")\n","\n","        self.normalize = normalize\n","        self.num_class = num_classes\n","        self.final_feature_size = feature_size * 2 * 2 ** (len(depths) - 1)\n","\n","        self.swinViT = SwinTransformer(\n","            in_chans=in_channels,\n","            embed_dim=feature_size,\n","            window_size=window_size,\n","            patch_size=patch_sizes,\n","            depths=depths,\n","            num_heads=num_heads,\n","            mlp_ratio=4.0,\n","            qkv_bias=True,\n","            drop_rate=drop_rate,\n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=dropout_path_rate,\n","            norm_layer=nn.LayerNorm,\n","            use_checkpoint=use_checkpoint,\n","            spatial_dims=spatial_dims,\n","            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n","            use_v2=use_v2,\n","        )\n","\n","        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n","        self.head = nn.Linear(self.final_feature_size, self.num_class)\n","        \"\"\"\n","        self.encoder1 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=in_channels,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder2 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder3 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=2 * feature_size,\n","            out_channels=2 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder4 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=4 * feature_size,\n","            out_channels=4 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder10 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=16 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder5 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=8 * feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder4 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 8,\n","            out_channels=feature_size * 4,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder3 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 4,\n","            out_channels=feature_size * 2,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","        self.decoder2 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 2,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder1 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n","        \"\"\"\n","    def load_from(self, weights):\n","        with torch.no_grad():\n","            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n","            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n","            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers1\")\n","            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers2\")\n","            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers3\")\n","            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers4\")\n","            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n","            )\n","\n","    @torch.jit.unused\n","    def _check_input_size(self, spatial_shape):\n","        img_size = np.array(spatial_shape)\n","        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n","        if remainder.any():\n","            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n","            raise ValueError(\n","                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n","                f\" must be divisible by {self.patch_size}**5.\"\n","            )\n","\n","    def forward(self, x_in):\n","        if not torch.jit.is_scripting():\n","            self._check_input_size(x_in.shape[2:])\n","        hidden_states_out = self.swinViT(x_in, self.normalize)\n","        #print('hidden states out',hidden_states_out.shape)\n","        \n","        \"\"\"\n","        enc0 = self.encoder1(x_in)\n","        enc1 = self.encoder2(hidden_states_out[0])\n","        enc2 = self.encoder3(hidden_states_out[1])\n","        enc3 = self.encoder4(hidden_states_out[2])\n","        dec4 = self.encoder10(hidden_states_out[4])\n","        dec3 = self.decoder5(dec4, hidden_states_out[3])\n","        dec2 = self.decoder4(dec3, enc3)\n","        dec1 = self.decoder3(dec2, enc2)\n","        dec0 = self.decoder2(dec1, enc1)\n","        out = self.decoder1(dec0, enc0)\n","        logits = self.out(out)\n","        return logits\n","        \"\"\"\n","        x = self.avgpool(hidden_states_out)\n","        x = torch.flatten(x, 1)\n","        x = self.head(x)\n","        return x\n","\n","def window_partition(x, window_size):\n","    \"\"\"window partition operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x: input tensor.\n","        window_size: local window size.\n","    \"\"\"\n","    x_shape = x.size()\n","    if len(x_shape) == 5:\n","        b, d, h, w, c = x_shape\n","        x = x.view(\n","            b,\n","            d // window_size[0],\n","            window_size[0],\n","            h // window_size[1],\n","            window_size[1],\n","            w // window_size[2],\n","            window_size[2],\n","            c,\n","        )\n","        windows = (\n","            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n","        )\n","    elif len(x_shape) == 4:\n","        b, h, w, c = x.shape\n","        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n","        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, dims):\n","    \"\"\"window reverse operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        windows: windows tensor.\n","        window_size: local window size.\n","        dims: dimension values.\n","    \"\"\"\n","    if len(dims) == 4:\n","        b, d, h, w = dims\n","        x = windows.view(\n","            b,\n","            d // window_size[0],\n","            h // window_size[1],\n","            w // window_size[2],\n","            window_size[0],\n","            window_size[1],\n","            window_size[2],\n","            -1,\n","        )\n","        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n","\n","    elif len(dims) == 3:\n","        b, h, w = dims\n","        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n","        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n","    return x\n","\n","\n","def get_window_size(x_size, window_size, shift_size=None):\n","    \"\"\"Computing window size based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x_size: input size.\n","        window_size: local window size.\n","        shift_size: window shifting size.\n","    \"\"\"\n","\n","    use_window_size = list(window_size)\n","    if shift_size is not None:\n","        use_shift_size = list(shift_size)\n","    for i in range(len(x_size)):\n","        if x_size[i] <= window_size[i]:\n","            use_window_size[i] = x_size[i]\n","            if shift_size is not None:\n","                use_shift_size[i] = 0\n","\n","    if shift_size is None:\n","        return tuple(use_window_size)\n","    else:\n","        return tuple(use_window_size), tuple(use_shift_size)\n","\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        qkv_bias: bool = False,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            attn_drop: attention dropout rate.\n","            proj_drop: dropout rate of output.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim**-0.5\n","        mesh_args = torch.meshgrid.__kwdefaults__\n","\n","        if len(self.window_size) == 3:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(\n","                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n","                    num_heads,\n","                )\n","            )\n","            coords_d = torch.arange(self.window_size[0])\n","            coords_h = torch.arange(self.window_size[1])\n","            coords_w = torch.arange(self.window_size[2])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 2] += self.window_size[2] - 1\n","            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n","            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n","        elif len(self.window_size) == 2:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","            )\n","            coords_h = torch.arange(self.window_size[0])\n","            coords_w = torch.arange(self.window_size[1])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        b, n, c = x.shape\n","        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.clone()[:n, :n].reshape(-1)\n","        ].reshape(n, n, -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","        if mask is not None:\n","            nw = mask.shape[0]\n","            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, n, n)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn).to(v.dtype)\n","        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        print(x.shape,'after window size')\n","        return x\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer block based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        shift_size: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        act_layer: str = \"GELU\",\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            shift_size: window shift size.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            drop_path: stochastic depth rate.\n","            act_layer: activation layer.\n","            norm_layer: normalization layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        self.use_checkpoint = use_checkpoint\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=self.window_size,\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    def forward_part1(self, x, mask_matrix):\n","        x_shape = x.size()\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = 0\n","            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n","            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","            _, hp, wp, _ = x.shape\n","            dims = [b, hp, wp]\n","\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","        x_windows = window_partition(shifted_x, window_size)\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n","        shifted_x = window_reverse(attn_windows, window_size, dims)\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if len(x_shape) == 5:\n","            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n","                x = x[:, :d, :h, :w, :].contiguous()\n","        elif len(x_shape) == 4:\n","            if pad_r > 0 or pad_b > 0:\n","                x = x[:, :h, :w, :].contiguous()\n","\n","        return x\n","\n","    def forward_part2(self, x):\n","        print(\"all < 0\",torch.all(self.norm2(x)<0))\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","    def load_from(self, weights, n_block, layer):\n","        root = f\"module.{layer}.0.blocks.{n_block}.\"\n","        block_names = [\n","            \"norm1.weight\",\n","            \"norm1.bias\",\n","            \"attn.relative_position_bias_table\",\n","            \"attn.relative_position_index\",\n","            \"attn.qkv.weight\",\n","            \"attn.qkv.bias\",\n","            \"attn.proj.weight\",\n","            \"attn.proj.bias\",\n","            \"norm2.weight\",\n","            \"norm2.bias\",\n","            \"mlp.fc1.weight\",\n","            \"mlp.fc1.bias\",\n","            \"mlp.fc2.weight\",\n","            \"mlp.fc2.bias\",\n","        ]\n","        with torch.no_grad():\n","            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n","            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n","            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n","            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n","            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n","            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n","            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n","            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n","            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n","            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n","            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n","            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n","            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n","            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n","\n","    def forward(self, x, mask_matrix):\n","        shortcut = x\n","        if self.use_checkpoint:\n","            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n","        else:\n","            x = self.forward_part1(x, mask_matrix)\n","        x = shortcut + self.drop_path(x)\n","        if self.use_checkpoint:\n","            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n","        else:\n","            x = x + self.forward_part2(x)\n","        return x\n","\n","\n","class PatchMergingV2(nn.Module):\n","    \"\"\"\n","    Patch merging layer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            norm_layer: normalization layer.\n","            spatial_dims: number of spatial dims.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        if spatial_dims == 3:\n","            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(8 * dim)\n","        elif spatial_dims == 2:\n","            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","            x = torch.cat(\n","                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n","            )\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n","            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","class PatchMerging(PatchMergingV2):\n","    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 4:\n","            return super().forward(x)\n","        if len(x_shape) != 5:\n","            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n","        b, d, h, w, c = x_shape\n","        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","        x0 = x[:, 0::2, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, 0::2, :]\n","        x3 = x[:, 0::2, 0::2, 1::2, :]\n","        x4 = x[:, 1::2, 0::2, 1::2, :]\n","        x5 = x[:, 0::2, 1::2, 0::2, :]\n","        x6 = x[:, 0::2, 0::2, 1::2, :]\n","        x7 = x[:, 1::2, 1::2, 1::2, :]\n","        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n","\n","\n","def compute_mask(dims, window_size, shift_size, device):\n","    \"\"\"Computing region masks based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        dims: dimension values.\n","        window_size: local window size.\n","        shift_size: shift size.\n","        device: device.\n","    \"\"\"\n","\n","    cnt = 0\n","\n","    if len(dims) == 3:\n","        d, h, w = dims\n","        #print('fucking dhw',d,h,w)\n","        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n","        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n","                    img_mask[:, d, h, w, :] = cnt\n","                    cnt += 1\n","\n","    elif len(dims) == 2:\n","        h, w = dims\n","        img_mask = torch.zeros((1, h, w, 1), device=device)\n","        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","    mask_windows = window_partition(img_mask, window_size)\n","    mask_windows = mask_windows.squeeze(-1)\n","    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","    return attn_mask\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        depth: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        drop_path: list,\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = False,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        downsample: nn.Module | None = None,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            depth: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            drop_path: stochastic depth rate.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            norm_layer: normalization layer.\n","            downsample: an optional downsampling layer at the end of the layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.window_size = window_size\n","        #就是除以windowsize,比如56X56 shiftsize就是3X3,因为windowssize 是7X7\n","        self.shift_size = tuple(i // 2 for i in window_size)\n","        self.no_shift = tuple(0 for i in window_size)\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=self.window_size,\n","                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                    use_checkpoint=use_checkpoint,\n","                )\n","                for i in range(depth)\n","            ]\n","        )\n","        #就是patch merging\n","        self.downsample = downsample\n","        if callable(self.downsample):\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, c, d, h, w = x_shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c d h w -> b d h w c\")\n","            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n","            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n","            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n","            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, d, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b d h w c -> b c d h w\")\n","\n","        elif len(x_shape) == 4:\n","            b, c, h, w = x_shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c h w -> b h w c\")\n","            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n","            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n","            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b h w c -> b c h w\")\n","        return x\n","\n","\n","class SwinTransformer(nn.Module):\n","    \"\"\"\n","    Swin Transformer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans: int,\n","        embed_dim: int,\n","        window_size: Sequence[int],\n","        patch_size: Sequence[int],\n","        depths: Sequence[int],\n","        num_heads: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        drop_path_rate: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        patch_norm: bool = False,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            in_chans: dimension of input channels.\n","            embed_dim: number of linear projection output channels.\n","            window_size: local window size.\n","            patch_size: patch size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            drop_path_rate: stochastic depth rate.\n","            norm_layer: normalization layer.\n","            patch_norm: add normalization after patch embedding.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: spatial dimension.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        self.window_size = window_size\n","        self.patch_size = patch_size\n","        self.patch_embed = PatchEmbed(\n","            patch_size=self.patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n","            spatial_dims=spatial_dims,\n","        )\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","        self.use_v2 = use_v2\n","        self.layers1 = nn.ModuleList()\n","        self.layers2 = nn.ModuleList()\n","        self.layers3 = nn.ModuleList()\n","        self.layers4 = nn.ModuleList()\n","        if self.use_v2:\n","            self.layers1c = nn.ModuleList()\n","            self.layers2c = nn.ModuleList()\n","            self.layers3c = nn.ModuleList()\n","            self.layers4c = nn.ModuleList()\n","        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2**i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=self.window_size,\n","                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                norm_layer=norm_layer,\n","                downsample=down_sample_mod,\n","                use_checkpoint=use_checkpoint,\n","            )\n","            if i_layer == 0:\n","                self.layers1.append(layer)\n","            elif i_layer == 1:\n","                self.layers2.append(layer)\n","            elif i_layer == 2:\n","                self.layers3.append(layer)\n","            elif i_layer == 3:\n","                self.layers4.append(layer)\n","            if self.use_v2:\n","                layerc = UnetrBasicBlock(\n","                    spatial_dims=3,\n","                    in_channels=embed_dim * 2**i_layer,\n","                    out_channels=embed_dim * 2**i_layer,\n","                    kernel_size=3,\n","                    stride=1,\n","                    norm_name=\"instance\",\n","                    res_block=True,\n","                )\n","                if i_layer == 0:\n","                    self.layers1c.append(layerc)\n","                elif i_layer == 1:\n","                    self.layers2c.append(layerc)\n","                elif i_layer == 2:\n","                    self.layers3c.append(layerc)\n","                elif i_layer == 3:\n","                    self.layers4c.append(layerc)\n","        #这里的number features跟层数有关系！比如说4层的话，最后的number features就是embed_dim * 2 ** 3\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","\n","    def proj_out(self, x, normalize=False):\n","        if normalize:\n","            x_shape = x.size()\n","            if len(x_shape) == 5:\n","\n","                n, ch, d, h, w = x_shape\n","                x = rearrange(x, \"n c d h w -> n d h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n d h w c -> n c d h w\")\n","            elif len(x_shape) == 4:\n","                n, ch, h, w = x_shape\n","                x = rearrange(x, \"n c h w -> n h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n h w c -> n c h w\")\n","        return x\n","\n","    def forward(self, x, normalize=True):\n","        #print(x.shape,'this is x shape')\n","        x0 = self.patch_embed(x)\n","        #print(x0.shape,'after embed')\n","        x0 = self.pos_drop(x0)\n","        #print(x0.shape,'this is x0 shape')\n","        x0_out = self.proj_out(x0, normalize)\n","        if self.use_v2:\n","            x0 = self.layers1c[0](x0.contiguous())\n","        x1 = self.layers1[0](x0.contiguous())\n","        print(x1.shape,'this is x1 shape')\n","        x1_out = self.proj_out(x1, normalize)\n","        if self.use_v2:\n","            x1 = self.layers2c[0](x1.contiguous())\n","        x2 = self.layers2[0](x1.contiguous())\n","        x2_out = self.proj_out(x2, normalize)\n","        print(x2.shape,'this is x2 shape')\n","        if self.use_v2:\n","            x2 = self.layers3c[0](x2.contiguous())\n","        x3 = self.layers3[0](x2.contiguous())\n","        x3_out = self.proj_out(x3, normalize)\n","        print(x3.shape,'this is x3 shape')\n","        if self.use_v2:\n","            x3 = self.layers4c[0](x3.contiguous())\n","        x4 = self.layers4[0](x3.contiguous())\n","        x4_out = self.proj_out(x4, normalize)\n","        #return [x0_out, x1_out, x2_out, x3_out, x4_out]\n","        return x4_out\n","\n","\n","def filter_swinunetr(key, value):\n","    \"\"\"\n","    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n","    This function is typically used with `monai.networks.copy_model_state`\n","    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n","    <https://arxiv.org/abs/2307.16896>\"\n","\n","    Args:\n","        key: the key in the source state dict used for the update.\n","        value: the value in the source state dict used for the update.\n","\n","    Examples::\n","\n","        import torch\n","        from monai.apps import download_url\n","        from monai.networks.utils import copy_model_state\n","        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n","\n","        model = SwinUNETR(img_size=(96, 96, 96), in_channels=1, out_channels=3, feature_size=48)\n","        resource = (\n","            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n","        )\n","        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n","        download_url(resource, ssl_weights_path)\n","        ssl_weights = torch.load(ssl_weights_path)[\"model\"]\n","\n","        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n","\n","    \"\"\"\n","    if key in [\n","        \"encoder.mask_token\",\n","        \"encoder.norm.weight\",\n","        \"encoder.norm.bias\",\n","        \"out.conv.conv.weight\",\n","        \"out.conv.conv.bias\",\n","    ]:\n","        return None\n","\n","    if key[:8] == \"encoder.\":\n","        if key[8:19] == \"patch_embed\":\n","            new_key = \"swinViT.\" + key[8:]\n","        else:\n","            new_key = \"swinViT.\" + key[8:18] + key[20:]\n","\n","        return new_key, value\n","    else:\n","        return None\n","\n"]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 849425780 bytes.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[202], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m SwinUNETR(img_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, feature_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[201], line 334\u001b[0m, in \u001b[0;36mSwinUNETR.forward\u001b[1;34m(self, x_in)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input_size(x_in\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m--> 334\u001b[0m hidden_states_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswinViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;66;03m#print('hidden states out',hidden_states_out.shape)\u001b[39;00m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03menc0 = self.encoder1(x_in)\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03menc1 = self.encoder2(hidden_states_out[0])\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03mreturn logits\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[201], line 1092\u001b[0m, in \u001b[0;36mSwinTransformer.forward\u001b[1;34m(self, x, normalize)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_v2:\n\u001b[0;32m   1091\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers1c[\u001b[38;5;241m0\u001b[39m](x0\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m-> 1092\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28mprint\u001b[39m(x1\u001b[38;5;241m.\u001b[39mshape,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis is x1 shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1094\u001b[0m x1_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x1, normalize)\n","File \u001b[1;32mc:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[201], line 922\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    920\u001b[0m hp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(h \u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m*\u001b[39m window_size[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    921\u001b[0m wp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(w \u001b[38;5;241m/\u001b[39m window_size[\u001b[38;5;241m2\u001b[39m])) \u001b[38;5;241m*\u001b[39m window_size[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 922\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m    924\u001b[0m     x \u001b[38;5;241m=\u001b[39m blk(x, attn_mask)\n","Cell \u001b[1;32mIn[201], line 839\u001b[0m, in \u001b[0;36mcompute_mask\u001b[1;34m(dims, window_size, shift_size, device)\u001b[0m\n\u001b[0;32m    837\u001b[0m mask_windows \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    838\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m mask_windows\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 839\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmasked_fill(attn_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.0\u001b[39m))\n\u001b[0;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_mask\n","\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 849425780 bytes."]}],"source":["model = SwinUNETR(img_size=(64, 256, 256), in_channels=1, out_channels=1, feature_size=48)\n","test_data = torch.randn(1, 1, 64, 256, 256)\n","model(test_data)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Copyright (c) MONAI Consortium\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\n","from __future__ import annotations\n","\n","import itertools\n","from collections.abc import Sequence\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from torch.nn import LayerNorm\n","from typing_extensions import Final\n","\n","from monai.networks.blocks import MLPBlock as Mlp\n","from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n","from monai.networks.layers import DropPath, trunc_normal_\n","from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n","from monai.utils.deprecate_utils import deprecated_arg\n","\n","rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n","\n","__all__ = [\n","    \"SwinUNETR\",\n","    \"window_partition\",\n","    \"window_reverse\",\n","    \"WindowAttention\",\n","    \"SwinTransformerBlock\",\n","    \"PatchMerging\",\n","    \"PatchMergingV2\",\n","    \"MERGING_MODE\",\n","    \"BasicLayer\",\n","    \"SwinTransformer\",\n","]\n","\n","\n","\n","class SwinUNETR(nn.Module):\n","    \"\"\"\n","    Swin UNETR based on: \"Hatamizadeh et al.,\n","    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n","    <https://arxiv.org/abs/2201.01266>\"\n","    \"\"\"\n","\n","    patch_size: Final[int] = 2\n","\n","    @deprecated_arg(\n","        name=\"img_size\",\n","        since=\"1.3\",\n","        removed=\"1.5\",\n","        msg_suffix=\"The img_size argument is not required anymore and \"\n","        \"checks on the input size are run during forward().\",\n","    )\n","    def __init__(\n","        self,\n","        img_size: Sequence[int] | int,\n","        in_channels: int,\n","        out_channels: int,\n","        depths: Sequence[int] = (3, 3, 3, 3),\n","        num_heads: Sequence[int] = (3, 6, 12, 24),\n","        feature_size: int = 24,\n","        norm_name: tuple | str = \"instance\",\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        dropout_path_rate: float = 0.0,\n","        normalize: bool = True,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","        num_classes = 2,\n","        interval = [(4,16),(2,8),(2,4),(2,4)]\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            img_size: spatial dimension of input image.\n","                This argument is only used for checking that the input image size is divisible by the patch size.\n","                The tensor passed to forward() can have a dynamic shape as long as its spatial dimensions are divisible by 2**5.\n","                It will be removed in an upcoming version.\n","            in_channels: dimension of input channels.\n","            out_channels: dimension of output channels.\n","            feature_size: dimension of network feature size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            norm_name: feature normalization type and arguments.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            dropout_path_rate: drop path rate.\n","            normalize: normalize output intermediate features in each stage.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: number of spatial dims.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beggining of each swin stage.\n","            num_class: number of classes for classification.\n","        Examples::\n","\n","            # for 3D single channel input with size (96,96,96), 4-channel output and feature size of 48.\n","            >>> net = SwinUNETR(img_size=(96,96,96), in_channels=1, out_channels=4, feature_size=48)\n","\n","            # for 3D 4-channel input with size (128,128,128), 3-channel output and (2,4,2,2) layers in each stage.\n","            >>> net = SwinUNETR(img_size=(128,128,128), in_channels=4, out_channels=3, depths=(2,4,2,2))\n","\n","            # for 2D single channel input with size (96,96), 2-channel output and gradient checkpointing.\n","            >>> net = SwinUNETR(img_size=(96,96), in_channels=3, out_channels=2, use_checkpoint=True, spatial_dims=2)\n","\n","        \"\"\"\n","\n","        super().__init__()\n","\n","        img_size = ensure_tuple_rep(img_size, spatial_dims)\n","        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n","        window_size = ensure_tuple_rep(7, spatial_dims)\n","        #print(patch_sizes,'666')\n","        d,h,w = img_size[0],img_size[1],img_size[2]\n","        sparse_window_size = []\n","        #calculatye sparse size\n","        for i in range(len(depths)):\n","            G_d = d//(2**(i+1) * interval[i][0])\n","            G_h = h//(2**(i+1) * interval[i][1])\n","            G_w = w//(2**(i+1) * interval[i][1])\n","            sparse_window_size.append((G_d,G_h,G_w))\n","        print('fuck sparse',sparse_window_size)\n","        if spatial_dims not in (2, 3):\n","            raise ValueError(\"spatial dimension should be 2 or 3.\")\n","\n","        self._check_input_size(img_size)\n","\n","        if not (0 <= drop_rate <= 1):\n","            raise ValueError(\"dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= attn_drop_rate <= 1):\n","            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n","\n","        if not (0 <= dropout_path_rate <= 1):\n","            raise ValueError(\"drop path rate should be between 0 and 1.\")\n","\n","        if feature_size % 12 != 0:\n","            raise ValueError(\"feature_size should be divisible by 12.\")\n","\n","        self.normalize = normalize\n","        self.num_class = num_classes\n","        self.final_feature_size = feature_size * 2 * 2 ** (len(depths) - 1)\n","\n","        self.swinViT = SwinTransformer(\n","            in_chans=in_channels,\n","            embed_dim=feature_size,\n","            window_size=window_size,\n","            patch_size=patch_sizes,\n","            depths=depths,\n","            num_heads=num_heads,\n","            mlp_ratio=4.0,\n","            qkv_bias=True,\n","            drop_rate=drop_rate,\n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=dropout_path_rate,\n","            norm_layer=nn.LayerNorm,\n","            use_checkpoint=use_checkpoint,\n","            spatial_dims=spatial_dims,\n","            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n","            use_v2=use_v2,\n","            interval = interval,\n","            sparse_window_size = sparse_window_size\n","        )\n","\n","        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n","        self.head = nn.Linear(self.final_feature_size, self.num_class)\n","        \"\"\"\n","        self.encoder1 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=in_channels,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder2 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder3 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=2 * feature_size,\n","            out_channels=2 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder4 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=4 * feature_size,\n","            out_channels=4 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.encoder10 = UnetrBasicBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=16 * feature_size,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder5 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=16 * feature_size,\n","            out_channels=8 * feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder4 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 8,\n","            out_channels=feature_size * 4,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder3 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 4,\n","            out_channels=feature_size * 2,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","        self.decoder2 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size * 2,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.decoder1 = UnetrUpBlock(\n","            spatial_dims=spatial_dims,\n","            in_channels=feature_size,\n","            out_channels=feature_size,\n","            kernel_size=3,\n","            upsample_kernel_size=2,\n","            norm_name=norm_name,\n","            res_block=True,\n","        )\n","\n","        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n","        \"\"\"\n","    def load_from(self, weights):\n","        with torch.no_grad():\n","            self.swinViT.patch_embed.proj.weight.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.weight\"])\n","            self.swinViT.patch_embed.proj.bias.copy_(weights[\"state_dict\"][\"module.patch_embed.proj.bias\"])\n","            for bname, block in self.swinViT.layers1[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers1\")\n","            self.swinViT.layers1[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers1[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers1.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers2[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers2\")\n","            self.swinViT.layers2[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers2[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers2.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers3[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers3\")\n","            self.swinViT.layers3[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers3[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers3.0.downsample.norm.bias\"]\n","            )\n","            for bname, block in self.swinViT.layers4[0].blocks.named_children():\n","                block.load_from(weights, n_block=bname, layer=\"layers4\")\n","            self.swinViT.layers4[0].downsample.reduction.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.reduction.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.weight.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.weight\"]\n","            )\n","            self.swinViT.layers4[0].downsample.norm.bias.copy_(\n","                weights[\"state_dict\"][\"module.layers4.0.downsample.norm.bias\"]\n","            )\n","\n","    @torch.jit.unused\n","    def _check_input_size(self, spatial_shape):\n","        img_size = np.array(spatial_shape)\n","        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n","        if remainder.any():\n","            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n","            raise ValueError(\n","                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n","                f\" must be divisible by {self.patch_size}**5.\"\n","            )\n","\n","    def forward(self, x_in):\n","        if not torch.jit.is_scripting():\n","            self._check_input_size(x_in.shape[2:])\n","        hidden_states_out = self.swinViT(x_in, self.normalize)\n","        print('hidden states out',hidden_states_out.shape)\n","        \n","        \"\"\"\n","        enc0 = self.encoder1(x_in)\n","        enc1 = self.encoder2(hidden_states_out[0])\n","        enc2 = self.encoder3(hidden_states_out[1])\n","        enc3 = self.encoder4(hidden_states_out[2])\n","        dec4 = self.encoder10(hidden_states_out[4])\n","        dec3 = self.decoder5(dec4, hidden_states_out[3])\n","        dec2 = self.decoder4(dec3, enc3)\n","        dec1 = self.decoder3(dec2, enc2)\n","        dec0 = self.decoder2(dec1, enc1)\n","        out = self.decoder1(dec0, enc0)\n","        logits = self.out(out)\n","        return logits\n","        \"\"\"\n","        x = self.avgpool(hidden_states_out)\n","        print('avgpool',x.shape)\n","        x = torch.flatten(x, 1)\n","        print('flatten',x.shape)\n","        x = self.head(x)\n","        print('head',x.shape)\n","        return x\n","\n","def window_partition(x, window_size):\n","    \"\"\"window partition operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x: input tensor.\n","        window_size: local window size.\n","    \"\"\"\n","    x_shape = x.size()\n","    if len(x_shape) == 5:\n","        b, d, h, w, c = x_shape\n","        x = x.view(\n","            b,\n","            d // window_size[0],\n","            window_size[0],\n","            h // window_size[1],\n","            window_size[1],\n","            w // window_size[2],\n","            window_size[2],\n","            c,\n","        )\n","        windows = (\n","            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n","        )\n","    elif len(x_shape) == 4:\n","        b, h, w, c = x.shape\n","        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n","        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n","    print(windows.shape,'this is windows shape in partiton')\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, dims):\n","    \"\"\"window reverse operation based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        windows: windows tensor.\n","        window_size: local window size.\n","        dims: dimension values.\n","    \"\"\"\n","    if len(dims) == 4:\n","        b, d, h, w = dims\n","        x = windows.view(\n","            b,\n","            d // window_size[0],\n","            h // window_size[1],\n","            w // window_size[2],\n","            window_size[0],\n","            window_size[1],\n","            window_size[2],\n","            -1,\n","        )\n","        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n","\n","    elif len(dims) == 3:\n","        b, h, w = dims\n","        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n","        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n","    return x\n","\n","\n","def get_window_size(x_size, window_size, shift_size=None):\n","    \"\"\"Computing window size based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        x_size: input size.\n","        window_size: local window size.\n","        shift_size: window shifting size.\n","    \"\"\"\n","\n","    use_window_size = list(window_size)\n","    if shift_size is not None:\n","        use_shift_size = list(shift_size)\n","    for i in range(len(x_size)):\n","        if x_size[i] <= window_size[i]:\n","            use_window_size[i] = x_size[i]\n","            if shift_size is not None:\n","                use_shift_size[i] = 0\n","\n","    if shift_size is None:\n","        return tuple(use_window_size)\n","    else:\n","        return tuple(use_window_size), tuple(use_shift_size)\n","\n","\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        qkv_bias: bool = False,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            attn_drop: attention dropout rate.\n","            proj_drop: dropout rate of output.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim**-0.5\n","        mesh_args = torch.meshgrid.__kwdefaults__\n","\n","        if len(self.window_size) == 3:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(\n","                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n","                    num_heads,\n","                )\n","            )\n","            coords_d = torch.arange(self.window_size[0])\n","            coords_h = torch.arange(self.window_size[1])\n","            coords_w = torch.arange(self.window_size[2])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 2] += self.window_size[2] - 1\n","            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n","            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n","        elif len(self.window_size) == 2:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","            )\n","            coords_h = torch.arange(self.window_size[0])\n","            coords_w = torch.arange(self.window_size[1])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        b, n, c = x.shape\n","        print(x.shape,'this is x shape in attention')\n","        print(self.qkv(x).shape,'this is qkv shape in attention')\n","        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.clone()[:n, :n].reshape(-1)\n","        ].reshape(n, n, -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","        if mask is not None:\n","            nw = mask.shape[0]\n","            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, n, n)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn).to(v.dtype)\n","        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        #print(x.shape,'this is x shape in attention')\n","        return x\n","\n","class SparseBlock(nn.Module):\n","    def __init__(self,\n","                 dim: int,\n","                 num_heads: int,\n","                 window_size: Sequence[int],\n","                 interval: tuple,\n","                 mlp_ratio: float = 4.0,\n","                 qkv_bias: bool = True,\n","                 attn_drop: float = 0.0,\n","                 drop: float = 0.0,\n","                 drop_path: float = 0.0,\n","                 act_layer: str = \"GELU\",\n","                 norm_layer: type[LayerNorm] = nn.LayerNorm,\n","                 ):\n","        \"\"\"\n","        args:\n","            interval: the interval of the sparse block, the first element is the interval of the depth, the second is the interval of the size\n","\n","        \"\"\"\n","        super(SparseBlock, self).__init__()\n","        self.window_size = window_size\n","        self.interval = interval\n","        self.depth_interval = interval[0]\n","        self.size_interval = interval[1]\n","        self.norm1 = norm_layer(dim)\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.mlp_ratio = mlp_ratio\n","        self.attn = WindowAttention(dim,\n","                                    window_size=window_size,\n","                                    num_heads=num_heads,\n","                                    qkv_bias=qkv_bias,\n","                                    attn_drop=attn_drop,\n","                                    proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    \n","    def forward_part1(self,x):\n","        x_shape = x.size()\n","        print('x shape',x_shape)\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            I_d, I_size, G_d, G_h, G_w = self.depth_interval, self.size_interval, d//self.depth_interval, h//self.size_interval, w//self.size_interval\n","\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (self.window_size[0] - d % self.window_size[0]) % self.window_size[0]\n","            pad_b = (self.window_size[1] - h % self.window_size[1]) % self.window_size[1]\n","            pad_r = (self.window_size[2] - w % self.window_size[2]) % self.window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","\n","\n","\n","\n","\n","        I_d, I_size, G_d, G_h, G_w = self.depth_interval, self.size_interval, d//self.depth_interval, h//self.size_interval, w//self.size_interval\n","        x_windows = x.reshape(b, G_d, I_d, G_h, I_size,  G_w, I_size, c).permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n","        #x = x.reshape(B*I_d*I_size*I_size,G_d,G_h,G_w,C)\n","        x_windows = x.reshape(b*I_d*I_size*I_size,G_d*G_h*G_w,c)\n","        print(x_windows.shape,'this is x shape sparse')\n","        attn_mask = None\n","\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(b,I_d,I_size,I_size,G_d,G_h,G_w,c).permute(0,4,1,5,2,6,3,7).contiguous()\n","        attn_windows = attn_windows.reshape(b,G_d*I_d,G_h*I_size,G_w*I_size,c)\n","        print(attn_windows.shape,'atten_shape')\n","        return x\n","    \n","\n","\n","\n","\n","    def forward_part2(self, x):\n","        #print(\"all < 0\",torch.all(self.norm2(x)<0))\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","    def forward(self, x, mask_matrix):\n","        x = self.forward_part1(x)\n","        x = self.forward_part2(x)\n","        return x\n","\n","class SwinTransformerBlock(nn.Module):\n","    \"\"\"\n","    Swin Transformer block based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        shift_size: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        drop_path: float = 0.0,\n","        act_layer: str = \"GELU\",\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        use_checkpoint: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            shift_size: window shift size.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            drop_path: stochastic depth rate.\n","            act_layer: activation layer.\n","            norm_layer: normalization layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        self.use_checkpoint = use_checkpoint\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=self.window_size,\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            attn_drop=attn_drop,\n","            proj_drop=drop,\n","        )\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    def forward_part1(self, x, mask_matrix):\n","        x_shape = x.size()\n","        print('x shape',x_shape)\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            print(window_size,'this is window size',self.window_size,'this is self window size')\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","            print(window_size,'this is window size',self.window_size,'this is self window size')\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = 0\n","            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n","            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","            _, hp, wp, _ = x.shape\n","            dims = [b, hp, wp]\n","\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","        x_windows = window_partition(shifted_x, window_size)\n","\n","\n","\n","        print(x_windows.shape,'x_windows')\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n","        shifted_x = window_reverse(attn_windows, window_size, dims)\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if len(x_shape) == 5:\n","            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n","                x = x[:, :d, :h, :w, :].contiguous()\n","        elif len(x_shape) == 4:\n","            if pad_r > 0 or pad_b > 0:\n","                x = x[:, :h, :w, :].contiguous()\n","\n","        return x\n","\n","    def forward_part2(self, x):\n","        #print(\"all < 0\",torch.all(self.norm2(x)<0))\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","    def load_from(self, weights, n_block, layer):\n","        root = f\"module.{layer}.0.blocks.{n_block}.\"\n","        block_names = [\n","            \"norm1.weight\",\n","            \"norm1.bias\",\n","            \"attn.relative_position_bias_table\",\n","            \"attn.relative_position_index\",\n","            \"attn.qkv.weight\",\n","            \"attn.qkv.bias\",\n","            \"attn.proj.weight\",\n","            \"attn.proj.bias\",\n","            \"norm2.weight\",\n","            \"norm2.bias\",\n","            \"mlp.fc1.weight\",\n","            \"mlp.fc1.bias\",\n","            \"mlp.fc2.weight\",\n","            \"mlp.fc2.bias\",\n","        ]\n","        with torch.no_grad():\n","            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n","            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n","            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n","            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])\n","            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n","            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n","            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n","            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n","            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n","            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n","            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n","            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n","            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n","            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n","\n","    def forward(self, x, mask_matrix):\n","        print('run bitch')\n","        shortcut = x\n","        if self.use_checkpoint:\n","            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n","        else:\n","            x = self.forward_part1(x, mask_matrix)\n","        x = shortcut + self.drop_path(x)\n","        if self.use_checkpoint:\n","            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n","        else:\n","            x = x + self.forward_part2(x)\n","        print('sbsbsb',x.shape)\n","        return x\n","\n","\n","class PatchMergingV2(nn.Module):\n","    \"\"\"\n","    Patch merging layer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            norm_layer: normalization layer.\n","            spatial_dims: number of spatial dims.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        if spatial_dims == 3:\n","            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(8 * dim)\n","        elif spatial_dims == 2:\n","            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","            self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","            x = torch.cat(\n","                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n","            )\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x_shape\n","            pad_input = (h % 2 == 1) or (w % 2 == 1)\n","            if pad_input:\n","                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n","            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","class PatchMerging(PatchMergingV2):\n","    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 4:\n","            return super().forward(x)\n","        if len(x_shape) != 5:\n","            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n","        b, d, h, w, c = x_shape\n","        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n","        if pad_input:\n","            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n","        x0 = x[:, 0::2, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, 0::2, :]\n","        x3 = x[:, 0::2, 0::2, 1::2, :]\n","        x4 = x[:, 1::2, 0::2, 1::2, :]\n","        x5 = x[:, 0::2, 1::2, 0::2, :]\n","        x6 = x[:, 0::2, 0::2, 1::2, :]\n","        x7 = x[:, 1::2, 1::2, 1::2, :]\n","        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","        return x\n","\n","\n","MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n","\n","\n","def compute_mask(dims, window_size, shift_size, device):\n","    \"\"\"Computing region masks based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","\n","     Args:\n","        dims: dimension values.\n","        window_size: local window size.\n","        shift_size: shift size.\n","        device: device.\n","    \"\"\"\n","\n","    cnt = 0\n","\n","    if len(dims) == 3:\n","        d, h, w = dims\n","        #print('fucking dhw',d,h,w)\n","        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n","        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n","                    img_mask[:, d, h, w, :] = cnt\n","                    cnt += 1\n","\n","    elif len(dims) == 2:\n","        h, w = dims\n","        img_mask = torch.zeros((1, h, w, 1), device=device)\n","        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n","            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","    mask_windows = window_partition(img_mask, window_size)\n","    mask_windows = mask_windows.squeeze(-1)\n","    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","    return attn_mask\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\"\n","    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        depth: int,\n","        num_heads: int,\n","        interval: tuple,\n","        window_size: Sequence[int],\n","        sparse_window_size: Sequence[int],\n","        drop_path: list,\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = False,\n","        drop: float = 0.0,\n","        attn_drop: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        downsample: nn.Module | None = None,\n","        use_checkpoint: bool = False,\n","        \n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            depth: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            drop_path: stochastic depth rate.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop: dropout rate.\n","            attn_drop: attention dropout rate.\n","            norm_layer: normalization layer.\n","            downsample: an optional downsampling layer at the end of the layer.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            interval: the interval of the sparse block, the first element is the interval of the depth, the second is the interval of the size\n","        \"\"\"\n","\n","        super().__init__()\n","        self.window_size = window_size\n","        #就是除以windowsize,比如56X56 shiftsize就是3X3,因为windowssize 是7X7\n","        self.shift_size = tuple(i // 2 for i in window_size)\n","        self.no_shift = tuple(0 for i in window_size)\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","        self.interval = interval\n","        self.sparse_window_size = sparse_window_size\n","        self.blocks = nn.ModuleList(\n","            [\n","                SwinTransformerBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    window_size=self.window_size,\n","                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                    norm_layer=norm_layer,\n","                    use_checkpoint=use_checkpoint,\n","                )\n","                for i in range(depth)\n","            ])\n","        print('droppath',drop_path)\n","        self.blocks.append(SparseBlock(\n","                    dim=dim,\n","                    num_heads=num_heads,\n","                    interval = interval,\n","                    window_size=sparse_window_size,\n","                    mlp_ratio=mlp_ratio,\n","                    qkv_bias=qkv_bias,\n","                    drop=drop,\n","                    attn_drop=attn_drop,\n","                    norm_layer=norm_layer,\n","                    drop_path = drop_path[2] if isinstance(drop_path, list) else drop_path\n","                    ))\n","        #就是patch merging\n","        self.downsample = downsample\n","        if callable(self.downsample):\n","            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n","\n","    def forward(self, x):\n","        x_shape = x.size()\n","        if len(x_shape) == 5:\n","            b, c, d, h, w = x_shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c d h w -> b d h w c\")\n","            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n","            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n","            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n","            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n","            \n","            #print(len(self.blocks),'longe of blco')\n","            print(shift_size,'this is shift size')\n","            for blk in self.blocks:\n","\n","                x = blk(x, attn_mask)\n","                \n","            x = x.view(b, d, h, w, -1)\n","            print(x.shape,'this is x shape after three blocks in each layer')\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b d h w c -> b c d h w\")\n","        \n","\n","        elif len(x_shape) == 4:\n","            b, c, h, w = x_shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            x = rearrange(x, \"b c h w -> b h w c\")\n","            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n","            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n","            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n","            for blk in self.blocks:\n","                x = blk(x, attn_mask)\n","            x = x.view(b, h, w, -1)\n","            if self.downsample is not None:\n","                x = self.downsample(x)\n","            x = rearrange(x, \"b h w c -> b c h w\")\n","        return x\n","\n","\n","class SwinTransformer(nn.Module):\n","    \"\"\"\n","    Swin Transformer based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        in_chans: int,\n","        embed_dim: int,\n","        window_size: Sequence[int],\n","        patch_size: Sequence[int],\n","        interval: Sequence[tuple],\n","        sparse_window_size: Sequence[tuple],\n","        depths: Sequence[int],\n","        num_heads: Sequence[int],\n","        mlp_ratio: float = 4.0,\n","        qkv_bias: bool = True,\n","        drop_rate: float = 0.0,\n","        attn_drop_rate: float = 0.0,\n","        drop_path_rate: float = 0.0,\n","        norm_layer: type[LayerNorm] = nn.LayerNorm,\n","        patch_norm: bool = False,\n","        use_checkpoint: bool = False,\n","        spatial_dims: int = 3,\n","        downsample=\"merging\",\n","        use_v2=False,\n","       \n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            in_chans: dimension of input channels.\n","            embed_dim: number of linear projection output channels.\n","            window_size: local window size.\n","            patch_size: patch size.\n","            depths: number of layers in each stage.\n","            num_heads: number of attention heads.\n","            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            drop_rate: dropout rate.\n","            attn_drop_rate: attention dropout rate.\n","            drop_path_rate: stochastic depth rate.\n","            norm_layer: normalization layer.\n","            patch_norm: add normalization after patch embedding.\n","            use_checkpoint: use gradient checkpointing for reduced memory usage.\n","            spatial_dims: spatial dimension.\n","            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n","                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n","                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n","            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.patch_norm = patch_norm\n","        self.window_size = window_size\n","        self.patch_size = patch_size\n","        self.patch_embed = PatchEmbed(\n","            patch_size=self.patch_size,\n","            in_chans=in_chans,\n","            embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n","            spatial_dims=spatial_dims,\n","        )\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n","        self.use_v2 = use_v2\n","        self.layers1 = nn.ModuleList()\n","        self.layers2 = nn.ModuleList()\n","        self.layers3 = nn.ModuleList()\n","        self.layers4 = nn.ModuleList()\n","        if self.use_v2:\n","            self.layers1c = nn.ModuleList()\n","            self.layers2c = nn.ModuleList()\n","            self.layers3c = nn.ModuleList()\n","            self.layers4c = nn.ModuleList()\n","        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n","        print(self.num_layers,'this is num layers')\n","        for i_layer in range(self.num_layers):\n","            layer = BasicLayer(\n","                dim=int(embed_dim * 2**i_layer),\n","                depth=depths[i_layer],\n","                num_heads=num_heads[i_layer],\n","                window_size=self.window_size,\n","                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n","                mlp_ratio=mlp_ratio,\n","                qkv_bias=qkv_bias,\n","                drop=drop_rate,\n","                attn_drop=attn_drop_rate,\n","                norm_layer=norm_layer,\n","                downsample=down_sample_mod,\n","                use_checkpoint=use_checkpoint,\n","                interval=interval[i_layer],\n","                sparse_window_size=sparse_window_size[i_layer],\n","                \n","            )\n","            if i_layer == 0:\n","                self.layers1.append(layer)\n","            elif i_layer == 1:\n","                self.layers2.append(layer)\n","            elif i_layer == 2:\n","                self.layers3.append(layer)\n","            elif i_layer == 3:\n","                self.layers4.append(layer)\n","            if self.use_v2:\n","                layerc = UnetrBasicBlock(\n","                    spatial_dims=3,\n","                    in_channels=embed_dim * 2**i_layer,\n","                    out_channels=embed_dim * 2**i_layer,\n","                    kernel_size=3,\n","                    stride=1,\n","                    norm_name=\"instance\",\n","                    res_block=True,\n","                )\n","                if i_layer == 0:\n","                    self.layers1c.append(layerc)\n","                elif i_layer == 1:\n","                    self.layers2c.append(layerc)\n","                elif i_layer == 2:\n","                    self.layers3c.append(layerc)\n","                elif i_layer == 3:\n","                    self.layers4c.append(layerc)\n","        #这里的number features跟层数有关系！比如说4层的话，最后的number features就是embed_dim * 2 ** 3\n","        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n","\n","    def proj_out(self, x, normalize=False):\n","        if normalize:\n","            x_shape = x.size()\n","            if len(x_shape) == 5:\n","\n","                n, ch, d, h, w = x_shape\n","                x = rearrange(x, \"n c d h w -> n d h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n d h w c -> n c d h w\")\n","            elif len(x_shape) == 4:\n","                n, ch, h, w = x_shape\n","                x = rearrange(x, \"n c h w -> n h w c\")\n","                x = F.layer_norm(x, [ch])\n","                x = rearrange(x, \"n h w c -> n c h w\")\n","        return x\n","\n","    def forward(self, x, normalize=True):\n","        #print(x.shape,'this is x shape')\n","        x0 = self.patch_embed(x)\n","        #print(x0.shape,'after embed')\n","        x0 = self.pos_drop(x0)\n","        #print(x0.shape,'this is x0 shape')\n","        x0_out = self.proj_out(x0, normalize)\n","        print(x0_out.shape,'this is x0_out shape')\n","        if self.use_v2:\n","            x0 = self.layers1c[0](x0.contiguous())\n","        x1 = self.layers1[0](x0.contiguous())\n","        print(x1.shape,'this is x1 shape')\n","        x1_out = self.proj_out(x1, normalize)\n","        if self.use_v2:\n","            x1 = self.layers2c[0](x1.contiguous())\n","        x2 = self.layers2[0](x1.contiguous())\n","        x2_out = self.proj_out(x2, normalize)\n","        #print(x2.shape,'this is x2 shape')\n","        if self.use_v2:\n","            x2 = self.layers3c[0](x2.contiguous())\n","        x3 = self.layers3[0](x2.contiguous())\n","        x3_out = self.proj_out(x3, normalize)\n","        if self.use_v2:\n","            x3 = self.layers4c[0](x3.contiguous())\n","        x4 = self.layers4[0](x3.contiguous())\n","        x4_out = self.proj_out(x4, normalize)\n","        #return [x0_out, x1_out, x2_out, x3_out, x4_out]\n","        return x4_out\n","\n","\n","def filter_swinunetr(key, value):\n","    \"\"\"\n","    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n","    This function is typically used with `monai.networks.copy_model_state`\n","    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n","    <https://arxiv.org/abs/2307.16896>\"\n","\n","    Args:\n","        key: the key in the source state dict used for the update.\n","        value: the value in the source state dict used for the update.\n","\n","    Examples::\n","\n","        import torch\n","        from monai.apps import download_url\n","        from monai.networks.utils import copy_model_state\n","        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n","\n","        model = SwinUNETR(img_size=(96, 96, 96), in_channels=1, out_channels=3, feature_size=48)\n","        resource = (\n","            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n","        )\n","        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n","        download_url(resource, ssl_weights_path)\n","        ssl_weights = torch.load(ssl_weights_path)[\"model\"]\n","\n","        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n","\n","    \"\"\"\n","    if key in [\n","        \"encoder.mask_token\",\n","        \"encoder.norm.weight\",\n","        \"encoder.norm.bias\",\n","        \"out.conv.conv.weight\",\n","        \"out.conv.conv.bias\",\n","    ]:\n","        return None\n","\n","    if key[:8] == \"encoder.\":\n","        if key[8:19] == \"patch_embed\":\n","            new_key = \"swinViT.\" + key[8:]\n","        else:\n","            new_key = \"swinViT.\" + key[8:18] + key[20:]\n","\n","        return new_key, value\n","    else:\n","        return None"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["x shape torch.Size([1, 8, 8, 8, 96])\n","torch.Size([32, 16, 96]) this is x shape sparse\n","torch.Size([32, 16, 96]) this is x shape in attention\n","torch.Size([32, 16, 288]) this is qkv shape in attention\n","torch.Size([1, 8, 8, 8, 96]) atten_shape\n"]},{"data":{"text/plain":["torch.Size([1, 8, 8, 8, 96])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["#Test Sparse\n","sp = SparseBlock(96,12,(4,2,2),(2,4))\n","test_data = torch.randn(1,8,8,8,96)\n","sp(test_data,None).shape"]},{"cell_type":"markdown","metadata":{},"source":["For classification"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["run bitch\n","x shape torch.Size([1, 8, 8, 8, 96])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([8, 343, 96]) this is windows shape in partiton\n","torch.Size([8, 343, 96]) x_windows\n","torch.Size([8, 343, 96]) this is x shape in attention\n","torch.Size([8, 343, 288]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 8, 8, 8, 96])\n"]},{"data":{"text/plain":["torch.Size([1, 8, 8, 8, 96])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["sw = SwinTransformerBlock(96,12,(7,7,7),[0])\n","sw(test_data,None).shape"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["import torch\n","test_data = torch.arange(0,512).reshape(1,1,8,8,8)\n","test_data_reshape = test_data.reshape(1,4,2,2,4,2,4,1)\n","test_data_after = test_data_reshape.permute(0,2,4,6,1,3,5,7).reshape(32,4,2,2)\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[  0,   4],\n","         [ 32,  36]],\n","\n","        [[128, 132],\n","         [160, 164]],\n","\n","        [[256, 260],\n","         [288, 292]],\n","\n","        [[384, 388],\n","         [416, 420]]])"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["test_data[0,0,0::2,0::4,0::4]"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([1, 4, 2, 2, 4, 2, 4, 96])"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["test_data_reshape.shape"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["fuck sparse [(8, 8, 8), (8, 8, 8), (4, 8, 8), (2, 4, 4)]\n","4 this is num layers\n","droppath [0.0, 0.0, 0.0]\n","droppath [0.0, 0.0, 0.0]\n","droppath [0.0, 0.0, 0.0]\n","droppath [0.0, 0.0, 0.0]\n","torch.Size([1, 48, 32, 128, 128]) this is x0_out shape\n","torch.Size([1805, 343, 1]) this is windows shape in partiton\n","(3, 3, 3) this is shift size\n","run bitch\n","x shape torch.Size([1, 32, 128, 128, 48])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([1805, 343, 48]) this is windows shape in partiton\n","torch.Size([1805, 343, 48]) x_windows\n","torch.Size([1805, 343, 48]) this is x shape in attention\n","torch.Size([1805, 343, 144]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 32, 128, 128, 48])\n","run bitch\n","x shape torch.Size([1, 32, 128, 128, 48])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([1805, 343, 48]) this is windows shape in partiton\n","torch.Size([1805, 343, 48]) x_windows\n","torch.Size([1805, 343, 48]) this is x shape in attention\n","torch.Size([1805, 343, 144]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 32, 128, 128, 48])\n","run bitch\n","x shape torch.Size([1, 32, 128, 128, 48])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([1805, 343, 48]) this is windows shape in partiton\n","torch.Size([1805, 343, 48]) x_windows\n","torch.Size([1805, 343, 48]) this is x shape in attention\n","torch.Size([1805, 343, 144]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 32, 128, 128, 48])\n","x shape torch.Size([1, 32, 128, 128, 48])\n","torch.Size([1024, 512, 48]) this is x shape sparse\n","torch.Size([1024, 512, 48]) this is x shape in attention\n","torch.Size([1024, 512, 144]) this is qkv shape in attention\n","torch.Size([1, 32, 128, 128, 48]) atten_shape\n","torch.Size([1, 32, 128, 128, 48]) this is x shape after three blocks in each layer\n","torch.Size([1, 96, 16, 64, 64]) this is x1 shape\n","torch.Size([300, 343, 1]) this is windows shape in partiton\n","(3, 3, 3) this is shift size\n","run bitch\n","x shape torch.Size([1, 16, 64, 64, 96])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([300, 343, 96]) this is windows shape in partiton\n","torch.Size([300, 343, 96]) x_windows\n","torch.Size([300, 343, 96]) this is x shape in attention\n","torch.Size([300, 343, 288]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 16, 64, 64, 96])\n","run bitch\n","x shape torch.Size([1, 16, 64, 64, 96])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([300, 343, 96]) this is windows shape in partiton\n","torch.Size([300, 343, 96]) x_windows\n","torch.Size([300, 343, 96]) this is x shape in attention\n","torch.Size([300, 343, 288]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 16, 64, 64, 96])\n","run bitch\n","x shape torch.Size([1, 16, 64, 64, 96])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([300, 343, 96]) this is windows shape in partiton\n","torch.Size([300, 343, 96]) x_windows\n","torch.Size([300, 343, 96]) this is x shape in attention\n","torch.Size([300, 343, 288]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 16, 64, 64, 96])\n","x shape torch.Size([1, 16, 64, 64, 96])\n","torch.Size([128, 512, 96]) this is x shape sparse\n","torch.Size([128, 512, 96]) this is x shape in attention\n","torch.Size([128, 512, 288]) this is qkv shape in attention\n","torch.Size([1, 16, 64, 64, 96]) atten_shape\n","torch.Size([1, 16, 64, 64, 96]) this is x shape after three blocks in each layer\n","torch.Size([50, 343, 1]) this is windows shape in partiton\n","(3, 3, 3) this is shift size\n","run bitch\n","x shape torch.Size([1, 8, 32, 32, 192])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([50, 343, 192]) this is windows shape in partiton\n","torch.Size([50, 343, 192]) x_windows\n","torch.Size([50, 343, 192]) this is x shape in attention\n","torch.Size([50, 343, 576]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 8, 32, 32, 192])\n","run bitch\n","x shape torch.Size([1, 8, 32, 32, 192])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([50, 343, 192]) this is windows shape in partiton\n","torch.Size([50, 343, 192]) x_windows\n","torch.Size([50, 343, 192]) this is x shape in attention\n","torch.Size([50, 343, 576]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 8, 32, 32, 192])\n","run bitch\n","x shape torch.Size([1, 8, 32, 32, 192])\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","(7, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([50, 343, 192]) this is windows shape in partiton\n","torch.Size([50, 343, 192]) x_windows\n","torch.Size([50, 343, 192]) this is x shape in attention\n","torch.Size([50, 343, 576]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 8, 32, 32, 192])\n","x shape torch.Size([1, 8, 32, 32, 192])\n","torch.Size([32, 256, 192]) this is x shape sparse\n","torch.Size([32, 256, 192]) this is x shape in attention\n","torch.Size([32, 256, 576]) this is qkv shape in attention\n","torch.Size([1, 8, 32, 32, 192]) atten_shape\n","torch.Size([1, 8, 32, 32, 192]) this is x shape after three blocks in each layer\n","torch.Size([9, 196, 1]) this is windows shape in partiton\n","(0, 3, 3) this is shift size\n","run bitch\n","x shape torch.Size([1, 4, 16, 16, 384])\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([9, 196, 384]) this is windows shape in partiton\n","torch.Size([9, 196, 384]) x_windows\n","torch.Size([9, 196, 384]) this is x shape in attention\n","torch.Size([9, 196, 1152]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 4, 16, 16, 384])\n","run bitch\n","x shape torch.Size([1, 4, 16, 16, 384])\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([9, 196, 384]) this is windows shape in partiton\n","torch.Size([9, 196, 384]) x_windows\n","torch.Size([9, 196, 384]) this is x shape in attention\n","torch.Size([9, 196, 1152]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 4, 16, 16, 384])\n","run bitch\n","x shape torch.Size([1, 4, 16, 16, 384])\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","(4, 7, 7) this is window size (7, 7, 7) this is self window size\n","torch.Size([9, 196, 384]) this is windows shape in partiton\n","torch.Size([9, 196, 384]) x_windows\n","torch.Size([9, 196, 384]) this is x shape in attention\n","torch.Size([9, 196, 1152]) this is qkv shape in attention\n","sbsbsb torch.Size([1, 4, 16, 16, 384])\n","x shape torch.Size([1, 4, 16, 16, 384])\n","torch.Size([32, 32, 384]) this is x shape sparse\n","torch.Size([32, 32, 384]) this is x shape in attention\n","torch.Size([32, 32, 1152]) this is qkv shape in attention\n","torch.Size([1, 4, 16, 16, 384]) atten_shape\n","torch.Size([1, 4, 16, 16, 384]) this is x shape after three blocks in each layer\n","hidden states out torch.Size([1, 768, 2, 8, 8])\n","avgpool torch.Size([1, 768, 1, 1, 1])\n","flatten torch.Size([1, 768])\n","head torch.Size([1, 2])\n"]},{"data":{"text/plain":["tensor([[ 0.5685, -0.3393]], grad_fn=<AddmmBackward0>)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["model = SwinUNETR(img_size=(64, 256, 256), in_channels=1, out_channels=3, feature_size=48)\n","#model = SwinTransformer(\n","test_data = torch.randn(1,1,64,256,256)\n","model(test_data)"]},{"cell_type":"code","execution_count":156,"metadata":{},"outputs":[],"source":["#this is not after patched\n","\n","class SparseBlock(nn.Module):\n","    def __init__(self,\n","                 dim: int,\n","                 num_heads: int,\n","                 #window_size: Sequence[int],\n","                 interval: tuple = (2,4),\n","                 mlp_ratio: float = 4.0,\n","                 qkv_bias: bool = True,\n","                 attn_drop: float = 0.0,\n","                 drop: float = 0.0,\n","                 drop_path: float = 0.0,\n","                 act_layer: str = \"GELU\",\n","                 norm_layer: type[LayerNorm] = nn.LayerNorm,\n","                 ):\n","        \"\"\"\n","        args:\n","            interval: the interval of the sparse block, the first element is the interval of the depth, the second is the interval of the size\n","\n","        \"\"\"\n","        super(SparseBlock, self).__init__()\n","        self.interval = interval\n","        self.depth_interval = interval[0]\n","        self.size_interval = interval[1]\n","        self.norm1 = norm_layer(dim)\n","        self.dim = dim\n","        self.num_heads = num_heads\n","        self.mlp_ratio = mlp_ratio\n","        self.attn = WindowAttention(dim,\n","                                    window_size=None,\n","                                    num_heads=num_heads,\n","                                    qkv_bias=qkv_bias,\n","                                    attn_drop=attn_drop,\n","                                    proj_drop=drop)\n","        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n","\n","    \n","    def forward_part1(self,x):\n","        x_shape = x.size()\n","        print('x shape',x_shape)\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            I_d, I_size, G_d, G_h, G_w = self.depth_interval, self.size_interval, D//self.depth_interval, H//self.size_interval, W//self.size_interval\n","\n","            self.att.window_size = I_d * I_size * I_size\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","\n","\n","\n","\n","        B,C,D,H,W = x.shape\n","        I_d, I_size, G_d, G_h, G_w = self.depth_interval, self.size_interval, D//self.depth_interval, H//self.size_interval, W//self.size_interval\n","        x = x.reshape(B, G_d, I_d, G_h, I_size,  G_w, I_size, C).permute(0, 2, 4, 6, 1, 3, 5, 7).contiguous()\n","        #x = x.reshape(B*I_d*I_size*I_size,G_d,G_h,G_w,C)\n","        x = x.reshape(B*I_d*I_size*I_size,G_d*G_h*G_w,C)\n","        self.attn.window_size = (G_d,G_h,G_w)\n","        #print('I_d, I_size, G_d, G_h, G_w',I_d, I_size, G_d, G_h, G_w)\n","        print(x.shape,'this is x shape')\n","        return x\n","    \n","\n","    def forward_part1(self, x, mask_matrix):\n","        x_shape = x.size()\n","        #print('x shape',x_shape)\n","        x = self.norm1(x)\n","        if len(x_shape) == 5:\n","            b, d, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = pad_d0 = 0\n","            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n","            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n","            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n","            _, dp, hp, wp, _ = x.shape\n","            dims = [b, dp, hp, wp]\n","\n","        elif len(x_shape) == 4:\n","            b, h, w, c = x.shape\n","            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n","            pad_l = pad_t = 0\n","            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n","            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n","            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n","            _, hp, wp, _ = x.shape\n","            dims = [b, hp, wp]\n","\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n","            attn_mask = mask_matrix\n","        else:\n","            shifted_x = x\n","            attn_mask = None\n","        x_windows = window_partition(shifted_x, window_size)\n","\n","\n","\n","        print(x_windows.shape,'x_windows')\n","        attn_windows = self.attn(x_windows, mask=attn_mask)\n","        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n","        shifted_x = window_reverse(attn_windows, window_size, dims)\n","        if any(i > 0 for i in shift_size):\n","            if len(x_shape) == 5:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n","            elif len(x_shape) == 4:\n","                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","\n","        if len(x_shape) == 5:\n","            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n","                x = x[:, :d, :h, :w, :].contiguous()\n","        elif len(x_shape) == 4:\n","            if pad_r > 0 or pad_b > 0:\n","                x = x[:, :h, :w, :].contiguous()\n","\n","        return x\n","\n","    def forward_part2(self, x):\n","        #print(\"all < 0\",torch.all(self.norm2(x)<0))\n","        return self.drop_path(self.mlp(self.norm2(x)))\n","\n","         "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if self.ds_flag == 1:  # Sparse Attention\n","            I, Gh, Gw = self.interval, Hd // self.interval, Wd // self.interval\n","            #print('I: ', I, 'Gh: ', Gh, 'Gw: ', Gw)\n","            #print(x[0,0::32,0::32,0],'shit')\n","            #divide the shape! sparse split! should be (B,4,32,4,32,C) \n","            x = x.reshape(B, Gh, I, Gw, I, C).permute(0, 2, 4, 1, 3, 5).contiguous()\n","            #print(x.shape,'after sparse split')\n","            #print(x[0,0,0,:,:,0],'this is first group')\n","            x = x.reshape(B * I * I, Gh * Gw, C)\n","            nP = I ** 2  # number of partitioning groups\n","            # attn_mask\n","            if pad_r > 0 or pad_b > 0:\n","                mask = mask.reshape(1, Gh, I, Gw, I, 1).permute(0, 2, 4, 1, 3, 5).contiguous()\n","                mask = mask.reshape(nP, 1, Gh * Gw)\n","                attn_mask = torch.zeros((nP, Gh * Gw, Gh * Gw), device=x.device)\n","                attn_mask = attn_mask.masked_fill(mask < 0, NEG_INF)\n","            else:\n","                attn_mask = None"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([32, 16, 1]) this is x shape\n"]},{"data":{"text/plain":["tensor([[  0],\n","        [  4],\n","        [ 32],\n","        [ 36],\n","        [128],\n","        [132],\n","        [160],\n","        [164],\n","        [256],\n","        [260],\n","        [288],\n","        [292],\n","        [384],\n","        [388],\n","        [416],\n","        [420]])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["sp = SparseBlock()\n","test_data = torch.arange(0,512).reshape(1,1,8,8,8)\n","x = sp(test_data)\n","x[0,]"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from __future__ import annotations\n","import torch\n","from torch import nn\n","import os\n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","from monai.networks.layers import DropPath, trunc_normal_\n","class WindowAttention(nn.Module):\n","    \"\"\"\n","    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n","    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n","    <https://arxiv.org/abs/2103.14030>\"\n","    https://github.com/microsoft/Swin-Transformer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        dim: int,\n","        num_heads: int,\n","        window_size: Sequence[int],\n","        qkv_bias: bool = False,\n","        attn_drop: float = 0.0,\n","        proj_drop: float = 0.0,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            dim: number of feature channels.\n","            num_heads: number of attention heads.\n","            window_size: local window size.\n","            qkv_bias: add a learnable bias to query, key, value.\n","            attn_drop: attention dropout rate.\n","            proj_drop: dropout rate of output.\n","        \"\"\"\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = head_dim**-0.5\n","        mesh_args = torch.meshgrid.__kwdefaults__\n","\n","        if len(self.window_size) == 3:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros(\n","                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n","                    num_heads,\n","                )\n","            )\n","            coords_d = torch.arange(self.window_size[0])\n","            coords_h = torch.arange(self.window_size[1])\n","            coords_w = torch.arange(self.window_size[2])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 2] += self.window_size[2] - 1\n","            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n","            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n","        elif len(self.window_size) == 2:\n","            self.relative_position_bias_table = nn.Parameter(\n","                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n","            )\n","            coords_h = torch.arange(self.window_size[0])\n","            coords_w = torch.arange(self.window_size[1])\n","            if mesh_args is not None:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n","            else:\n","                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n","            coords_flatten = torch.flatten(coords, 1)\n","            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n","            relative_coords[:, :, 0] += self.window_size[0] - 1\n","            relative_coords[:, :, 1] += self.window_size[1] - 1\n","            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","\n","        relative_position_index = relative_coords.sum(-1)\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","        trunc_normal_(self.relative_position_bias_table, std=0.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask):\n","        b, n, c = x.shape\n","        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]\n","        q = q * self.scale\n","        attn = q @ k.transpose(-2, -1)\n","        relative_position_bias = self.relative_position_bias_table[\n","            self.relative_position_index.clone()[:n, :n].reshape(-1)\n","        ].reshape(n, n, -1)\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","        if mask is not None:\n","            nw = mask.shape[0]\n","            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, n, n)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn).to(v.dtype)\n","        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        print(x.shape,'after window size')\n","        return x"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3000, 16, 96]) after window size\n"]},{"data":{"text/plain":["torch.Size([3000, 16, 96])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["win_att = WindowAttention(dim=96,num_heads=3,window_size=(4,2,2))\n","x = torch.randn(3000,16,96)\n","mask = None\n","win_att(x,mask).shape"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["set()"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["set(test_data[0,0,0::8,0::32,0::32].flatten()).intersection(set(x[0,:,0]))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([-0.5939, -0.7092,  1.8806,  1.5746, -1.0066, -0.2840,  0.5812, -0.8473,\n","        -1.2320, -0.4511,  0.5638, -1.6766,  1.0486,  0.1693, -1.5872,  0.1523,\n","         0.0272, -0.0958,  0.2731,  0.2261,  1.3336,  2.0896, -0.7613, -0.0621,\n","        -0.3534,  0.0565,  0.9133,  1.8550, -0.2348, -0.1768, -0.1819, -0.8254,\n","         1.2134,  1.0779, -0.7269, -0.7629,  0.5622,  0.6580, -1.4655,  2.0265,\n","        -0.0028, -0.2014,  0.4629, -0.7120, -1.6446, -1.1594,  0.1618, -0.1333,\n","         0.6704,  1.0082, -1.9194, -1.8175, -0.2792, -0.0311,  0.8773,  0.4927,\n","        -1.3043, -0.0391,  1.4080, -0.5695,  0.1952, -0.2087,  1.8345,  0.4098])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["x[0,:,0]"]},{"cell_type":"markdown","metadata":{},"source":["# prepare Data\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import nibabel as nib\n","from monai.transforms import Compose, Resize, EnsureChannelFirst\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["#add one channel for data_1\n","data_1 = nib.load('./Test_3D/CILM_CT_102030_0000.nii.gz').get_fdata()\n","data_1 = data_1.reshape(1,512,512,280)\n","transoformer_1 = Compose([EnsureChannelFirst(channel_dim=0),Resize((96,96,96))])\n","resize_data = transoformer_1(data_1)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#change data to original shape\n","resize_data = resize_data.reshape(96,96,96)\n","resize_data_5c = resize_data.reshape(1,1,96,96,96)\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 1, 96, 96, 96]) this is x shape\n"]}],"source":["#define model\n","swin_model = SwinTransformer(in_chans=1,embed_dim=48,window_size=(7,7,7),patch_size=(2,2,2),depths=[2,2,6,2],num_heads=[3,6,12,24],mlp_ratio=4.0,qkv_bias=True,drop_rate=0.0,attn_drop_rate=0.0,drop_path_rate=0.0,norm_layer=nn.LayerNorm,patch_norm=True,use_checkpoint=False,spatial_dims=3,downsample=\"merging\",use_v2=True)\n","swin_vit_out = swin_model(res\n","                          ize_data_5c)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["untre_block = UnetrBasicBlock(\n","            spatial_dims=3,\n","            in_channels=48,\n","            out_channels=48,\n","            kernel_size=3,\n","            stride=1,\n","            norm_name='instance',\n","            res_block=True,\n","        )"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 1, 96, 96, 96]) this is x shape\n","torch.Size([1, 48, 48, 48, 48]) this is x0 shape\n","torch.Size([1, 96, 24, 24, 24]) this is x1 shape\n","torch.Size([1, 192, 12, 12, 12]) this is x2 shape\n","hidden states out torch.Size([1, 768, 3, 3, 3])\n"]}],"source":["swin_untre_model = SwinUNETR(img_size=(96,96,96),in_channels=1,out_channels=3,feature_size=48)\n","swin_untre_out = swin_untre_model(resize_data_5c)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["metatensor(0.5111, grad_fn=<AliasBackward0>)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["torch.nn.functional.softmax(swin_untre_out)\n","#loss\n","loss = torch.nn.CrossEntropyLoss()\n","loss(swin_untre_out,torch.tensor([1]))\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'SwinUNETR' object has no attribute 'feature_size'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32md:\\Onedrive\\bioinformatics_textbook\\VU_Study\\internship\\Eramus_project\\CRLM_Yizhou\\Source_Code\\3DSwin_Trans Class.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/3DSwin_Trans%20Class.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m swin_untre_model\u001b[39m.\u001b[39;49mfeature_size\n","File \u001b[1;32mc:\\Users\\Bacon\\.conda\\envs\\samuel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n","\u001b[1;31mAttributeError\u001b[0m: 'SwinUNETR' object has no attribute 'feature_size'"]}],"source":["swin_untre_model.feature_size"]}],"metadata":{"kernelspec":{"display_name":"samuel","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
