{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bacon\\.conda\\envs\\samuel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#random generate 224*224*3\n",
    "img_1 = np.random.rand(224,224,3)\n",
    "img_1 = torch.from_numpy(img_1)\n",
    "#将image1转为float\n",
    "img_1 = img_1.float()\n",
    "img_1 = img_1.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "if a:\n",
    "    print(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\098986\\AppData\\Local\\anaconda3\\envs\\CILM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = (\n",
    "        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(\n",
    "        B, H // window_size, W // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\"Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PatchMerging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Onedrive\\bioinformatics_textbook\\VU_Study\\internship\\Eramus_project\\CRLM_Yizhou\\Source_Code\\Test_Transformer.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m patch_1 \u001b[39m=\u001b[39m PatchMerging(input_resolution\u001b[39m=\u001b[39m(\u001b[39m224\u001b[39m,\u001b[39m224\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m img_patch_1 \u001b[39m=\u001b[39m patch_1\u001b[39m.\u001b[39mforward(img_1\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PatchMerging' is not defined"
     ]
    }
   ],
   "source": [
    "patch_1 = PatchMerging(input_resolution=(224,224), dim=3)\n",
    "img_patch_1 = patch_1.forward(img_1.reshape(1,-1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12544, 6])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_patch_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3136, 4, 4, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_1 = Mlp(in_features=4*4*3, hidden_features=None, out_features=4*4*6, act_layer=nn.GELU, drop=0.0)\n",
    "window_partition(img_1, 4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def draw_windows(img_size, window_size, shift_size):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    # 绘制原始图像\n",
    "    ax.set_xlim(0, img_size)\n",
    "    ax.set_ylim(0, img_size)\n",
    "    ax.set_xticks(range(0, img_size+1, window_size))\n",
    "    ax.set_yticks(range(0, img_size+1, window_size))\n",
    "    plt.grid(True, which='both', color='gray', linewidth=0.5)\n",
    "\n",
    "    # 添加窗口\n",
    "    for x in range(0, img_size, shift_size):\n",
    "        for y in range(0, img_size, shift_size):\n",
    "            if x + window_size <= img_size and y + window_size <= img_size:\n",
    "                rect = patches.Rectangle((x, y), window_size, window_size, linewidth=1, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "    plt.gca().invert_yaxis()  # y轴反向，使图像与常规坐标系统一致\n",
    "    plt.show()\n",
    "\n",
    "# 参数\n",
    "img_size = 56\n",
    "window_size = 4\n",
    "shift_size = 2\n",
    "\n",
    "draw_windows(img_size, window_size, shift_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7]\n",
    "a[5:]\n",
    "a[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scans_data = pd.read_excel('../../New_Extra_Data/combination_archive_with_dates.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_scans_data['PID Lokaal'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hgp_scores = pd.read_excel('../../New_Extra_Data/CRLM_Status_230622.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_access_scans = pd.read_csv('../../New_Extra_Data/CILM_xnatsort_20230323.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_scans_data = pd.read_csv('../Phase_Detector/Data/Final_Phase_Label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    105\n",
       "1     96\n",
       "0     62\n",
       "Name: Phase, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(all_scans_data['Phase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = (\n",
    "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        )  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        print(B_, N, C,'6666')\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )  # make torchscript happy (cannot use tensor as tuple)\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        print(attn.shape,'111')\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)\n",
    "        ].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1\n",
    "        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        #print(relative_position_bias.shape,'666')\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            #mask值设为-100不为关注！\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n",
    "                1\n",
    "            ).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\"Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert (\n",
    "            0 <= self.shift_size < self.window_size\n",
    "        ), \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            #这里其实是每个window像右下shift因此需要mask计算的是每个windowsize里的mask！\n",
    "            \n",
    "            # calculate attention mask for SW-MSA\n",
    "            #是先算mask再叠加到计算好的attention上！\n",
    "            #再mask到计算好后的上面！注意对角线采用一二维相减！\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "            print(f\" mask  {img_mask.flatten().tolist()}\")\n",
    "            mask_windows = window_partition(\n",
    "                img_mask, self.window_size\n",
    "            )  # nW, window_size, window_size, 1\n",
    "            print(f\"hi mask{Counter(mask_windows.flatten().tolist())}\")\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            print(f\"this is mask1 {mask_windows.unsqueeze(1).shape},and mask 2 {mask_windows.unsqueeze(2).shape}\")\n",
    "            print(f\" atten mask{attn_mask.shape}\")\n",
    "            attn_mask = attn_mask.masked_fill(\n",
    "                attn_mask != 0, float(-100.0)\n",
    "            ).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('num of heads',self.num_heads)\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(\n",
    "                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(\n",
    "            shifted_x, self.window_size\n",
    "        )  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(\n",
    "            -1, self.window_size * self.window_size, C\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        #这里是先rollwindow后计算atten再mask,mask就是shift后的图像上对应的位置\n",
    "        attn_windows = self.attn(\n",
    "            x_windows, mask=self.attn_mask\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(\n",
    "                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \"\n",
    "            f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "        )\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\"Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, norm_layer=norm_layer\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\"Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert (\n",
    "            H == self.img_size[0] and W == self.img_size[1]\n",
    "        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = (\n",
    "            Ho\n",
    "            * Wo\n",
    "            * self.embed_dim\n",
    "            * self.in_chans\n",
    "            * (self.patch_size[0] * self.patch_size[1])\n",
    "        )\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "a  = a.flatten().unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "         [-1,  0,  1,  2,  3,  4,  5,  6,  7],\n",
       "         [-2, -1,  0,  1,  2,  3,  4,  5,  6],\n",
       "         [-3, -2, -1,  0,  1,  2,  3,  4,  5],\n",
       "         [-4, -3, -2, -1,  0,  1,  2,  3,  4],\n",
       "         [-5, -4, -3, -2, -1,  0,  1,  2,  3],\n",
       "         [-6, -5, -4, -3, -2, -1,  0,  1,  2],\n",
       "         [-7, -6, -5, -4, -3, -2, -1,  0,  1],\n",
       "         [-8, -7, -6, -5, -4, -3, -2, -1,  0]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(1)-a.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bacon\\AppData\\Local\\Temp\\ipykernel_4872\\3017939059.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  c = torch.tensor(a).unsqueeze(1) - torch.tensor(a).unsqueeze(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [-1,  0,  1],\n",
       "         [-2, -1,  0]],\n",
       "\n",
       "        [[ 0,  1,  2],\n",
       "         [-1,  0,  1],\n",
       "         [-2, -1,  0]],\n",
       "\n",
       "        [[ 0,  1,  2],\n",
       "         [-1,  0,  1],\n",
       "         [-2, -1,  0]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = [1,2,3,4,5,6,7,8,9]\n",
    "a = torch.tensor(a).reshape(3,3)\n",
    "c = torch.tensor(a).unsqueeze(1) - torch.tensor(a).unsqueeze(2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('../Phase_Detector/Phase_Label.csv')\n",
    "data_1 = data[(data['Phase'] != \"Other\") & (data['Phase'] != \"Unknown\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = PatchEmbed()\n",
    "img_1 = img_1.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mask  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0]\n",
      "hi maskCounter({0.0: 2401, 1.0: 196, 3.0: 196, 2.0: 147, 6.0: 147, 4.0: 16, 5.0: 12, 7.0: 12, 8.0: 9})\n",
      "this is mask1 torch.Size([64, 1, 49]),and mask 2 torch.Size([64, 49, 1])\n",
      " atten masktorch.Size([64, 49, 49])\n"
     ]
    }
   ],
   "source": [
    "#patch_embd_1 = patch_embed.forward(img_1)\n",
    "patch_merg = PatchMerging((56,56),96)\n",
    "#patch_merg_1 = patch_merg.forward(patch_embd_1)\n",
    "swin_attn = SwinTransformerBlock(96,(56,56),3,shift_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_embd_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 49 96 6666\n",
      "torch.Size([64, 3, 49, 49]) 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#计算attention\n",
    "#改dim为1,64，-1,192；-1代表自动计算\n",
    "patch_embd_1_attn = patch_embd_1.reshape(64,49,-1)\n",
    "#forward 的不是同一个!\n",
    "swin_attn.forward(patch_embd_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 7]) 666666666\n",
      "666 torch.Size([2, 49]) 666\n",
      "tensor([[[ 0,  0],\n",
      "         [ 0, -1],\n",
      "         [ 0, -2],\n",
      "         ...,\n",
      "         [-6, -4],\n",
      "         [-6, -5],\n",
      "         [-6, -6]],\n",
      "\n",
      "        [[ 0,  1],\n",
      "         [ 0,  0],\n",
      "         [ 0, -1],\n",
      "         ...,\n",
      "         [-6, -3],\n",
      "         [-6, -4],\n",
      "         [-6, -5]],\n",
      "\n",
      "        [[ 0,  2],\n",
      "         [ 0,  1],\n",
      "         [ 0,  0],\n",
      "         ...,\n",
      "         [-6, -2],\n",
      "         [-6, -3],\n",
      "         [-6, -4]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 6,  4],\n",
      "         [ 6,  3],\n",
      "         [ 6,  2],\n",
      "         ...,\n",
      "         [ 0,  0],\n",
      "         [ 0, -1],\n",
      "         [ 0, -2]],\n",
      "\n",
      "        [[ 6,  5],\n",
      "         [ 6,  4],\n",
      "         [ 6,  3],\n",
      "         ...,\n",
      "         [ 0,  1],\n",
      "         [ 0,  0],\n",
      "         [ 0, -1]],\n",
      "\n",
      "        [[ 6,  6],\n",
      "         [ 6,  5],\n",
      "         [ 6,  4],\n",
      "         ...,\n",
      "         [ 0,  2],\n",
      "         [ 0,  1],\n",
      "         [ 0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, window_size):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def my_method(self):\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
    "        print(coords.shape,'666666666')  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        print('666',coords_flatten.shape,'666')\n",
    "        relative_coords = (\n",
    "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        )  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        print(relative_coords)\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "\n",
    "my_transformer = MyTransformer((7,7))\n",
    "my_transformer.my_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 1])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 8, 1])\n"
     ]
    }
   ],
   "source": [
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.window_size = 7\n",
    "        self.shift_size = 3\n",
    "        self.img_mask  = torch.zeros((1, 8, 8, 1))\n",
    "        print(self.img_mask.shape)\n",
    "    def my_method(self):\n",
    "        h_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        w_slices = (\n",
    "            slice(0, -self.window_size),\n",
    "            slice(-self.window_size, -self.shift_size),\n",
    "            slice(-self.shift_size, None),\n",
    "        )\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                self.img_mask[:, h, w, :] = cnt \n",
    "                cnt += 1\n",
    "        return self.img_mask\n",
    "\n",
    "my_class = MyClass()\n",
    "final = my_class.my_method()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({4.0: 16,\n",
       "         5.0: 12,\n",
       "         7.0: 12,\n",
       "         8.0: 9,\n",
       "         1.0: 4,\n",
       "         3.0: 4,\n",
       "         2.0: 3,\n",
       "         6.0: 3,\n",
       "         0.0: 1})"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(list(final.flatten().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = 7,7\n",
    "img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2],\n",
       "        [ 5,  6,  7],\n",
       "        [10, 11, 12]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设计一个5x5数组\n",
    "a = torch.arange(25).reshape(5,5)\n",
    "a[-5:-2,-5:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 0), dtype=torch.int64)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0:-5,0:-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_slice = slice(-2,None)\n",
    "w_slice = slice(-2,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[18, 19],\n",
       "        [23, 24]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[h_slice,w_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CT_10104',\n",
       " 'CT_10203',\n",
       " 'CT_10455',\n",
       " 'CT_10785',\n",
       " 'CT_11111',\n",
       " 'CT_11155',\n",
       " 'CT_11447',\n",
       " 'CT_11656',\n",
       " 'CT_12190',\n",
       " 'CT_12545',\n",
       " 'CT_12553',\n",
       " 'CT_12931',\n",
       " 'CT_12957',\n",
       " 'CT_13419',\n",
       " 'CT_13714',\n",
       " 'CT_13999',\n",
       " 'CT_14025',\n",
       " 'CT_14146',\n",
       " 'CT_14151',\n",
       " 'CT_14299',\n",
       " 'CT_14776',\n",
       " 'CT_15014',\n",
       " 'CT_15300',\n",
       " 'CT_15566',\n",
       " 'CT_15813',\n",
       " 'CT_16078',\n",
       " 'CT_16466',\n",
       " 'CT_16641',\n",
       " 'CT_17684',\n",
       " 'CT_19046',\n",
       " 'CT_19082',\n",
       " 'CT_19381',\n",
       " 'CT_19738',\n",
       " 'CT_19876',\n",
       " 'CT_19901',\n",
       " 'CT_20185',\n",
       " 'CT_20299',\n",
       " 'CT_20370',\n",
       " 'CT_20646',\n",
       " 'CT_20985',\n",
       " 'CT_21017',\n",
       " 'CT_21103',\n",
       " 'CT_21114',\n",
       " 'CT_21502',\n",
       " 'CT_21564',\n",
       " 'CT_21919',\n",
       " 'CT_21927',\n",
       " 'CT_22762',\n",
       " 'CT_22829',\n",
       " 'CT_22958',\n",
       " 'CT_23288',\n",
       " 'CT_23350',\n",
       " 'CT_23517',\n",
       " 'CT_24329',\n",
       " 'CR_24329',\n",
       " 'CT_24650',\n",
       " 'CT_25359',\n",
       " 'CT_25522',\n",
       " 'CT_25531',\n",
       " 'CT_25615',\n",
       " 'CT_25848',\n",
       " 'CT_25963',\n",
       " 'CT_26140',\n",
       " 'CT_26579',\n",
       " 'CT_26885',\n",
       " 'CT_27035',\n",
       " 'CT_27139',\n",
       " 'CT_27158',\n",
       " 'CT_27167',\n",
       " 'CT_27232',\n",
       " 'CT_27479',\n",
       " 'CT_27750',\n",
       " 'CT_27862',\n",
       " 'CT_27925',\n",
       " 'CT_28467',\n",
       " 'CT_30065',\n",
       " 'CT_30136',\n",
       " 'CT_30722',\n",
       " 'CT_30821',\n",
       " 'CT_30903',\n",
       " 'CT_31196',\n",
       " 'CT_31434',\n",
       " 'CT_32094',\n",
       " 'CT_32156',\n",
       " 'CT_32341',\n",
       " 'CT_32505',\n",
       " 'CT_32675',\n",
       " 'CT_32761',\n",
       " 'CT_32838',\n",
       " 'CT_33370',\n",
       " 'CT_33512',\n",
       " 'CT_33554',\n",
       " 'CT_33933',\n",
       " 'CT_35126',\n",
       " 'CT_40034',\n",
       " 'CT_49508',\n",
       " 'CT_50348',\n",
       " 'CT_50410',\n",
       " 'CT_54293',\n",
       " 'CT_54838',\n",
       " 'CT_54863',\n",
       " 'CT_62611',\n",
       " 'CT_67513',\n",
       " 'CT_69644',\n",
       " 'CT_76415',\n",
       " 'CT_79221',\n",
       " 'CT_80259',\n",
       " 'CT_80390',\n",
       " 'CT_83958',\n",
       " 'CT_88799',\n",
       " 'CT_95582',\n",
       " 'CT_95769',\n",
       " 'CT_99089',\n",
       " 'CT_99400']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    experiment  scan other_column1  other_column2\n",
      "0  experiment1     5             a              1\n",
      "1  experiment2    10             b              2\n",
      "2  experiment3    15             c              3\n",
      "3  experiment4    20             d              4\n",
      "    experiment  scan\n",
      "0  experiment1     5\n",
      "1  experiment3    15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_data = pd.DataFrame({\n",
    "    'experiment': ['experiment1', 'experiment2', 'experiment3', 'experiment4'],\n",
    "    'scan': [5, 10, 15, 20],\n",
    "    'other_column1': ['a', 'b', 'c', 'd'],\n",
    "    'other_column2': [1, 2, 3, 4]\n",
    "})\n",
    "\n",
    "print(original_data)\n",
    "\n",
    "filtered_data = pd.DataFrame({\n",
    "    'experiment': ['experiment1', 'experiment3'],\n",
    "    'scan': [5, 15]\n",
    "})\n",
    "\n",
    "print(filtered_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def a_b(num=None):\n",
    "    return num or 3\n",
    "\n",
    "a_b(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Insert_Date</th>\n",
       "      <th>Scan</th>\n",
       "      <th>Series_description</th>\n",
       "      <th>Modality</th>\n",
       "      <th>Manufacturer</th>\n",
       "      <th>Headtype</th>\n",
       "      <th>Subtype</th>\n",
       "      <th>...</th>\n",
       "      <th>seriesdate</th>\n",
       "      <th>acquisition_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>patient_ID</th>\n",
       "      <th>scanning_sequence</th>\n",
       "      <th>model_name</th>\n",
       "      <th>patient_position</th>\n",
       "      <th>Phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, Subject, Experiment, Insert_Date, Scan, Series_description, Modality, Manufacturer, Headtype, Subtype, Direction, Contrast, orientation, contrast_status, modality, acquistion_type, content_time, spacing_between_slices, acquisition_time, image_type, series_description, tube_current, sequence_variant, series_time, patient_name, pixel_spacing, number_of_averages, studydate, study_description, study_time, protocol_name, kvp, scan_options, percent_sampling, station_name, slice_thickness, manufacturer, convolution_kernel, seriesdate, acquisition_type, gender, age, birthdate, patient_ID, scanning_sequence, model_name, patient_position, Phase]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 48 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1[data_1['Subject'].isin(b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_mask = torch.zeros((1, 49, 49, 1))\n",
    "img_mask[:, h1, w1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuck mask aaa Counter({0.0: 2401, 1.0: 196, 3.0: 196, 2.0: 147, 6.0: 147, 4.0: 16, 5.0: 12, 7.0: 12, 8.0: 9})\n",
      "fuck masktorch.Size([64, 7, 7, 1])\n",
      "64 49 96 6666\n",
      "torch.Size([64, 3, 49, 49]) 111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_trans = SwinTransformerBlock(96,(56,56),3,shift_size=3)\n",
    "swin_trans.forward(patch_embd_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\"Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=2,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        ape=False,\n",
    "        patch_norm=True,\n",
    "        use_checkpoint=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, num_patches, embed_dim)\n",
    "            )\n",
    "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                input_resolution=(\n",
    "                    patches_resolution[0] // (2**i_layer),\n",
    "                    patches_resolution[1] // (2**i_layer),\n",
    "                ),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = (\n",
    "            nn.Linear(self.num_features, num_classes)\n",
    "            if num_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"absolute_pos_embed\"}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"relative_position_bias_table\"}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += (\n",
    "            self.num_features\n",
    "            * self.patches_resolution[0]\n",
    "            * self.patches_resolution[1]\n",
    "            // (2**self.num_layers)\n",
    "        )\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyxnat\n",
    "import pyxnat\n",
    "\n",
    "# XNAT服务器的URL\n",
    "xnat_url = 'https://bigr-rad-xnat.erasmusmc.nl'\n",
    "\n",
    "# 用户的凭据\n",
    "username = 'yliu'\n",
    "password = 'x37vnp78'\n",
    "\n",
    "# 连接到XNAT\n",
    "with pyxnat.Interface(server=xnat_url, user=username, password=password) as xnat:\n",
    "\n",
    "    # 指定项目、主题和实验\n",
    "    project_id = 'CILM'\n",
    "    #subject_id = 'your_subject_id'\n",
    "   # experiment_id = 'your_experiment_id'\n",
    "\n",
    "    # 指定要上传的文件路径\n",
    "    #file_path = '/path/to/your/file'\n",
    "\n",
    "    # 创建主题和实验（如果它们不存在）\n",
    "    #if not xnat.select.project(project_id).exists():\n",
    "   #     xnat.select.project(project_id).create()\n",
    "   # if not xnat.select.subject(subject_id).exists():\n",
    "    #    xnat.select.project(project_id).subject(subject_id).create()\n",
    "    #if not xnat.select.experiment(experiment_id).exists():\n",
    "     #   xnat.select.project(project_id).subject(subject_id).experiment(experiment_id).create()\n",
    "\n",
    "    # 上传文件\n",
    "   # xnat.select.project(project_id).subject(subject_id).experiment(experiment_id).resource('new_resource').file(file_path.split('/')[-1]).put(file_path, overwrite=True)\n",
    "\n",
    "    #print(\"File uploaded successfully.\")\n",
    "    #get project\n",
    "    project = xnat.select.project(project_id)\n",
    "    #get all subjuects\n",
    "    subjects = project.subjects\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "a = nib.load('../Test_Data/Raw_Phase_data/CILM_CT_229581_0000.nii.gz')\n",
    "b = nib.load('../Test_Data/Seg_Phase_Data/CILM_CT_229581.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_a = b.get_fdata()\n",
    "\n",
    "mask_a[mask_a==2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "labeled = skimage.measure.label(mask_a, connectivity=2)\n",
    "labeled[labeled != 1] = 0\n",
    "mask = labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask!=1]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "767547"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count time\n",
    "import time\n",
    "a = [i for i in range(100000000)]\n",
    "b = [i for i in range(100000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6330320835113525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Onedrive\\bioinformatics_textbook\\VU_Study\\internship\\Eramus_project\\CRLM_Yizhou\\Source_Code\\Test_Transformer.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#Y113sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#Y113sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100000000\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#Y113sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(a))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Onedrive/bioinformatics_textbook/VU_Study/internship/Eramus_project/CRLM_Yizhou/Source_Code/Test_Transformer.ipynb#Y113sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#calculate time\n",
    "start = time.time()\n",
    "for i in a:\n",
    "    pass\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "#calculate time\n",
    "start = time.time()\n",
    "for i in range(100000000):\n",
    "    next(iter(a))\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xnat\n",
    "xnat_session = xnat.connect('https://bigr-rad-xnat.erasmusmc.nl', user='yliu', password='x37vnp78')\n",
    "xnat_project = xnat_session.projects['CILM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_scans = pd.read_csv('../../../Eramus_project/Code/CRLM_Samuel-main/scans_used_all_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20bc0090130>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = nib.load('../Test_Data/CILM_CT_696440_0000.nii.gz')\n",
    "b = nib.load('../Test_Data/CILM_CT_696440.nii.gz')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(a.get_fdata()[:,:,:],cmap='gray')\n",
    "#plt.imshow(b.get_fdata()[:,:,20],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.get_fdata()\n",
    "b[b==2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "labeld = skimage.measure.label(b,connectivity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeld[labeld!=1]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_probs = skimage.measure.regionprops((labeld))\n",
    "for props in image_probs:\n",
    "    bbox = props.bbox\n",
    "    min_row, min_col, min_slice, max_row, max_col, max_slice = bbox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 162 4 349 413 32\n"
     ]
    }
   ],
   "source": [
    "print(min_row, min_col, min_slice, max_row, max_col, max_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "image_a = nib.load('../Test_Data/Cropped_Data/CILM_CT_696440_0000.nii.gz')\n",
    "image_b = nib.load('../Test_Data/Samule_Data/CT_24836/scans/1-Ax_5_4_LONG/resources/NIFTI/files/image.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Swing transformer\n",
    "# --------------------------------------------------------\n",
    "# Swin Transformer\n",
    "# Copyright (c) 2021 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ze Liu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = (\n",
    "        x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    )\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(\n",
    "        B, H // window_size, W // window_size, window_size, window_size, -1\n",
    "    )\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    r\"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        window_size,\n",
    "        num_heads,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "        )  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = (\n",
    "            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        )  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(\n",
    "            1, 2, 0\n",
    "        ).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        q, k, v = (\n",
    "            qkv[0],\n",
    "            qkv[1],\n",
    "            qkv[2],\n",
    "        )  # make torchscript happy (cannot use tensor as tuple)\n",
    "        print(q.shape,'q.shape')\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)\n",
    "        ].view(\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            self.window_size[0] * self.window_size[1],\n",
    "            -1,\n",
    "        )  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(\n",
    "            2, 0, 1\n",
    "        ).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(\n",
    "                1\n",
    "            ).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "        print(attn.shape,'attention shape')\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        print(x.shape,'after attention x shape')\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        print(x.shape,'x.shape,attention')\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}\"\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    r\"\"\"Swin Transformer Block.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        num_heads,\n",
    "        window_size=7,\n",
    "        shift_size=0,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert (\n",
    "            0 <= self.shift_size < self.window_size\n",
    "        ), \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=to_2tuple(self.window_size),\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            # calculate attention mask for SW-MSA\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n",
    "            h_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            w_slices = (\n",
    "                slice(0, -self.window_size),\n",
    "                slice(-self.window_size, -self.shift_size),\n",
    "                slice(-self.shift_size, None),\n",
    "            )\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(\n",
    "                img_mask, self.window_size\n",
    "            )  # nW, window_size, window_size, 1\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(\n",
    "                attn_mask != 0, float(-100.0)\n",
    "            ).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(\n",
    "                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(\n",
    "            shifted_x, self.window_size\n",
    "        )  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(\n",
    "            -1, self.window_size * self.window_size, C\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(\n",
    "            x_windows, mask=self.attn_mask\n",
    "        )  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(\n",
    "                shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)\n",
    "            )\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return (\n",
    "            f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \"\n",
    "            f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "        )\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    r\"\"\"Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"A basic Swin Transformer layer for one stage.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        input_resolution,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    input_resolution=input_resolution,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=window_size,\n",
    "                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(\n",
    "                input_resolution, dim=dim, norm_layer=norm_layer\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for blk in self.blocks:\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    r\"\"\"Image to Patch Embedding\n",
    "\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [\n",
    "            img_size[0] // patch_size[0],\n",
    "            img_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert (\n",
    "            H == self.img_size[0] and W == self.img_size[1]\n",
    "        ), f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = (\n",
    "            Ho\n",
    "            * Wo\n",
    "            * self.embed_dim\n",
    "            * self.in_chans\n",
    "            * (self.patch_size[0] * self.patch_size[1])\n",
    "        )\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    r\"\"\"Swin Transformer\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 224\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        patch_size=4,\n",
    "        in_chans=3,\n",
    "        num_classes=2,\n",
    "        embed_dim=96,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        num_heads=[3, 6, 12, 24],\n",
    "        window_size=7,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        ape=False,\n",
    "        patch_norm=True,\n",
    "        use_checkpoint=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, num_patches, embed_dim)\n",
    "            )\n",
    "            trunc_normal_(self.absolute_pos_embed, std=0.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [\n",
    "            x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))\n",
    "        ]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                input_resolution=(\n",
    "                    patches_resolution[0] // (2**i_layer),\n",
    "                    patches_resolution[1] // (2**i_layer),\n",
    "                ),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = (\n",
    "            nn.Linear(self.num_features, num_classes)\n",
    "            if num_classes > 0\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {\"absolute_pos_embed\"}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"relative_position_bias_table\"}\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        if self.ape:\n",
    "            x = x + self.absolute_pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.norm(x)  # B L C\n",
    "        x = self.avgpool(x.transpose(1, 2))  # B C 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.patch_embed.flops()\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            flops += layer.flops()\n",
    "        flops += (\n",
    "            self.num_features\n",
    "            * self.patches_resolution[0]\n",
    "            * self.patches_resolution[1]\n",
    "            // (2**self.num_layers)\n",
    "        )\n",
    "        flops += self.num_features * self.num_classes\n",
    "        return flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_image = torch.rand(1, 3, 224, 224)\n",
    "model = SwinTransformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 49, 32]) q.shape\n",
      "torch.Size([64, 3, 49, 49]) attention shape\n",
      "torch.Size([64, 49, 96]) after attention x shape\n",
      "torch.Size([64, 49, 96]) x.shape,attention\n",
      "torch.Size([64, 3, 49, 32]) q.shape\n",
      "torch.Size([64, 3, 49, 49]) attention shape\n",
      "torch.Size([64, 49, 96]) after attention x shape\n",
      "torch.Size([64, 49, 96]) x.shape,attention\n",
      "torch.Size([16, 6, 49, 32]) q.shape\n",
      "torch.Size([16, 6, 49, 49]) attention shape\n",
      "torch.Size([16, 49, 192]) after attention x shape\n",
      "torch.Size([16, 49, 192]) x.shape,attention\n",
      "torch.Size([16, 6, 49, 32]) q.shape\n",
      "torch.Size([16, 6, 49, 49]) attention shape\n",
      "torch.Size([16, 49, 192]) after attention x shape\n",
      "torch.Size([16, 49, 192]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([4, 12, 49, 32]) q.shape\n",
      "torch.Size([4, 12, 49, 49]) attention shape\n",
      "torch.Size([4, 49, 384]) after attention x shape\n",
      "torch.Size([4, 49, 384]) x.shape,attention\n",
      "torch.Size([1, 24, 49, 32]) q.shape\n",
      "torch.Size([1, 24, 49, 49]) attention shape\n",
      "torch.Size([1, 49, 768]) after attention x shape\n",
      "torch.Size([1, 49, 768]) x.shape,attention\n",
      "torch.Size([1, 24, 49, 32]) q.shape\n",
      "torch.Size([1, 24, 49, 49]) attention shape\n",
      "torch.Size([1, 49, 768]) after attention x shape\n",
      "torch.Size([1, 49, 768]) x.shape,attention\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0143, -0.1912]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(rand_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samuel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
